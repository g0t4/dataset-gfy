{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765897131,
  "id": "chatcmpl-qjqsOq7eat93OISeFx4xD8zUkSXcL6oy",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 0,
    "prompt_n": 3778,
    "prompt_ms": 504.623,
    "prompt_per_token_ms": 0.133568819481207,
    "prompt_per_second": 7486.777257477364,
    "predicted_n": 154,
    "predicted_ms": 695.396,
    "predicted_per_token_ms": 4.515558441558441,
    "predicted_per_second": 221.4565513750439
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 1,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 154,
    "tokens_evaluated": 3778,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-16\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n## Recent yanks across all files in the project:\n.. yanked from auto_edit/new_silence/player.py:\n    def add_arrow(time_point: float, arrow_y: float, label: str | None = None, color: str = \"red\") -> None:\n\n\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/player.py:138-155\n                    start,\n                    end,\n                    color=\"orange\",\n                    alpha=0.3,\n                    label=\"Window Gap\" if \"Window Gap\" not in self.ax.get_legend_handles_labels()[1] else None,\n                )\n\n                # * show detected minima inside the window as vertical lines\n                if win.first_min_seconds:\n                    self.ax.axvline(float(win.first_min_seconds), color=\"blue\", linestyle=\":\")\n                if win.last_min_seconds:\n                    self.ax.axvline(float(win.last_min_seconds), color=\"blue\", linestyle=\":\")\n\n        self.ax.set_xlabel(\"Time (s)\")\n        self.ax.set_ylabel(\"Amplitude\")\n        self.ax.set_title(f\"RMS Segments {self.title_addon}\")\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:152-165\n\n\n@dataclass\nclass GapWindow:\n    left_speech: DetectedSegment | None\n    gap: NonSpeechGap\n    right_speech: DetectedSegment | None\n\n    first_min_seconds: float | None = None\n    first_min_rms: float | None = None\n    last_min_seconds: float | None = None\n    last_min_rms: float | None = None\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:154-182\n@dataclass\nclass GapWindow:\n    left_speech: DetectedSegment | None\n    gap: NonSpeechGap\n    right_speech: DetectedSegment | None\n\n    first_min_seconds: float | None = None\n    first_min_rms: float | None = None\n    last_min_seconds: float | None = None\n    last_min_rms: float | None = None\n\n    def all_segments(self) -> list[DetectedSegment]:\n        segments: list[DetectedSegment] = []\n        if self.left_speech:\n            segments.append(self.left_speech)\n        segments.extend(self.gap.segments)\n        if self.right_speech:\n            segments.append(self.right_speech)\n        return segments\n\n    def get_window_start(self) -> float:\n        if self.left_speech:\n            return self.left_speech.start\n        return self.gap.segments[0].start\n\n    def get_window_end(self) -> float:\n        if self.right_speech:\n            return self.right_speech.end\n        return self.gap.segments[-1].end\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/player.py:43-217\nclass SegmentPlotter:\n\n    @staticmethod\n    def for_window(analysis: AudioAnalysisResult, window: GapWindow):\n        segments = window.all_segments()\n        return SegmentPlotter(analysis, segments, windows_subset=[window])\n\n    @staticmethod\n    def for_entire(analysis: AudioAnalysisResult):\n        return SegmentPlotter(analysis, analysis.merged, windows_subset=analysis.windows)\n\n    def __init__(\n        self,\n        analysis: AudioAnalysisResult,\n        segments_subset: list[DetectedSegment] | None = None,\n        title_addon: str | None = None,\n        windows_subset: list[GapWindow] | None = None,\n    ):\n        self.analysis = analysis\n        # simple idea => pass a subset of either OR just use all available on the analysis\n\n        self.segments = segments_subset or []\n        self.windows = windows_subset or []\n\n        self.fig, self.ax = plt.subplots(figsize=(20, 10))\n        self.title_addon = title_addon or \"\"\n        self._setup_plot()\n        self._connect_events()\n\n    def _play_segment(self, start: float, end: float) -> None:\n        sr = self.analysis.sample_rate\n        audio = self.analysis.audio_samples\n        start_idx = int(start * sr)\n        end_idx = int(end * sr)\n        segment = audio[start_idx:end_idx]\n        sd.play(segment, sr)\n\n    def setup_frame_ticks(self) -> None:\n\n        # keep in mind we don't necessarily plot all segments in the analysis\n        start_seconds = min(seg.start for seg in self.segments)\n        end_seconds = max(seg.end for seg in self.segments)\n\n        self.ax.set_xlim(start_seconds, end_seconds)\n\n        # major ticks: every second\n        major_interval = 1.0\n        major_start = np.ceil(start_seconds / major_interval) * major_interval\n        major_ticks = np.arange(major_start, end_seconds + major_interval, major_interval)\n        self.ax.set_xticks(major_ticks)\n\n        # labels show frame number for each major tick\n        major_labels = [f\"{int(t)}\" for t in major_ticks]\n        self.ax.set_xticklabels(major_labels)\n\n        # minor ticks: every frame\n        fps = self.analysis.fps\n        frame_interval = 1.0 / fps\n        minor_start = np.ceil(start_seconds / frame_interval) * frame_interval\n        minor_ticks = np.arange(minor_start, end_seconds + frame_interval, frame_interval)\n        self.ax.set_xticks(minor_ticks, minor=True)\n\n        self.ax.tick_params(axis=\"x\", which=\"minor\", length=4, colors=\"gray\")\n        self.ax.tick_params(axis=\"x\", which=\"major\", length=8, colors=\"black\")\n\n    def _setup_plot(self) -> None:\n\n        self.setup_frame_ticks()\n\n        for seg in self.segments:\n            times = np.linspace(seg.start, seg.end, len(seg.rms_values))\n            if len(seg.rms_values) == 1:\n                self.ax.scatter(\n                    times,\n                    seg.rms_values,\n                    color=seg.plot_color(),\n                    label=seg.get_plot_message(),\n                    s=10,\n                )\n            else:\n                self.ax.plot(\n                    times,\n                    seg.rms_values,\n                    color=seg.plot_color(),\n                    label=seg.get_plot_message(),\n                    linewidth=0.5,\n                )\n\n        if self.windows:\n            for win in self.windows:\n                start = win.get_window_start()\n                end = win.get_window_end()\n                if start is None or end is None:\n                    continue\n                self.ax.axvspan(\n                    start,\n                    end,\n                    color=\"orange\",\n                    alpha=0.3,\n                    label=\"Window Gap\" if \"Window Gap\" not in self.ax.get_legend_handles_labels()[1] else None,\n                )\n\n                # * show detected minima inside the window as vertical lines\n                if win.first_min_seconds:\n                    self.ax.axvline(float(win.first_min_seconds), color=\"blue\", linestyle=\":\")\n                if win.last_min_seconds:\n                    self.ax.axvline(float(win.last_min_seconds), color=\"blue\", linestyle=\":\")\n\n        self.ax.set_xlabel(\"Time (s)\")\n        self.ax.set_ylabel(\"Amplitude\")\n        self.ax.set_title(f\"RMS Segments {self.title_addon}\")\n\n    def _on_click(self, event) -> None:\n        if event.inaxes != self.ax:\n            return\n        click_time = event.xdata\n        for seg in self.segments:\n            if seg.start <= click_time <= seg.end:\n                self._play_segment(seg.start, seg.end)\n                break\n\n    def _connect_events(self) -> None:\n        setup_events(self.fig, self.ax, self._on_click)\n\n    def show(self) -> None:\n        plt.ion()  # else won't show new plots after initial call to plt.show()\n        plt.show()\n\n    def add_segments(self, new_segments: list[DetectedSegment]) -> None:\n        self.segments.extend(new_segments)\n        for seg in new_segments:\n            times = np.linspace(seg.start, seg.end, len(seg.rms_values))\n            self.ax.plot(\n                times,\n                seg.rms_values,\n                color=seg.plot_color(),\n                linewidth=0.5,\n            )\n        # self.fig.canvas.draw()\n        # self.fig.canvas.flush_events()\n\n    @staticmethod\n    def add_arrow(time_point: float, arrow_y: float, label: str | None = None, color: str = \"red\") -> None:\n        \"\"\"Draw a vertical arrow at *time_point* on the existing axes.\"\"\"\n\n        print(\"DRAWING ARROW\")\n\n        # place the arrow head at the top of the plot\n        ax = plt.gca()\n        # y_min, y_max = ax.get_ylim() # PRN use if no y_value provided\n        # arrow_y = y_max\n\n        # optional label placed slightly above the arrow\n        if label is None:\n            label = f\"{time_point:.2f}s\"\n        ax.annotate(\n            label,\n            xy=(time_point, arrow_y),\n            xytext=(time_point, arrow_y - 3),\n            ha=\"center\",\n            va=\"bottom\",\n            color=color,\n            arrowprops=dict(\n                arrowstyle=\"->\",\n                color=color,\n                linewidth=1.5,\n            ),\n        )\n\n        # refresh the canvas so the arrow appears immediately\n        plt.gcf().canvas.draw_idle()\n\n        # plotter.draw_time_arrow(12.3, label=\"Cue\", color=\"lime\")\n        # The function computes the current y‑limits, positions the arrow at the top of the plot, and optionally adds a text label.\n        #     Adjust `color`, `arrowstyle`, or the vertical offset as needed.\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:52-65\n# 128 ~= 5.80ms (5.68 per video frame)\n# 256 ~= 11.61ms (2.84 per video frame)\n# 512 ~= 23.22ms (1.42 per video frame)\n#\n# FYI video FPS = 30 ==> 33.333ms per frame (at least want to sample once per frame... => hop_length=734.999265 (once per frame)\n#   but lower helps smooth out sharp changes (nearly instantaneous in waveform, i.e. when start talking)... so I will use much smaller so I can see the full \"power form\" (think waveform but for RMS dB)...\n#   b/c I use transition shapes to find bounce/breathing sections reliably (so far!)\n# PRN do a timing analysis of perf difference w/ smaller hop_length values vs bigger (closer to 512)\n#\n# REASONS I LIKE SMALLER HOP_LENGTH\n# - more points == easier to see breathing bounce\n# - fewer segments in final analysis\n#    often very clear when breathing from a glance!\n\n\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nfrom dataclasses import dataclass\nfrom itertools import groupby\nfrom more_itertools import windowed\nfrom typing import Iterable\nimport numpy as np\nimport rich\nfrom .detects.segments import RMS_HOP_LENGTH, AudioAnalysisResult, DetectedSegment, SegmentLabel, GapWindow, NonSpeechGap\n\ndef grouped(segments):\n\n    @dataclass(frozen=True)\n    class SegmentGroup:\n        is_speech: bool\n        segments: list[DetectedSegment]\n\n    # return GAP(1+ segments), SPEECH(1 segment), GAP(segs), SPEECH(seg), …\n    for is_speech, segments in groupby(segments, key=lambda seg: seg.label == SegmentLabel.SPEECH):\n        yield SegmentGroup(is_speech=is_speech, segments=list(segments))\n\ndef build_gap_decisions(analysis: AudioAnalysisResult):\n\n    # DO NOT RUN on raw segments, use merged only!\n    segments = analysis.merged\n\n    # TODO tests of:\n    # - start is silence (common)\n    # - start is speech (rare but possible)\n    # - end is silence (common)\n    # - end is speech (also rare)\n\n    def gap(windowed):\n\n        left_speech = None\n        if windowed[0] and windowed[0].is_speech:\n            left_speech = windowed[0].segments[0]\n            assert len(windowed[0].segments) == 1, \"Expected only one left speech segment\"\n\n        right_speech = None\n        if windowed[2] and windowed[2].is_speech:\n            right_speech = windowed[2].segments[0]\n            assert len(windowed[2].segments) == 1, \"Expected only one right speech segment\"\n\n        gap_window = GapWindow(\n            left_speech=left_speech,\n            gap=NonSpeechGap(segments=windowed[1].segments),\n            right_speech=right_speech,\n        )\n\n        mark_safe_speech_boundaries(gap_window)\n        return gap_window\n\n    def mark_safe_speech_boundaries(window: GapWindow):\n\n        all_rms_values = np.concatenate([v.rms_values for v in window.gap.segments])\n\n        # * find left_speech's end of taper (using first local minimum found)\n        first_min_idx = 0\n        first_min = all_rms_values[0]\n        for idx, value in enumerate(all_rms_values):\n            if value < first_min:\n                first_min = value\n                first_min_idx = idx  # track separately so I can capture when first_min is first encountered\n                continue\n            if value > first_min + 3:  # jump in 3dB == reversed course, is this sufficient?\n                break\n\n        # * ramp up for right_speech's start (using first local min found, in reverse)\n        last_min_idx_backwards = 0\n        last_min = all_rms_values[-1]\n        for idx, value in enumerate(np.flip(all_rms_values)):\n            if value < last_min:\n                last_min = value\n                last_min_idx_backwards = idx\n                continue\n            if value > last_min + 3:\n                break\n\n        # * mark times\n        hops_per_second = analysis.sample_rate / RMS_HOP_LENGTH\n        gap_start_seconds = window.gap.segments[0].start\n        #\n        window.first_min_seconds = gap_start_seconds + first_min_idx / hops_per_second\n        <|fim_middle|>\n        #\n        # TODO get some tests in place, especially for -1 logic and edge cases\n        last_min_idx_forwards = len(all_rms_values) - 1 - last_min_idx_backwards\n        window.last_min_seconds = gap_start_seconds + last_min_idx_forwards / hops_per_second\n\n        # PRN breathing checks? to decide if I want to silence or not?\n        # _100ms = 0.1\n        # _400ms = 0.4\n        # minimums_gap = last_min - first_min\n        # if minimums_gap > _100ms and minimums_gap < _400ms:\n        #     # AND if rises up and down\n        #     #  then assume is breathing and just be careful about rounding inward... for now I don't think this is even needed though!\n\n    windows = [\n        gap(w) for w in windowed(grouped(segments), 3) \\\n            if w[1] and not w[1].is_speech\n    ]\n\n    return windows<|end|><|start|>assistant",
    "has_new_line": false,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 3931,
    "timings": {
      "cache_n": 0,
      "prompt_n": 3778,
      "prompt_ms": 504.623,
      "prompt_per_token_ms": 0.133568819481207,
      "prompt_per_second": 7486.777257477364,
      "predicted_n": 154,
      "predicted_ms": 695.396,
      "predicted_per_token_ms": 4.515558441558441,
      "predicted_per_second": 221.4565513750439
    }
  }
}

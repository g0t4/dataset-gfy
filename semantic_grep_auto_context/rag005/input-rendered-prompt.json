{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765148329,
  "id": "chatcmpl-9woiQwFkdTelBZ8B92PvY7dkfRmOrO2X",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 0,
    "prompt_n": 2597,
    "prompt_ms": 406.69,
    "prompt_per_token_ms": 0.15659992298806316,
    "prompt_per_second": 6385.699181194522,
    "predicted_n": 66,
    "predicted_ms": 291.746,
    "predicted_per_token_ms": 4.420393939393939,
    "predicted_per_second": 226.2241813083984
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 0,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 66,
    "tokens_evaluated": 2597,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-07\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/lsp/config.py:17-24\ndef load_config(yaml_text: str) -> Config:\n    raw = yaml.safe_load(yaml_text)\n\n    return Config(\n        ignores=raw.get(\"ignores\") or [],\n        include=raw.get(\"include\") or [],\n        global_languages=raw.get(\"global_languages\") or [],\n    )\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/lsp/config.py:11-15\n@staticmethod\n    def default() -> \"Config\":\n        return Config(\n            include=[\"lua\", \"py\", \"fish\"],  #\n        )\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/lsp/config.py:4-15\n@dataclass\nclass Config:\n    ignores: list[str] = field(default_factory=list)\n    include: list[str] = field(default_factory=list)\n    global_languages: list[str] = field(default_factory=list)\n    enabled: bool = field(default=True)\n\n    @staticmethod\n    def default() -> \"Config\":\n        return Config(\n            include=[\"lua\", \"py\", \"fish\"],  #\n        )\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/lsp/inference/client/retrieval.py:0-19\nfrom dataclasses import dataclass\nfrom pathlib import Path\n\nimport attrs\nfrom lsp.inference.client.embedder import encode_query, signal_hotpath_done_in_background\nfrom lsp.stoppers import Stopper\nfrom lsp.storage import ChunkType, Datasets\nfrom lsp.inference.client import *\nfrom lsp.fs import get_config, relative_to_workspace\n\n@dataclass\nclass LSPRankedMatch:\n    id: str\n    id_int: str\n    text: str\n    file: str\n    # using _base0 b/c this is serialized to clients so it must be clear as base0, clients can wrap and add .base0.start_line or .base1.start_line if desired\n    # also server side I am not doing much with this, mostly just serialize responses to client so I don't need easy access to base1 on this type\n    start_line_base0: int\n    start_column_base0: int\n\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/lsp/server.py:0-19\nimport asyncio\nimport logging\nimport os\nimport signal\nfrom pathlib import Path\n\nimport lsprotocol.types as types\nfrom pygls import uris\nfrom pygls.lsp.server import LanguageServer\nfrom pygls.protocol.json_rpc import MsgId\n\nfrom lsp.chunks.chunker import RAGChunkerOptions\nfrom lsp import ignores, rag\nfrom lsp import fs\nfrom lsp.context.imports import imports\nfrom lsp.logs import get_logger, logging_fwk_to_language_server_log_file, disable_printtmp\n\ndisable_printtmp()  # LSP uses STDOUT for comms!\n\nfrom lsp.stoppers import request_stop, create_stopper, remove_stopper\n\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nimport aiofiles\nimport asyncio\nfrom lsp.logs import disable_printtmp, get_logger, disable_printtmp\nfrom lsp.inference.client.embedder import get_shape, encode_passages, signal_hotpath_done_in_background\n\nlogger = get_logger(__name__)\n\nimport argparse\nimport logging\nimport subprocess\nimport sys\nimport yaml\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Optional, Set\n\nimport faiss\nimport numpy as np\n\nfrom pydants import write_json\n\nimport fs\nfrom lsp.storage import Chunk, FileStat, load_prior_data\nfrom lsp.chunks.chunker import RAGChunkerOptions, build_chunks_from_file, get_file_stat\nfrom lsp.config import Config, <|fim_middle|>\n\n# constants for subprocess.run for readability\nIGNORE_FAILURE = False\nSTOP_ON_FAILURE = True\n\n@dataclass\nclass FilesDiff:\n    # FYI type mismatch IS FINE with type hints... LEAVE IT!\n    changed: Set[Path]\n    deleted: Set[str]\n    not_changed: Set[str]\n\n@dataclass\nclass ProgramArgs:\n    verbose: bool\n    info: bool\n    in_githook: bool\n    rebuild: bool\n    level: int\n    only_extension: str | None = None\n\nclass IncrementalRAGIndexer:\n\n    def __init__(self, dot_rag_dir, source_code_dir, options: RAGChunkerOptions, program_args: ProgramArgs | None = None):\n        self.options = options\n        self.dot_rag_dir = Path(dot_rag_dir)\n        self.source_code_dir = Path(source_code_dir)\n        self.program_args = program_args\n\n    async def main(self):\n        index_these_file_extensions = await self.get_indexed_file_extensions()\n\n        if self.program_args and self.program_args.only_extension:\n            index_these_file_extensions = [self.program_args.only_extension]\n            logger.info(f\"Indexing only extension: {index_these_file_extensions[0]}\")\n\n        for file_extension in index_these_file_extensions:\n            await self.build_index(file_extension)\n        self.warn_about_other_extensions(index_these_file_extensions)\n        await signal_hotpath_done_in_background()\n\n    def get_default_indexed_file_extensions(self):\n        # TODO I need to create groupings of related extensions... i.e. fish+zsh+bash+sh as 'shell' type\n        #   TODO! combine alt file extensions that are the same: yaml/yml\n        #   PRN also use shebang when chunking files? and look at plaintext, extensionless files w/ a shebang (esp chmod +x files)\n        return [\n            \"lua\", \"py\", \"java\", \"js\", \"ts\", \"html\",\n            \"md\", \"json\", \"yaml\", \"yml\",\n            \"fish\", \"zsh\", \"sh\", # shells\n            \"cpp\", \"cc\", \"c\", \"h\", \"hpp\", # c related\n            \"m\", \"mm\", # objective c\n            \"cu\", \"cuh\", \"cl\", # GPU\n            \"rs\",\n            \"go\",\n        ] # yapf: disable\n\n\n    async def get_indexed_file_extensions(self):\n        rag_yaml = self.source_code_dir / \".rag.yaml\"\n        if not rag_yaml.exists():\n            logger.info(f\"no rag config found {rag_yaml}, using default config\")\n            return self.get_default_indexed_file_extensions()\n\n        async with aiofiles.open(rag_yaml, mode=\"r\") as f:\n            content = await f.read()\n        config = yaml.safe_load(content)\n        logger.pp_debug(f\"found rag config: {rag_yaml}\", config)\n\n        if \"include\" in config and config[\"include\"]:\n            return config[\"include\"]\n        return self.get_default_indexed_file_extensions()\n\n    def warn_about_other_extensions(self, index_languages: list[str]):\n\n        result = subprocess.run(\n            [\"fish\", \"-c\", f\"fd {self.source_code_dir} --exclude='\\\\.ctags\\\\.d' --exclude='\\\\.rag' --exec basename\"],\n            stdout=subprocess.PIPE,\n            text=True,\n            check=STOP_ON_FAILURE,\n        )\n        basenames = result.stdout.strip().splitlines()\n\n        ignore_files = [\"ctags\", \"ctags.d\"]\n\n        extensions = [basename.split(\".\")[-1] \\\n            for basename in basenames \\\n            if basename and \".\" in basename \\\n            and basename not in ignore_files\n        ]\n\n        import itertools\n        unindexed_extensions = [(ext, len(list(group))) \\\n            for ext, group in itertools.groupby(sorted(extensions)) \\\n            if ext not in index_languages\n        ]\n\n        # TODO pair with an ignore style file for what not to index in a given repo\n\n        # require at least 2 of a file before warning\n        #   within one file you don't need RAG for context :)\n        noteworthy_extensions = [ \\\n            f\"{ext}={count}\" \\\n            for ext, count in unindexed_extensions \\\n            if count > 1\n        ]\n\n        if noteworthy_extensions:\n            logger.debug(f\"Found unindexed extensions: {' '.join(noteworthy_extensions)}\")\n\n    def get_files_diff(self, language_extension: str, prior_stat_by_path: dict[str, FileStat]) -> FilesDiff:\n        \"\"\"Split files into: changed (added/updated), unchagned, deleted\"\"\"\n\n        # PRN add in gitignore detection, right now I am using fd so I s/b mostly fine, still might want explicit checks here too\n        #   use whatever I come up with from LS's text document events to filter... i.e. files in a .venv that I open (F12)\n        #    though that again isn't an issue for this part of indexing\n        # PRN add .ask.config or similar w/ ignore section to block things like manual_prompting folder! in ask-openai repo!\n\n        # * current files\n        result = subprocess.run(\n            [\"fd\", f\".*\\\\.{language_extension}$\", str(self.source_code_dir), \"--absolute-path\", \"--type\", \"f\"],\n            stdout=subprocess.PIPE,\n            text=True,\n            check=True,\n        )\n        current_path_strs = set(result.stdout.strip().splitlines())\n\n        # * added, modified (aka changed)\n        changed_paths: Set[Path] = set()\n        for file_path_str in current_path_strs:\n            file_path = Path(file_path_str)\n            is_new_file = file_path_str not in prior_stat_by_path\n            if is_new_file:\n                changed_paths.add(file_path)\n                logger.debug(f\"[green]New file: {file_path}\")\n            else:\n                current_mod_time = file_path.stat().st_mtime<|end|><|start|>assistant",
    "has_new_line": false,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 2662,
    "timings": {
      "cache_n": 0,
      "prompt_n": 2597,
      "prompt_ms": 406.69,
      "prompt_per_token_ms": 0.15659992298806316,
      "prompt_per_second": 6385.699181194522,
      "predicted_n": 66,
      "predicted_ms": 291.746,
      "predicted_per_token_ms": 4.420393939393939,
      "predicted_per_second": 226.2241813083984
    }
  }
}

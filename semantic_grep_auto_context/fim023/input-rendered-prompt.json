{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1767292882,
  "id": "chatcmpl-wBQoCrkhIipe3MflyYmR6s6hCSnIwHmb",
  "model": "ggml-org/gpt-oss-120b-GGUF",
  "system_fingerprint": "b7453-8faa87db0",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 1086,
    "prompt_n": 584,
    "prompt_ms": 290.452,
    "prompt_per_token_ms": 0.49734931506849317,
    "prompt_per_second": 2010.6592483439604,
    "predicted_n": 244,
    "predicted_ms": 1150.85,
    "predicted_per_token_ms": 4.716598360655738,
    "predicted_per_second": 212.01720467480558
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 0,
    "stop": true,
    "model": "ggml-org/gpt-oss-120b-GGUF",
    "tokens_predicted": 244,
    "tokens_evaluated": 1670,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2026-01-01\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Lua Code Preferences\n\nNaming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\nType Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nlocal log = require('ask-openai.logs.logger').predictions()\n\nlocal M = {\n    last = {}\n}\n\nlocal function get_delta(sse_parsed)\n    if sse_parsed.choices and sse_parsed.choices[1] then\n        return sse_parsed.choices[1].delta\n    end\nend\n\n---@param sse_parsed table\n---@param request CurlRequest\n---@param frontend StreamingFrontend\nfunction M.log_sse_to_request(sse_parsed, request, frontend)\n    log:error(\"TEST\", vim.inspect(sse_parsed))\n\n    -- {\n    --   choices = { {\n    --       delta = {\n    --         content = vim.NIL,\n    --         role = \"assistant\"\n    --       },\n    --       finish_reason = vim.NIL,\n    --       index = 0\n    --     } },\n    --   created = 1767288436,\n    --   id = \"chatcmpl-hRLC4x8ugX6ET01g8pQ9QPsoU0wVv8MS\",\n    --   model = \"ggml-org/gpt-oss-120b-GGUF\",\n    --   object = \"chat.completion.chunk\",\n    --   system_fingerprint = \"b7453-8faa87db0\"\n    -- }\n\n    --       first delta has role\n    --       delta = {\n    --         content = vim.NIL,\n    --         role = \"assistant\"\n    --       },\n\n    -- then reasoning\n    -- delta = {\n    --   reasoning_content = \"User\"\n    -- },\n    -- delta = {\n    --   reasoning_content = \"did\"\n    -- },\n    -- ...\n\n    -- then content\n    -- delta = {\n    --   content = \"---@\"\n    -- },\n    -- delta = {\n    --   content = \"foo\"\n    -- },\n    -- ...\n\n    -- LAST SSE will have final choices like this:\n    -- choices = { {\n    --   delta = vim.empty_dict(),\n    --   finish_reason = \"stop\",\n    --   index = 0\n    -- } },\n\n    -- ensure the generated subâ€‘table exists\n    accum = request.accum or {}\n    request.accum = accum\n    delta = get_delta(sse_parsed)\n    -- FYI delta is the only part that changes along the way (at the end some other parts get set like finish_reason (above delta) but delta is where the partials are at\n    if not delta then\n        return\n    end\n    <|fim_middle|>\n\n\n\n    -- TODO On final SSE  sse_parsed.timings /  choices[0].finish_reason is set...\n    --  then... do this to save to disk (if I want that feature)\n    -- log_sse_non_blocking\n\n    if sse_parsed.timings then\n        M.log_final_sse(sse_parsed, request, frontend)\n    end\nend\n\n-- TODO can I use this approach for building the diff too? and if new tokens arrive then discard the diff, and increase the throttle?\n--   would it meaningfully help perf?\n--   TODO only add this if I can measure perf benefit\n--\n-- function M.log_sse_non_blocking(???)\n--     -- Run the potentially blocking logger in a libuv thread pool.\n--     vim.loop.new_thread(function()\n--         -- This runs in a worker thread; keep it pure Lua.\n--         -- TODO expensive, i.e. file write\n--     end, function(_, result)\n--         -- This callback runs back on the main thread.\n--         -- If the logger returns data that needs to be processed,\n--         -- do it here (e.g., update UI buffers, flash messages, etc.).\n--         if result then\n--             -- Example: refresh a buffer displaying the completion.\n--             vim.schedule(function()\n--                 -- Assuming `frontend:update` refreshes the UI.\n--                 if frontend.update then\n--                     frontend:update(result)\n--                 end\n--             end)\n--         end\n--     end)\n-- end\n\n---@param sse_parsed table\n---@param request CurlRequest\n---@param frontend StreamingFrontend\nfunction M.log_final_sse(sse_parsed, request, frontend)\n    M.last = {\n        sse_parsed = sse_parsed,\n        request = request,\n        frontend = frontend,\n    }\n\n    -- let's do it by time so I can easily find a specific request based on timeframe\n    --   and purge them too that way\n    local created = sse_parsed.created\n    log:error(\"created\", created)\n\n    log:error(\"full sse\", vim.inspect(sse_parsed))\n    log:error(\"full request\", vim.inspect(request)) -- for reasoning+ content (parsed output message) + body (input-messages)\n    log:error(\"input-messages.json\", vim.inspect(request.body)) -- keep this to reproduce issues with template rendering\n    log:error(\"input-prompt.json\", vim.inspect(sse_parsed.__verbose.prompt)) -- this is the raw prompt into the model\n    log:error(\"output.harmony\", vim.inspect(M.last.generated)) -- this is the raw prompt into the model\n\n    -- FYI if stream=false, then the last SSE has .__verbose.content (but not for streaming)\nend\n\nreturn M<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 1913,
    "timings": {
      "cache_n": 1086,
      "prompt_n": 584,
      "prompt_ms": 290.452,
      "prompt_per_token_ms": 0.49734931506849317,
      "prompt_per_second": 2010.6592483439604,
      "predicted_n": 244,
      "predicted_ms": 1150.85,
      "predicted_per_token_ms": 4.716598360655738,
      "predicted_per_second": 212.01720467480558
    }
  }
}

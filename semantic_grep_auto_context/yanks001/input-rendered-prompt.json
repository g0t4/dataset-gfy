{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765400941,
  "id": "chatcmpl-7rrLmN2b9WZSNG6mDo6ZVtyTUySYA1Dm",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 375,
    "prompt_n": 2935,
    "prompt_ms": 452.982,
    "prompt_per_token_ms": 0.15433798977853494,
    "prompt_per_second": 6479.286152650657,
    "predicted_n": 51,
    "predicted_ms": 240.029,
    "predicted_per_token_ms": 4.706450980392157,
    "predicted_per_second": 212.47432601893937
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 1,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 51,
    "tokens_evaluated": 3310,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-10\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n## Recent yanks across all files in the project:\n.. yanked from auto_edit/ranges.py:\n    transcriptions: list[WhisperSegment] = []\n\n.. yanked from auto_edit/transcribe.py:\n        print(\"  START >= END... skipping\")\n\n.. yanked from auto_edit/transcribe.py:\n        c.transcriptions = []\n\n\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/research/compare_whisper_cpp/absadiki/test_new_segement_ready_callback.py:29-30\ndef segemnt_ready(s):\n    print(f\"  ready: {s}\")\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/research/compare_whisper_cpp/absadiki/test_n_processors_full_parallel.py:29-30\ndef segemnt_ready(s):\n    print(f\"  ready: {s}\")\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/research/compare_whisper_cpp/absadiki/test_wav_file.py:30-37\nfor segment in segments:\n    print(segment)\n    # print(segment)\n\n\n\n\n\n\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/auto_edit/tests/raw-tests.py:8-10\ndef test_str_representation(self):\n        silence = RawSilencePeriod(5.5, 12.75)\n        assert str(silence) == \"Raw[5.5âžœ12.75)\"\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/auto_edit/tests/aligned-tests.py:67-73\n        # aligned = align_silences_to_frame_boundaries_asymmetric_pad_WIP(\n        #     raw_silences, fps=30, frame_offset=0\n        # )\n        # assert isinstance(aligned, list)\n        # assert all(hasattr(s, \"start_frames\") and hasattr(s, \"end_frames\") for s in aligned)\n\n    \n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\n        self.resources = ET.SubElement(self.root, \"resources\")\n\n        # library(s) must come after resources\n        #   FYI library's location ATTR doesn't work on imports so I don't have it here (see import-options above)\n        self.library = ET.SubElement(\n            self.root,\n            \"library\",\n        )\n\n        self.event = ET.SubElement(self.library, \"event\", name=\"auto\")\n\n        self.project = ET.SubElement(self.event, \"project\", name=project_name)\n\n        self.format_id = \"r1\"\n        self.sequence = ET.SubElement(self.project, \"sequence\", format=self.format_id)\n        self.spine = ET.SubElement(self.sequence, \"spine\")\n\n        ET.SubElement(\n            self.resources,\n            \"format\",\n            id=self.format_id,\n            name=f\"FFVideoFormat3840x2160p{int(original.fps)}\",\n            # IIRC I can set width/height too and it works? could use source resolution (moviepy video attrs)\n            #         <format id=\"r1\" name=\"FFVideoFormat3840x2160p30\" frameDuration=\"100/3000s\" width=\"3840\" height=\"2160\" colorSpace=\"1-1-1 (Rec. 709)\"/>\n            # currently rec 709 is default it appears\n            # 4k exports beautifully and then let youtube/rumble/etc transcode to 1080p etc...\n            #   plus stills in FCPX come out as 4k resolution if project is 4k (b/c underlying video is 4k, but if project is 1080p then stills are exported as 1080p and not that crisp... 4k is also nice for \"Copy Subject\" in Photos app to make my viseage bigger)\n        )\n\n        self.r2_id = \"r2\"\n        self.r2_video = ET.SubElement(\n            self.resources,\n            \"asset\",\n            id=self.r2_id,\n            format=self.format_id,\n        )\n        video_path_relative_to_fcpxml = os.path.relpath(original.filename, output_dir)\n        self.media_rep = ET.SubElement(self.r2_video, \"media-rep\", kind=\"original-media\", src=video_path_relative_to_fcpxml)\n\n    def export_xml(self):\n        with open(fcpxml_file, \"w\") as xml_file:\n            pretty_xml = prettify(self.root)\n            xml_file.write(pretty_xml)\n        print(f\"exported: {fcpxml_file}\")\n\nfcpxml = FcpXml(original)\n\n# WORKS WELL TOO! and for longer periods this is WAY better than constant speed up factor...\n#   0.25 might be too small, if so use 0.3 to 0.5\n\n# FYI => mistakes aren't a big deal! so maybe I should spend less time editing out retakes and just get in habit of correct myself as if talking to a person and then just keep going and leave the flub for youtube at least... and for PS where perfection is required I can fix in POST\n\nspine_offset_frames = 0  # FYI NOT USED,  leaving just in case need to bring back but w/o this a lot of timing issues go away! let FCPX figure this out w/ other factors it modifies as it reads fcpxml file\nuse_lane1 = True\n\nfirst_silence = all_ranges[0] if isinstance(all_ranges[0], SilenceRange) else None\n# last_silence = all_ranges[-1] if isinstance(all_ranges[-1], SilenceRange) else None\nfor r in all_ranges:\n\n    if isinstance(r, ContentRange):\n        content: ContentRange = r\n\n        print(f\"offset: {spine_offset_frames}, duration: {content.duration_frames()}\")\n\n        name = f\"content{content.start_frame}-{content.end_frame} / {show_seconds(content.start_frame)}-{show_seconds(content.end_frame)}\"\n        content_clip_xml = ET.SubElement(\n            fcpxml.spine,\n            \"asset-clip\",\n            # FYI info panel name field auto expands if text overflows one line (like note below)\n            name=name,\n            ref=\"r2\",\n            # offset=frames_to_fraction(spine_offset_frames),  # within the spine ==> duration of all previous clips in spine, so if 3 seconds of clips before this one then offset=3s\n            # offset != start unless not cutting anything nor adding anything before this clip\n            start=frames_to_fraction(content.start_frame),  # start time w/in the media asset itself, so if this was originally 20 seconds to 30 seconds, then start=20s\n            duration=frames_to_fraction(content.duration_frames()),  # duration of this clip in the timeline, so if this was originally 20 seconds to 30 seconds, then duration=10s\n            audioRole=\"dialogue\",\n        )\n\n        # ET.SubElement(content_clip_xml, \"note\").text = name\n\n        for s in <|fim_middle|>\n            print(f\"segment: {s}\")\n            what: WhisperSegment = s\n            lane = \"1\" if use_lane1 else \"2\"\n            use_lane1 = not use_lane1\n            segment_start_frame = what.start_frame + content.start_frame  # b/c transcribed relative to content_range\n            duration_frames = what.duration_frames()\n            if (what.start_frame == 0):\n                # never mark transcription as starting at very start of an asset-clip (many do) b/c it makes edits a PITA:\n                #   otherwise \\ and [ keys selects the start of the transcription and not asset clip I wanted to select\n                #   FYI won't fix issue of end overlapping with next edit point(s)\n                #       Also won't fix the last caption going past end of entire video (causes black frames)\n                segment_start_frame += 1\n            caption_xml = ET.SubElement(\n                content_clip_xml,\n                \"caption\",\n                lane=lane,  # PRN alternate lane1 / lane2 to avoid collisions (overlap)\n                offset=f\"{frames_to_fraction(segment_start_frame)}\",  # offset is the position in original media timeline\n                # start=f\"{str(relative_start)}s\", # not needed (IIUC) => IIAC its computed if needed by FCPX\n                duration=f\"{frames_to_fraction(duration_frames)}\",\n                role=\"iTT?captionFormat=ITT.en\",\n            )\n            text = ET.SubElement(caption_xml, \"text\", placement=\"bottom\")\n            text.text = what.text\n\n        if include_suggests and content.id in dict_lines_to_remove:\n            # leave red marker if openai suggested to remove the item, can easily select and drag to delete them until I can be confident it won't delete unnecessary clips... this is valuable alone!\n            ET.SubElement(\n                content_clip_xml,\n                \"marker\",\n                completed=\"0\",\n                start=f\"{frames_to_fraction(content.start_frame)}\",\n                duration=\"1s\",\n                value=f\"{dict_lines_to_remove[content.id]}\",\n            )\n\n        # add duration of this clip to total so far:\n        spine_offset_frames = round(spine_offset_frames + content.duration_frames())\n\n    # add silence clip (if any)\n    if isinstance(r, SilenceRange):\n        silence = r\n        notes = \"\"\n        name = \"\"\n        print(silence)\n        # 2-3x both work, and higher (8x in still period tested and not noticeable) when I am not making drastic movements in webcam overlay view\n\n        # *** TOGGLE SPEED UP ON/OFF ***\n        # TODO rip out SPEED UP logic (after 2025-02-01) if I am still happy with the cut instead of speed up (WORKS SO MUCH BETTER SO FAR - I found myself removing speed ups and cutting them short... wasn't liking speed ups at all..) - see comment below where I disable speed up when speed up would apply!\n        disable_speed = False  # turn speedup on/off so I can easily toggle this as needed\n        max_silence_frames = round(0.5 * original.fps)\n        if silence == first_silence:\n            notes += \"trim first silence\"\n            name += \"trim-\"\n            # TODO move this logic to an interim filter step... have some new representation of clips where I can mark speed up vs no speed up and alter durations/starts/ends too ... that way this loop is all about mapping to FCPXML and not about altering the clips themselves\n            disable_speed = True\n            # limit to last 0.5s @ 1x\n            if silence.duration_frames() > max_silence_frames:\n                new_start_frame = round(silence.end_frame - max_silence_frames)\n                silence = SilenceRange(new_start_frame, silence.end_frame)\n\n        # limit to first 0.5s @ 1x - for all long silences\n        if silence.duration_frames() > max_silence_frames:\n            # !!! FYI THIS EFFECTIVELY DISABLES SPEED UPS (IIUC in my current re-review)\n            notes += \"trim silence\"\n            name += \"trim-\"\n            # FYI long silences aren't likely breathing at the start (gasps are usually short silences, IIAC)\n            # minor tweak - maybe - is there a way to tell when maybe a longer silence is warranted vs 0.5 always? i.e. end of a thought, any correlation to audio patterns and/or length of silence? maybe cut 0.75s in some cases? playback a video to find some examples (look for trim- marked clips)\n            disable_speed = True\n            new_end_frame = round(silence.start_frame + max_silence_frames)\n            silence = SilenceRange(silence.start_frame, new_end_frame)\n\n        speed_factor = 1\n        if not disable_speed:\n            speed_factor = silence.duration_frames() / (0.5 * original.fps)\n            speed_factor = max(1, speed_factor)  # 1x min => no slower sections\n\n        # IIAC rounding here is also why join doesn't always produce the dashed boundary indicating a clean cut, but that's fine\n        silence_start_frame = silence.start_frame / speed_factor  # ! fractions on start frame on silences lead to randomly extra parts of silences at end of spine... IIGC FCPX rounds somehow and if it goes down then it adds to the duration and thus duration of clip is more than spine_offset_frames accounts for and so the overflow wraps then... so lets round for it to avoid that and then IF ISSUES IN ROUNDING adddress them AS THEY ARISE... i.e. higher speed ups are likely to maybe have too much of a shift to the right? I am rounding up (ceil) so I stay in the silence period (unless its super super short then I might have issues) again figure this out as you go\n        # CONFIRMED if I drop spine offset frames then FCPX doesn't obviously put anything on the end so I will leave that unless it causes issues?!\n        #     also I CONFIRMED that once I droppped spine offsets and left partial decimals in start=\"24.5/25s\" then the duration of the clip was expanded in some cases so its not that FCPX doesn't understand decimals its just that it expands duration in some situations and I...<|end|><|start|>assistant",
    "has_new_line": false,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 3360,
    "timings": {
      "cache_n": 375,
      "prompt_n": 2935,
      "prompt_ms": 452.982,
      "prompt_per_token_ms": 0.15433798977853494,
      "prompt_per_second": 6479.286152650657,
      "predicted_n": 51,
      "predicted_ms": 240.029,
      "predicted_per_token_ms": 4.706450980392157,
      "predicted_per_second": 212.47432601893937
    }
  }
}

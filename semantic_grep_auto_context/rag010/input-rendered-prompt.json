{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765325463,
  "id": "chatcmpl-PACl6guPkiCCwd9vFxnwobOGvhAPJakL",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 0,
    "prompt_n": 5212,
    "prompt_ms": 704.292,
    "prompt_per_token_ms": 0.1351289332310054,
    "prompt_per_second": 7400.33963185724,
    "predicted_n": 177,
    "predicted_ms": 860.296,
    "predicted_per_token_ms": 4.860429378531074,
    "predicted_per_second": 205.7431395705664
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 1,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 177,
    "tokens_evaluated": 5212,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-09\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence_detect/plot_helpers.py:0-49\ndef setup_events(fig, ax, on_click):\n\n    def on_key(event):\n        # existing navigation\n        if event.key == \"left\":\n            cur_xlim = ax.get_xlim()\n            shift = (cur_xlim[1] - cur_xlim[0]) * 0.1\n            ax.set_xlim(cur_xlim[0] - shift, cur_xlim[1] - shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"right\":\n            cur_xlim = ax.get_xlim()\n            shift = (cur_xlim[1] - cur_xlim[0]) * 0.1\n            ax.set_xlim(cur_xlim[0] + shift, cur_xlim[1] + shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"up\":\n            cur_ylim = ax.get_ylim()\n            shift = (cur_ylim[1] - cur_ylim[0]) * 0.1\n            ax.set_ylim(cur_ylim[0] + shift, cur_ylim[1] + shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"down\":\n            cur_ylim = ax.get_ylim()\n            shift = (cur_ylim[1] - cur_ylim[0]) * 0.1\n            ax.set_ylim(cur_ylim[0] - shift, cur_ylim[1] - shift)\n            fig.canvas.draw_idle()\n        # zoom handling\n        elif event.key in (\"+\", \"=\"):  # both '+' and '=' share the same keycode on many keyboards\n            # zoom in by 20 %\n            cur_xlim = ax.get_xlim()\n            cur_ylim = ax.get_ylim()\n            x_center = sum(cur_xlim) / 2\n            y_center = sum(cur_ylim) / 2\n            x_range = (cur_xlim[1] - cur_xlim[0]) * 0.8\n            y_range = (cur_ylim[1] - cur_ylim[0]) * 0.8\n            ax.set_xlim(x_center - x_range / 2, x_center + x_range / 2)\n            ax.set_ylim(y_center - y_range / 2, y_center + y_range / 2)\n            fig.canvas.draw_idle()\n        elif event.key == \"-\":\n            # zoom out by 20 %\n            cur_xlim = ax.get_xlim()\n            cur_ylim = ax.get_ylim()\n            x_center = sum(cur_xlim) / 2\n            y_center = sum(cur_ylim) / 2\n            x_range = (cur_xlim[1] - cur_xlim[0]) / 0.8\n            y_range = (cur_ylim[1] - cur_ylim[0]) / 0.8\n            ax.set_xlim(x_center - x_range / 2, x_center + x_range / 2)\n            ax.set_ylim(y_center - y_range / 2, y_center + y_range / 2)\n            fig.canvas.draw_idle()\n\n    fig.canvas.mpl_connect(\"key_press_event\", on_key)\n    fig.canvas.mpl_connect(\"button_press_event\", on_click)\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence_detect/plot_helpers.py:0-19\ndef setup_events(fig, ax, on_click):\n\n    def on_key(event):\n        # existing navigation\n        if event.key == \"left\":\n            cur_xlim = ax.get_xlim()\n            shift = (cur_xlim[1] - cur_xlim[0]) * 0.1\n            ax.set_xlim(cur_xlim[0] - shift, cur_xlim[1] - shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"right\":\n            cur_xlim = ax.get_xlim()\n            shift = (cur_xlim[1] - cur_xlim[0]) * 0.1\n            ax.set_xlim(cur_xlim[0] + shift, cur_xlim[1] + shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"up\":\n            cur_ylim = ax.get_ylim()\n            shift = (cur_ylim[1] - cur_ylim[0]) * 0.1\n            ax.set_ylim(cur_ylim[0] + shift, cur_ylim[1] + shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"down\":\n\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/generate_fcpxml.py:474-528\ndef __init__(self, original: VideoFileClip):\n        # set early so its available in other places:\n        self.root = ET.Element(\"fcpxml\", version=\"1.10\")  # ? 1.12?\n\n        # *** import-options MUST come FIRST, before resources\n        #   https://developer.apple.com/documentation/professional-video-applications/import-options\n        #      more docs about import location: https://developer.apple.com/documentation/professional-video-applications/sending-data-programmatically-to-final-cut-pro#Specify-a-Library-for-the-FCPXML-File\n        #   (pay attention to warning on import if there is a mismatch and check the DTD - ordering is a common issue and the warning is not clear!)\n        #   IIUC import-options must come first b/c they affect importing resources (i.e. media files)\n        self.import_options = ET.SubElement(self.root, \"import-options\")\n        ET.SubElement(self.import_options, \"option\", key=\"library location\", value=library_file)\n        # \"copy assets\" to 1 (copy) or 0 (link) => I would prefer to move them here for final storage\n        ET.SubElement(self.import_options, \"option\", key=\"copy assets\", value=\"1\")\n        # \"base url\" is for resolving src ATTR on resource elements # ET.SubElement(self.import_options, \"option\", key=\"base url\", value=\"\")\n        #    FYI relative paths work (w/o base url) if relative to this fcpxml file\n        #    IIAC, \"base url\" lets me use a diff relative base path for resources\n        # \"suppress warnings\" is available, but DO NOT CHANGE IT, I wanna see import failures\n\n        self.resources = ET.SubElement(self.root, \"resources\")\n\n        # library(s) must come after resources\n        #   FYI library's location ATTR doesn't work on imports so I don't have it here (see import-options above)\n        self.library = ET.SubElement(\n            self.root,\n            \"library\",\n        )\n\n        self.event = ET.SubElement(self.library, \"event\", name=\"auto\")\n\n        self.project = ET.SubElement(self.event, \"project\", name=project_name)\n\n        self.format_id = \"r1\"\n        self.sequence = ET.SubElement(self.project, \"sequence\", format=self.format_id)\n        self.spine = ET.SubElement(self.sequence, \"spine\")\n\n        ET.SubElement(\n            self.resources,\n            \"format\",\n            id=self.format_id,\n            name=f\"FFVideoFormat3840x2160p{int(original.fps)}\")\n            # IIRC I can set width/height too and it works? could use source resolution (moviepy video attrs)\n            #         <format id=\"r1\" name=\"FFVideoFormat3840x2160p30\" frameDuration=\"100/3000s\" width=\"3840\" height=\"2160\" colorSpace=\"1-1-1 (Rec. 709)\"/>\n            # currently rec 709 is default it appears\n            # 4k exports beautifully and then let youtube/rumble/etc transcode to 1080p etc...\n            #   plus stills in FCPX come out as 4k resolution if project is 4k (b/c underlying video is 4k, but if project is 1080p then stills are exported as 1080p and not that crisp... 4k is also nice for \"Copy Subject\" in Photos app to make my viseage bigger)\n\n        self.r2_id = \"r2\"\n        self.r2_video = ET.SubElement(\n            self.resources,\n            \"asset\",\n            id=self.r2_id,\n            format=self.format_id,\n        )\n        video_path_relative_to_fcpxml = os.path.relpath(original.filename, output_dir)\n        self.media_rep = ET.SubElement(self.r2_video, \"media-rep\", kind=\"original-media\", src=video_path_relative_to_fcpxml)\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/generate_fcpxml.py:472-534\nclass FcpXml:\n\n    def __init__(self, original: VideoFileClip):\n        # set early so its available in other places:\n        self.root = ET.Element(\"fcpxml\", version=\"1.10\")  # ? 1.12?\n\n        # *** import-options MUST come FIRST, before resources\n        #   https://developer.apple.com/documentation/professional-video-applications/import-options\n        #      more docs about import location: https://developer.apple.com/documentation/professional-video-applications/sending-data-programmatically-to-final-cut-pro#Specify-a-Library-for-the-FCPXML-File\n        #   (pay attention to warning on import if there is a mismatch and check the DTD - ordering is a common issue and the warning is not clear!)\n        #   IIUC import-options must come first b/c they affect importing resources (i.e. media files)\n        self.import_options = ET.SubElement(self.root, \"import-options\")\n        ET.SubElement(self.import_options, \"option\", key=\"library location\", value=library_file)\n        # \"copy assets\" to 1 (copy) or 0 (link) => I would prefer to move them here for final storage\n        ET.SubElement(self.import_options, \"option\", key=\"copy assets\", value=\"1\")\n        # \"base url\" is for resolving src ATTR on resource elements # ET.SubElement(self.import_options, \"option\", key=\"base url\", value=\"\")\n        #    FYI relative paths work (w/o base url) if relative to this fcpxml file\n        #    IIAC, \"base url\" lets me use a diff relative base path for resources\n        # \"suppress warnings\" is available, but DO NOT CHANGE IT, I wanna see import failures\n\n        self.resources = ET.SubElement(self.root, \"resources\")\n\n        # library(s) must come after resources\n        #   FYI library's location ATTR doesn't work on imports so I don't have it here (see import-options above)\n        self.library = ET.SubElement(\n            self.root,\n            \"library\",\n        )\n\n        self.event = ET.SubElement(self.library, \"event\", name=\"auto\")\n\n        self.project = ET.SubElement(self.event, \"project\", name=project_name)\n\n        self.format_id = \"r1\"\n        self.sequence = ET.SubElement(self.project, \"sequence\", format=self.format_id)\n        self.spine = ET.SubElement(self.sequence, \"spine\")\n\n        ET.SubElement(\n            self.resources,\n            \"format\",\n            id=self.format_id,\n            name=f\"FFVideoFormat3840x2160p{int(original.fps)}\")\n            # IIRC I can set width/height too and it works? could use source resolution (moviepy video attrs)\n            #         <format id=\"r1\" name=\"FFVideoFormat3840x2160p30\" frameDuration=\"100/3000s\" width=\"3840\" height=\"2160\" colorSpace=\"1-1-1 (Rec. 709)\"/>\n            # currently rec 709 is default it appears\n            # 4k exports beautifully and then let youtube/rumble/etc transcode to 1080p etc...\n            #   plus stills in FCPX come out as 4k resolution if project is 4k (b/c underlying video is 4k, but if project is 1080p then stills are exported as 1080p and not that crisp... 4k is also nice for \"Copy Subject\" in Photos app to make my viseage bigger)\n\n        self.r2_id = \"r2\"\n        self.r2_video = ET.SubElement(\n            self.resources,\n            \"asset\",\n            id=self.r2_id,\n            format=self.format_id,\n        )\n        video_path_relative_to_fcpxml = os.path.relpath(original.filename, output_dir)\n        self.media_rep = ET.SubElement(self.r2_video, \"media-rep\", kind=\"original-media\", src=video_path_relative_to_fcpxml)\n\n    def export_xml(self):\n        with open(fcpxml_file, \"w\") as xml_file:\n            pretty_xml = prettify(self.root)\n            xml_file.write(pretty_xml)\n        print(f\"exported: {fcpxml_file}\")\n\n## /Users/wesdemos/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence_detect/plot_helpers.py:2-46\ndef on_key(event):\n        # existing navigation\n        if event.key == \"left\":\n            cur_xlim = ax.get_xlim()\n            shift = (cur_xlim[1] - cur_xlim[0]) * 0.1\n            ax.set_xlim(cur_xlim[0] - shift, cur_xlim[1] - shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"right\":\n            cur_xlim = ax.get_xlim()\n            shift = (cur_xlim[1] - cur_xlim[0]) * 0.1\n            ax.set_xlim(cur_xlim[0] + shift, cur_xlim[1] + shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"up\":\n            cur_ylim = ax.get_ylim()\n            shift = (cur_ylim[1] - cur_ylim[0]) * 0.1\n            ax.set_ylim(cur_ylim[0] + shift, cur_ylim[1] + shift)\n            fig.canvas.draw_idle()\n        elif event.key == \"down\":\n            cur_ylim = ax.get_ylim()\n            shift = (cur_ylim[1] - cur_ylim[0]) * 0.1\n            ax.set_ylim(cur_ylim[0] - shift, cur_ylim[1] - shift)\n            fig.canvas.draw_idle()\n        # zoom handling\n        elif event.key in (\"+\", \"=\"):  # both '+' and '=' share the same keycode on many keyboards\n            # zoom in by 20 %\n            cur_xlim = ax.get_xlim()\n            cur_ylim = ax.get_ylim()\n            x_center = sum(cur_xlim) / 2\n            y_center = sum(cur_ylim) / 2\n            x_range = (cur_xlim[1] - cur_xlim[0]) * 0.8\n            y_range = (cur_ylim[1] - cur_ylim[0]) * 0.8\n            ax.set_xlim(x_center - x_range / 2, x_center + x_range / 2)\n            ax.set_ylim(y_center - y_range / 2, y_center + y_range / 2)\n            fig.canvas.draw_idle()\n        elif event.key == \"-\":\n            # zoom out by 20 %\n            cur_xlim = ax.get_xlim()\n            cur_ylim = ax.get_ylim()\n            x_center = sum(cur_xlim) / 2\n            y_center = sum(cur_ylim) / 2\n            x_range = (cur_xlim[1] - cur_xlim[0]) / 0.8\n            y_range = (cur_ylim[1] - cur_ylim[0]) / 0.8\n            ax.set_xlim(x_center - x_range / 2, x_center + x_range / 2)\n            ax.set_ylim(y_center - y_range / 2, y_center + y_range / 2)\n            fig.canvas.draw_idle()\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nfrom auto_edit.new_silence_detect.full import *\nfrom auto_edit.new_silence_detect.plot_helpers import setup_events\nfrom auto_edit.tests.test_files import gptoss\nfrom numpy.typing import NDArray\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport rich\nimport sounddevice as sd\n\n# what_file = gptoss.minute1\nwhat_file = gptoss.full\n\n# %%\n\nsilence_threshold = np.float64(-40.0)\nbreath_threshold = np.float64(-22.0)\nanalysis = speech_analysis(what_file, silence_threshold, breath_threshold)\n\nrich.print(f'{analysis.silence_threshold=}')\nrich.print(f'{analysis.breath_threshold=}')\n\nanalysis.merged = merge_close_speech_segments(analysis, max_gap=0.1)\n\n# %%\n\ndef play_merged_segments_by_id_range(start_id, end_id):\n    segments = [s for s in analysis.merged if s.id >= start_id and s.id <= end_id]\n    rich.print(segments)\n    play_segments(analysis, segments)\n\ndef plot_segments(analysis: AudioAnalysisResult, segments: list[DetectedSegment] | None = None) -> None:\n    import matplotlib.pyplot as plt\n\n    # Prepare data for stacked plot\n    segments = segments or analysis.segments\n    segment_starts = [seg.start for seg in segments]\n    segment_ends = [seg.end for seg in segments]\n    segment_labels = [seg.label for seg in segments]\n    segment_rms = [seg.rms_segment for seg in segments]\n\n    # Colors matching the label color map used in DetectedRange.__str__\n    label_color_map = {\n        SegmentLabel.SILENCE: \"blue\",\n        SegmentLabel.BREATH: \"magenta\",\n        SegmentLabel.SPEECH: \"red\",\n    }\n    colors = [label_color_map.get(lbl, \"green\") for lbl in segment_labels]\n    rich.print(colors)\n\n    # * plot line per segment:\n\n    def _play_segment(start: float, end: float) -> None:\n        \"\"\"Play a slice of the audio array.\"\"\"\n        sr = analysis.sample_rate\n        audio = analysis.audio_samples\n        start_idx = int(start * sr)\n        end_idx = int(end * sr)\n        print(f'play {start_idx=} {end_idx=}')\n        segment = audio[start_idx:end_idx]\n        # Ensure the data is in the correct dtype for simpleaudio\n        # audio_bytes = (segment * 32767).astype(np.int16).tobytes()\n        sd.play(segment, analysis.sample_rate)\n        # todo cancel if click elsewhere! which means don't wait here!\n        sd.wait()\n\n    def plot_segments_with_click() -> None:\n        \"\"\"Plot RMS segments and enable click‑to‑play.\"\"\"\n        fig, ax = plt.subplots(figsize=(20, 10))\n\n        for seg, color in zip(segments, colors):\n            times = np.linspace(seg.start, seg.end, len(seg.rms_segment))\n            ax.plot(times, seg.rms_segment, color=color)\n\n        def on_click(event):\n            if event.inaxes != ax:\n                return\n            click_time = event.xdata\n            # Find the first segment that contains the click time\n            for seg in segments:\n                if seg.start <= click_time <= seg.end:\n                    _play_segment(seg.start, seg.end)\n                    break\n\n                setup_events(<|fim_middle|>\n\n\n        plt.xlabel(\"Time (s)\")\n        plt.ylabel(\"Amplitude\")\n        plt.title(\"RMS Segments – click to play\")\n        plt.show()\n\n    plot_segments_with_click()\n\n    # * scatterplots:\n    # # Compute x positions for each sample within each segment\n    # # We'll concatenate all rms samples sequentially, keeping track of segment boundaries\n    # concatenated_rms = np.concatenate(segment_rms)\n    # sample_counts = [len(r) for r in segment_rms]\n    # x_positions = np.arange(len(concatenated_rms))\n    #\n    # # Build a color array matching each sample to its segment's color\n    # sample_colors = np.empty(len(concatenated_rms), dtype=object)\n    # idx = 0\n    # for count, color in zip(sample_counts, colors):\n    #     sample_colors[idx:idx + count] = color\n    #     idx += count\n\n    # plt.scatter(x_positions, concatenated_rms, c=sample_colors, s=2, marker=\"|\")\n\n    # plt.tight_layout()\n    # plt.show()\n\ndef play_time_range(start, end):\n    segs = [s for s in analysis.merged if start < s.end and s.start < end]\n    rich.print(segs)\n    play_segments(analysis, segs)\n\ndef mute_and_play_segments(analysis: AudioAnalysisResult, segments: list[DetectedSegment]) -> None:\n    combined = []\n    for segment in segments:\n        start = int(segment.start * analysis.sample_rate)\n        end = int(segment.end * analysis.sample_rate)\n        if segment.label in (SegmentLabel.SILENCE, SegmentLabel.BREATH):\n            # combined.append(analysis.audio_samples[start:end])\n            # continue\n            last_sample = combined[-1][-1] if combined else 0\n            # cannot drop to zero... must fade down?! wtf?\n            # mute_slice = [-40 for r in segment.rms_segment]\n            mute_slice = [last_sample for r in segment.rms_segment]\n            print(mute_slice)\n            # return\n            # mute_slice = np.full(end - start, -70.0, dtype=analysis.audio_samples.dtype)\n            combined.append(mute_slice)\n        else:\n            combined.append(analysis.audio_samples[start:end])\n    if combined:\n        audio = np.concatenate(combined)\n        print(f'{len(audio)=}')\n        sd.play(audio, analysis.sample_rate)\n        sd.wait()\n\n# %%\n\n# analysis.list_by_label(SegmentLabel.SILENCE)\n\n# for s in analysis.merged:\n#     print(s)\n\n# play_merged_segments_by_id_range(5909, 5911)\n# plot_segments(analysis, analysis.merged[50:100])\n# play_time_range(43.2, 43.6)\n# play_segments(analysis, [s for s in analysis.merged if s.label == SegmentLabel.SILENCE and s.duration < 0.03])\n\n# mute_and_play_segments(analysis, [s for s in analysis.merged if s.start > 7 and s.start < 14])\nplot_segments(analysis, [s for s in analysis.merged if s.start > 6 and s.start < 80])<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 5388,
    "timings": {
      "cache_n": 0,
      "prompt_n": 5212,
      "prompt_ms": 704.292,
      "prompt_per_token_ms": 0.1351289332310054,
      "prompt_per_second": 7400.33963185724,
      "predicted_n": 177,
      "predicted_ms": 860.296,
      "predicted_per_token_ms": 4.860429378531074,
      "predicted_per_second": 205.7431395705664
    }
  }
}

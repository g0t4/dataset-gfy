{
  "stream": true,
  "raw": false,
  "top_p": 1,
  "model": "gpt-oss:120b",
  "temperature": 1,
  "messages": [
    {
      "content": "You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n",
      "role": "developer"
    },
    {
      "content": "Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n## Recent yanks across all files in the project:\n.. yanked from auto_edit/new_silence/ranges_tests.py:\n        win1 = GapWindow(\n            left_speech=speech_seg(0, 1.5),\n            gap=NonSpeechGap(segments=[speech_seg(1.5, 2.1)]),\n            right_speech=speech_seg(2.1, 3),\n        )\n\n\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/gap.py:26-45\ndef gap(windowed: list[_SegmentGroup]):\n\n        left_speech = None\n        if windowed[0] and windowed[0].is_speech:\n            left_speech = windowed[0].segments[0]\n            assert len(windowed[0].segments) == 1, \"Expected only one left speech segment\"\n\n        right_speech = None\n        if windowed[2] and windowed[2].is_speech:\n            right_speech = windowed[2].segments[0]\n            assert len(windowed[2].segments) == 1, \"Expected only one right speech segment\"\n\n        gap_window = GapWindow(\n            left_speech=left_speech,\n            gap=NonSpeechGap(segments=windowed[1].segments),\n            right_speech=right_speech,\n        )\n\n        mark_safe_speech_boundaries(gap_window)\n        return gap_window\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:154-182\n@dataclass\nclass GapWindow:\n    left_speech: DetectedSegment | None\n    gap: NonSpeechGap\n    right_speech: DetectedSegment | None\n\n    first_min_seconds: float | None = None\n    first_min_rms: float | None = None\n    last_min_seconds: float | None = None\n    last_min_rms: float | None = None\n\n    def all_segments(self) -> list[DetectedSegment]:\n        segments: list[DetectedSegment] = []\n        if self.left_speech:\n            segments.append(self.left_speech)\n        segments.extend(self.gap.segments)\n        if self.right_speech:\n            segments.append(self.right_speech)\n        return segments\n\n    def get_window_start_seconds(self) -> float:\n        if self.left_speech:\n            return self.left_speech.start_seconds\n        return self.gap.segments[0].start_seconds\n\n    def get_window_end_seconds(self) -> float:\n        if self.right_speech:\n            return self.right_speech.end_seconds\n        return self.gap.segments[-1].end_seconds\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:152-165\n\n\n@dataclass\nclass GapWindow:\n    left_speech: DetectedSegment | None\n    gap: NonSpeechGap\n    right_speech: DetectedSegment | None\n\n    first_min_seconds: float | None = None\n    first_min_rms: float | None = None\n    last_min_seconds: float | None = None\n    last_min_rms: float | None = None\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:150-152\n@dataclass\nclass NonSpeechGap:\n    segments: list[DetectedSegment]\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/gap.py:8-98\ndef build_gap_decisions(analysis: AudioAnalysisResult):\n\n    @dataclass(frozen=True)\n    class _SegmentGroup:\n        is_speech: bool\n        segments: list[DetectedSegment]\n\n    def grouped(segments):\n        # return GAP(1+ segments), SPEECH(1 segment), GAP(segs), SPEECH(seg), â€¦\n        for is_speech, segments in groupby(segments, key=lambda seg: seg.label == SegmentLabel.SPEECH):\n            yield _SegmentGroup(is_speech=is_speech, segments=list(segments))\n\n    # TODO tests of:\n    # - start is silence (common)\n    # - start is speech (rare but possible)\n    # - end is silence (common)\n    # - end is speech (also rare)\n\n    def gap(windowed: list[_SegmentGroup]):\n\n        left_speech = None\n        if windowed[0] and windowed[0].is_speech:\n            left_speech = windowed[0].segments[0]\n            assert len(windowed[0].segments) == 1, \"Expected only one left speech segment\"\n\n        right_speech = None\n        if windowed[2] and windowed[2].is_speech:\n            right_speech = windowed[2].segments[0]\n            assert len(windowed[2].segments) == 1, \"Expected only one right speech segment\"\n\n        gap_window = GapWindow(\n            left_speech=left_speech,\n            gap=NonSpeechGap(segments=windowed[1].segments),\n            right_speech=right_speech,\n        )\n\n        mark_safe_speech_boundaries(gap_window)\n        return gap_window\n\n    def mark_safe_speech_boundaries(window: GapWindow):\n\n        all_rms_values = np.concatenate([v.rms_values for v in window.gap.segments])\n\n        # * find left_speech's end of taper (using first local minimum found)\n        first_min_idx = 0\n        first_min = all_rms_values[0]\n        for idx, value in enumerate(all_rms_values):\n            if value < first_min:\n                first_min = value\n                first_min_idx = idx  # track separately so I can capture when first_min is first encountered\n                continue\n            if value > first_min + 3:  # jump in 3dB == reversed course, is this sufficient?\n                break\n\n        # * ramp up for right_speech's start (using first local min found, in reverse)\n        last_min_idx_backwards = 0\n        last_min = all_rms_values[-1]\n        for idx, value in enumerate(np.flip(all_rms_values)):\n            if value < last_min:\n                last_min = value\n                last_min_idx_backwards = idx\n                continue\n            if value > last_min + 3:\n                break\n\n        # * mark times\n        hops_per_second = analysis.sample_rate / RMS_HOP_LENGTH\n        gap_start_seconds = window.gap.segments[0].start_seconds\n        #\n        window.first_min_seconds = gap_start_seconds + first_min_idx / hops_per_second\n        window.first_min_rms = all_rms_values[first_min_idx]\n        #\n        # TODO get some tests in place, especially for -1 logic and edge cases\n        last_min_idx_forwards = len(all_rms_values) - 1 - last_min_idx_backwards\n        window.last_min_seconds = gap_start_seconds + last_min_idx_forwards / hops_per_second\n        window.last_min_rms = all_rms_values[last_min_idx_forwards]\n\n        # PRN breathing checks? to decide if I want to silence or not?\n        # _100ms = 0.1\n        # _400ms = 0.4\n        # minimums_gap = last_min - first_min\n        # if minimums_gap > _100ms and minimums_gap < _400ms:\n        #     # AND if rises up and down\n        #     #  then assume is breathing and just be careful about rounding inward... for now I don't think this is even needed though!\n\n    windows = [\n        gap(w) for w in windowed(grouped(analysis.merged_segments), 3) \\\n            if w[1] and not w[1].is_speech\n    ]\n\n    return windows\n",
      "role": "user"
    },
    {
      "content": "Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nimport numpy as np\nimport pytest\nfrom auto_edit.new_silence.detects.segments import VIDEO_FPS, DetectedSegment, GapWindow, NonSpeechGap, SegmentLabel\nfrom auto_edit.new_silence.ranges import get_ranges\n\nid_counter = 0\n\n@pytest.fixture(autouse=True)\ndef reset_id_counter():\n    # ensure the module variable is reset\n    import importlib\n    import auto_edit.new_silence.detect_tests as ft\n    importlib.reload(ft)\n\ndef silence_seg(start, end):\n    global id_counter\n    seg = DetectedSegment(\n        id=id_counter,\n        start_seconds=start,\n        end_seconds=end,\n        label=SegmentLabel.SILENCE,\n        rms_values=np.array([id_counter]),\n    )\n    id_counter += 1\n    return seg\n\ndef speech_seg(start, end):\n    global id_counter\n    seg = DetectedSegment(\n        id=id_counter,\n        start_seconds=start,\n        end_seconds=end,\n        label=SegmentLabel.SPEECH,\n        rms_values=np.array([id_counter]),  # make the samples identifiable is all I need for now\n    )\n    id_counter += 1\n    return seg\n\nclass TestNewSilenceToRanges:\n\n    # TODO what if only partial gap_window? (i.e. leading silence at start of video, or end, plus then at start you have speech after, or at end speech is only before)\n\n    def test_one_window(self):\n        win = GapWindow(\n            left_speech=speech_seg(0, 1.5),\n            gap=NonSpeechGap(segments=[speech_seg(1.5, 2.1)]),\n            right_speech=speech_seg(2.1, 3),\n        )\n        wins = [win]\n        ranges = get_ranges(wins, VIDEO_FPS)\n\n        # assert len(ranges) == 3\n        assert ranges[0].start_frame == 0\n        assert ranges[0].end_frame == 45\n        assert ranges[1].start_frame == 45\n        assert ranges[1].end_frame == 63\n        assert ranges[2].start_frame == 63\n        assert ranges[2].end_frame == 90\n\n    def test_two_back_to_back_windows(self):\n        win1 = GapWindow(\n            left_speech=speech_seg(0, 1.5),\n            gap=NonSpeechGap(segments=[speech_seg(1.5, 2.1)]),\n            right_speech=speech_seg(2.1, 3),\n        )\n        win2 = GapWindow(\n            # remember gaps overlap => sliding by two at a time basically\n            left_speech=speech_seg(2.1, 3),  # same as prior right_speech\n            gap=NonSpeechGap(segments=[speech_seg(3, 5.1)]),\n            right_speech=speech_seg(5.1, 6),\n        )\n        wins = [win1, win2]\n        ranges = get_ranges(wins, VIDEO_FPS)\n        # first window\n        assert ranges[0].start_frame == 0\n        assert ranges[0].end_frame == 45\n        assert ranges[1].start_frame == 45\n        assert ranges[1].end_frame == 63\n\n        assert ranges[2].start_frame == 63\n        assert ranges[2].end_frame == 90\n        assert ranges[3].start_frame == 90\n        assert ranges[3].end_frame == 153\n\n        # this is the edge case at the end (if no silence after, then need to take this from last window)\n        assert ranges[4].start_frame == 153\n        assert ranges[4].end_frame == 180\n\n    def test_taper_out_and_in(self):\n        # TODO frame rounding tests here too\n        win1 = GapWindow(\n            left_speech=speech_seg(0, 1.5),\n            gap=NonSpeechGap(segments=[speech_seg(1.5, 2.1)]),\n            right_speech=speech_seg(2.1, 3),\n\n            <|fim_middle|>\n        )\n\n    def test_window_without_left_speech(self):\n        win = GapWindow(\n            left_speech=None,\n            gap=NonSpeechGap(segments=[speech_seg(0, 1)]),\n            right_speech=speech_seg(1, 2),\n        )\n        wins = [win]\n        ranges = get_ranges(wins, VIDEO_FPS)\n        # silence at the start\n        assert ranges[0].start_frame == 0\n        assert ranges[0].end_frame == 30\n        # following speech segment\n        assert ranges[1].start_frame == 30\n        assert ranges[1].end_frame == 60\n\n    def test_window_without_right_speech(self):\n        win = GapWindow(\n            left_speech=speech_seg(0, 1),\n            gap=NonSpeechGap(segments=[speech_seg(1, 2)]),\n            right_speech=None,\n        )\n        wins = [win]\n        ranges = get_ranges(wins, VIDEO_FPS)\n        # speech at the start\n        assert ranges[0].start_frame == 0\n        assert ranges[0].end_frame == 30\n        # following silence segment\n        assert ranges[1].start_frame == 30\n        assert ranges[1].end_frame == 60\n\n    def test_silence_at_start_of_video(self):\n        # FYI I will want to add this to the build_gap_decisions b/c the same logic for first/last_min will apply\n        # TODO? add another window at start w/\n        #   left_speech=None\n        #   + start silence == first gap\n        #   + right_speech == first speech segment\n        pass\n\n    def test_silence_at_end_of_video(self):\n        # FYI I will want to add this to the build_gap_decisions b/c the same logic for first/last_min will apply\n        # TODO? add another window at end w/\n        #    left_speech == last speech segment\n        #    + end silence == final gap\n        #    + right_speech = None\n        pass\n\n    def test_only_one_speech_segment(self):\n        # NO gap before/after... that's a problem for my representation choices right?\n        #    I can't do window w/o the gap... unless I set an empty gap?\n        # either left or right for speech is fine\n        pass",
      "role": "user"
    }
  ],
  "options": [],
  "max_tokens": 4096,
  "chat_template_kwargs": {
    "reasoning_effort": "low"
  }
}

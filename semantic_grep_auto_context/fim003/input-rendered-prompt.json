{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765796139,
  "id": "chatcmpl-wwX78mhB3AUzdKVKA0p4ngK4cjdeK9Lw",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 1628,
    "prompt_n": 1726,
    "prompt_ms": 288.515,
    "prompt_per_token_ms": 0.1671581691772885,
    "prompt_per_second": 5982.357936329134,
    "predicted_n": 102,
    "predicted_ms": 506.518,
    "predicted_per_token_ms": 4.965862745098039,
    "predicted_per_second": 201.3748771020971
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 2,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 102,
    "tokens_evaluated": 3354,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-15\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n## Recent yanks across all files in the project:\n.. yanked from generate_fcpxml_notebook.py:\n        name = f\"content{content.start_frame}-{content.end_frame} / {fcpxml.show_seconds(content.start_frame)}-{fcpxml.show_seconds(content.end_frame)}\"\n        if content.special_note: \n            name += f\" {content.special_note}\"\n        content_clip_xml = ET.SubElement(\n            fcpxml.spine,\n            \"asset-clip\",\n            name=name,\n            ref=\"r2\",\n            start=fcpxml.frames_to_fraction(content.start_frame),  # start time w/in the media asset itself\n            duration=fcpxml.frames_to_fraction(content.duration_frames()),\n            audioRole=\"dialogue\",\n            # DO NOT SET offset, that caused timing issues, FCPX computes it from duration of ordered clips \n        )\n\n.. yanked from generate_fcpxml_notebook.py:\n        name = f\"content{content.start_frame}-{content.end_frame} / {fcpxml.show_seconds(content.start_frame)}-{fcpxml.show_seconds(content.end_frame)}\"\n        if content.special_note: \n            name += f\" {content.special_note}\"\n        content_clip_xml = ET.SubElement(\n            fcpxml.spine,\n            \"asset-clip\",\n            name=name,\n            ref=\"r2\",\n            start=fcpxml.frames_to_fraction(content.start_frame),  # start time w/in the media asset itself\n            duration=fcpxml.frames_to_fraction(content.duration_frames()),\n            audioRole=\"dialogue\",\n            # DO NOT SET offset, that caused timing issues, FCPX computes it from duration of ordered clips \n        )\n\n\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/ranges.py:62-63\ndef __str__(self):\n        return f\"Content[{self.start_frame}➜{self.end_frame})\"\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/ranges.py:54-69\nclass ContentRange(FrameRange):\n\n    transcriptions: list[WhisperSegment]\n\n    def __init__(self, start_frame: int, end_frame: int):\n        super().__init__(start_frame, end_frame)\n        self.transcriptions = []\n\n    def __str__(self):\n        return f\"Content[{self.start_frame}➜{self.end_frame})\"\n\n    def __copy__(self):\n        return ContentRange(self.start_frame, self.end_frame)\n\n    def __deepcopy__(self, memo=None):\n        return ContentRange(self.start_frame, self.end_frame)\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/generate_fcpxml.py:541-560\n\n    if isinstance(r, ContentRange):\n        content: ContentRange = r\n\n        print(f\"offset: {spine_offset_frames}, duration: {content.duration_frames()}\")\n\n        name = f\"content{content.start_frame}-{content.end_frame} / {show_seconds(content.start_frame)}-{show_seconds(content.end_frame)}\"\n        if hasattr(content, \"orig_timing\"):\n            name += f\" o:{content.orig_timing}\"\n        content_clip_xml = ET.SubElement(\n            fcpxml.spine,\n            \"asset-clip\",\n            # FYI info panel name field auto expands if text overflows one line (like note below)\n            name=name,\n            ref=\"r2\",\n            # offset=frames_to_fraction(spine_offset_frames),  # within the spine ==> duration of all previous clips in spine, so if 3 seconds of clips before this one then offset=3s\n            # offset != start unless not cutting anything nor adding anything before this clip\n            start=frames_to_fraction(content.start_frame),  # start time w/in the media asset itself, so if this was originally 20 seconds to 30 seconds, then start=20s\n            duration=frames_to_fraction(content.duration_frames()),  # duration of this clip in the timeline, so if this was originally 20 seconds to 30 seconds, then duration=10s\n            audioRole=\"dialogue\",\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/ranges.py:68-69\ndef __deepcopy__(self, memo=None):\n        return ContentRange(self.start_frame, self.end_frame)\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/fcpxml/builder.py:125-139\ndef add_content(self, content: ContentRange) -> ET.Element:\n        name = f\"content{content.start_frame}-{content.end_frame} / {self.show_seconds(content.start_frame)}-{self.show_seconds(content.end_frame)}\"\n        if content.special_note:\n            name += f\" {content.special_note}\"\n        content_clip_xml = ET.SubElement(\n            self.spine,\n            \"asset-clip\",\n            name=name,\n            ref=\"r2\",\n            start=self.frames_to_fraction(content.start_frame),  # start time w/in the media asset itself\n            duration=self.frames_to_fraction(content.duration_frames()),\n            audioRole=\"dialogue\",\n            # DO NOT SET offset, that caused timing issues, FCPX computes it from duration of ordered clips\n        )\n        return content_clip_xml\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\n            #\n            #      DO NOT ADD ANYTHING FANCY WITHOUT UNIT TESTS OF THE MERGING BEHVAIOR\n            #\n            #      when processing speeches split up\n            #        start of speech\n            #        vs end of speech\n            # !!! LEFT OFF HERE... WES YOU MUST GET STARTED ON BASH ASAP ... YOUTUBE CAN WAIT... YOU CAN HAVE xMAS VIDEOS!!! THAT IS FINE... you need bash done by year's end!!!!\n            #    HOW ABOUT EDIT the gptoss video with AS-IS steps w/o the new above cycles and roundign frames... and see what all is painful that i missed... that might be wise so I am not missing something that would obviate what I think I need to do next!!!\n            #      if it is painful you can stop and come back to it NEXT WEEK\n            if seg.label == SegmentLabel.SPEECH:\n                content = ContentRange(start_frame, end_frame)\n                content.special_note = f\"o:{seg.start:.3f}s-{seg.end:.3f}s\"\n                all_ranges.append(content)\n            else:\n                # FYI might also be a mid range button press... see code above that removes it\n                all_ranges.append(SilenceRange(start_frame, end_frame))\n        all_ranges = sorted(all_ranges)\n\n        rich.print(\"[bold green]NEW[/]\", all_ranges)\n\n# %% * transcribe\n\ninclude_transcriptions = False\n\nif include_transcriptions:\n    from auto_edit.transcribe import transcribe_audio_for_content_range, ffmpeg_load_resampled_audio_data\n\n    # ??? use new_silence's librosa audio samples? likely can resample too (IIRC it is loading 22kHz sample rate)\n    ffmpeg_resampled_audio_data = ffmpeg_load_resampled_audio_data(which_video)\n    # print(f\"audio_samples: {ffmpeg_resampled_audio_data.shape[0]} samples, type: {ffmpeg_resampled_audio_data.dtype}\")\n\n    for c in content_ranges:\n        print(c)\n        transcribe_audio_for_content_range(c, ffmpeg_resampled_audio_data, original.fps)\n\n# %%\n\ninclude_suggests = False\nif include_suggests:\n    # TODO try with gptoss with high reasoning level!?\n    id = 0\n    segments = \"Here are the lines, the line number is at the start of the line followed by a colon (i.e. '0:'), by the way some lines have multiple segments so consider them all-or-none for removal:\"\n    for c in content_ranges:\n        c.id = id\n        id += 1\n        result = \"\"\n        for t in c.transcriptions:\n            result += f\"{t.text} \"\n        segments += f\"\\n{c.id}: {result}\"\n\n    print(segments)\n\n    from openai import OpenAI\n    from auto_edit.ai.copied_services import use_openai\n\n    use = use_openai()\n    client = OpenAI(api_key=use.api_key, base_url=use.base_url)\n\n    try:\n        completion = client.chat.completions.create(\n            model=use.model,\n            messages=[{\n                \"role\": \"system\",\n                \"content\": \"You are a video editing expert, I need you to suggest some edits to make based on my transcript of my video I am editing. I just need you to suggest which lines have a retake on them that should be removed. By the way each line was transcribed based on splitting the clip on silence periods (i.e. breathing, pauses) so a single sentence/thought can be split across multiple lines, that is normal, do not remove lines b/c they look incomplete. In you response, list each number on a its own line with a colon and then a concise reason why to remove it (5 words max and fewer is preferred)... i.e. '1: retake\\n4: retake\\n\"\n            }, {\n                \"role\": \"user\",\n                \"content\": f\"{segments}\"\n            }],\n            max_tokens=200,\n            n=1  # default\n        )\n\n        response = completion.choices[0].message.content\n        print(response)\n\n    except Exception as e:\n        print(f\"{e}\")\n\n    lines_to_remove = response.split(\"\\n\")\n    dict_lines_to_remove = {}\n    for l in lines_to_remove:\n        parts = l.split(\":\")\n        if len(parts) == 2:\n            dict_lines_to_remove[int(parts[0])] = parts[1]\n\n    # if segment is removed, then remove its silence period AFTER but not BEFORE it\n    # - b/c silence after mostly correlates to prior segement (i.e. trailing letters)\n    # - whereas cuts almost always work well at very start of next segment b/c segments start abruptly unlike how they end\n\nelse:\n    dict_lines_to_remove = {}\n\n# %%\n\nimport xml.etree.ElementTree as ET\n\nfcpxml = FcpXml(original, library_file, project_name, output_dir, fcpxml_file)\n\nfirst_silence = all_ranges[0] if isinstance(all_ranges[0], SilenceRange) else None\nfor r in all_ranges:\n    rich.print(r)\n    if r.duration_frames() <= 0:\n        # TODO move out to a health check of sorts, keep out of the FCPXML gen code\n        # TODO this SHOULD ALSO NOT really be happening... my logic for creating segments needs to adjust for this and not just wipe out some ranges\n        rich.print(\"  [bold #000000 on #ffd700]skipping zero duration segment (range)[/]\")\n        continue\n\n    if isinstance(r, ContentRange):\n        content = r\n\n        <|fim_middle|>\n\n\n        for t in content.transcriptions:\n            whisper: WhisperSegment = t\n            fcpxml.use_lane1 = not fcpxml.use_lane1\n\n            # shift transcription timing to avoid start/end of clip (makes editing a PITA)\n            # fyi NOT handling if this happens on the boundary of a future ContentRange/SilenceRange \n            start_frame = whisper.start_frame + content.start_frame\n            if (whisper.start_frame == 0):\n                start_frame += 1\n            duration_frames = whisper.duration_frames()\n            end_frame = start_frame + duration_frames\n            if end_frame == content.end_frame:\n                if duration_frames > 1:\n                    rich.print(\"[black on yellow bold]  TODO TEST ME... FOUND whisper.end_frame == content.end_frame, so I adjusted the duration down a frame! Remove this comment if it looks ok.[/]\")\n                    duration_frames -= 1\n                else:\n                    rich.print(\"   [white on red bold]FOUND whisper.end_frame == content.end_frame, but transcription is not big enough to decrement the frame count[/]\")\n                    # FYI could add one but then you might overlap with end of video... that said, shifting right by one can do that too!\n                    \n            caption_xml = ET.SubElement(\n                content_clip_xml,\n                \"caption\",\n                lane=\"1\" if fcpxml.use_lane1 else \"2\",  # PRN alternate lane1 / lane2 to avoid collisions (overlap)\n                offset=f\"{fcpxml.frames_to_fraction(start_frame)}\",  # offset is the position in original media timeline\n                # DO NOT SET start (IIGC start here would also mean offset into a media file, and captions don't have media files)\n                duration=f\"{fcpxml.frames_to_fraction(duration_frames)}\",\n                role=\"iTT?captionFormat=ITT.en\",\n            )\n            text = ET.SubElement(caption_xml, \"text\", placement=\"bottom\")\n            text.text = whisper.text\n\n        if include_suggests and content.id in dict_lines_to_remove:\n            # leave red marker if openai suggested to remove the item, can easily select and drag to delete them until I can be confident it won't delete unnecessary clips... this is valuable alone!\n            ET.SubElement(\n                content_clip_xml,\n                \"marker\",\n                completed=\"0\",\n                start=f\"{fcpxml.frames_to_fraction(content.start_frame)}\",\n                duration=\"1s\",\n                value=f\"{dict_lines_to_remove[content.id]}\",\n            )\n\n    if isinstance(r, SilenceRange):\n        fcpxml.add_silence(r, r == first_silence)\n\nfcpxml.export_xml()\n<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 3455,
    "timings": {
      "cache_n": 1628,
      "prompt_n": 1726,
      "prompt_ms": 288.515,
      "prompt_per_token_ms": 0.1671581691772885,
      "prompt_per_second": 5982.357936329134,
      "predicted_n": 102,
      "predicted_ms": 506.518,
      "predicted_per_token_ms": 4.965862745098039,
      "predicted_per_second": 201.3748771020971
    }
  }
}

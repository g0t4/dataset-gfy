{
  "chat_template_kwargs": {
    "reasoning_effort": "low"
  },
  "max_tokens": 4096,
  "raw": false,
  "messages": [
    {
      "role": "developer",
      "content": "You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n"
    },
    {
      "role": "user",
      "content": "Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Python Code Preferences\n\nNaming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\nType Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n\n## Recent yanks across all files in the project:\n.. yanked from auto_edit/new_silence/prober.py:\ndef ensure_no_timing_offsets(video_path: Path) -> None:\n\n.. yanked from auto_edit/new_silence/prober.py:\ndef ensure_no_timing_offsets(video_path: Path) -> None:\n    ffprobe_data = _run_ffprobe(video_path)\n    rich.inspect(ffprobe_data, title=str(video_path))\n    if _has_nonzero_start(ffprobe_data):\n        raise ValueError(f\"Video {video_path} has nonâ€‘zero start time in container or a stream\")\n\n\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/segments.py:184-193\n@dataclass\nclass AudioAnalysisResult:\n    audio_samples: MonoAudioSamplesArray\n    sample_rate: int | float\n    rms_db: Float32Array\n    time_stamps: Float32Array\n    thresholds: DetectThresholds\n    merged_segments: list[DetectedSegment] = field(default_factory=list)\n    windows: list[GapWindow] = field(default_factory=list)\n    fps: int = VIDEO_FPS\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/gaps_tests.py:349-363\n        #      b/c max fails on slice of no items\n        #\n        analysis = compute_speech_boundaries( \\\n            left_speech = ONE_SECOND_at_MINUS20(),\n            gap = start(-20)\n                .down(588), # 20th frame\n            right_speech = ONE_SECOND_at_MINUS20(),\n        )\n        window = analysis.windows[0]\n        assert window.first_min\n        assert window.first_min._testing_frame_before == 49\n        assert window.first_min._testing_frame_after == 50\n        assert window.first_min.frame_aligned_at_seconds == pytest.approx(50 / 30)\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/gaps_tests.py:15-28\nfrom numpy.typing import NDArray\nfrom auto_edit.new_silence.detects.segments import DEFAULT_THRESHOLDS, AudioAnalysisResult, DetectThresholds, DetectedSegment, GapWindow, LoadedAudio, NonSpeechGap, SegmentLabel\nfrom auto_edit.new_silence.gaps import build_gap_decisions\nfrom dataclasses import dataclass\n\n# * turn on debug if issues with segment values, better yet, use plot()\nDEBUG = False\n\n@dataclass\nclass SegmentBuilderForTesting:\n    values: NDArray[np.float32]\n    debug_me: bool = False\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/gaps_tests.py:114-128\n    )\n    right_seg = DetectedSegment(\n        id=3,\n        start_seconds=gap_seg.end_seconds,\n        end_seconds=gap_seg.end_seconds + len(right_speech.values) / analysis.hops_per_second(),\n        label=SegmentLabel.SPEECH,\n        rms_values=right_speech.values,\n    )\n    analysis.merged_segments = [left_seg, gap_seg, right_seg]\n\n    # currently this is a helper that builds a single window\n    analysis.windows = build_gap_decisions(analysis)  # PRN return analysis\n    assert len(analysis.windows) == 1  # all tests assume this b/c I only pass segments for one gap, so, validate that here!\n    return analysis\n\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/gaps_tests.py:71-127\ndef compute_speech_boundaries(\n        left_speech: SegmentBuilderForTesting,\n        gap: SegmentBuilderForTesting,\n        right_speech: SegmentBuilderForTesting,\n        up_db=3,  # FYI tests are designed with up_db=3 except a few that set 2 explicitly\n):\n    TEST_THRESHOLDS = DetectThresholds(\n        # set these here so I can change them in prod code and the tests still work with these values\n        # > -22 == speech segments\n        breath=np.float32(-22.0),\n        # > -40 == breathe segments (possible breathing, also keystrokes)\n        silence=np.float32(-40.0),\n        # < -40 == silence segment\n    )\n\n    # FYI fill in more values only as needed...\n    empty_timestamps = np.array([], dtype=np.float32)\n    empty_audio = np.array([], dtype=np.float32)\n    analysis = AudioAnalysisResult(\n        audio=LoadedAudio(\n            samples=empty_audio,\n            samples_per_second=22050,\n        ),\n        rms_db=np.concatenate([left_speech.values, gap.values, right_speech.values]),\n        time_stamps=empty_timestamps,\n        thresholds=TEST_THRESHOLDS,\n        hop_length=25,  # DO NOT REMOVE HARD CODED 25 for test cases\n    )\n\n    # * build merged segments for one gap\n    left_seg = DetectedSegment(\n        id=1,\n        start_seconds=0,\n        end_seconds=len(left_speech.values) / analysis.hops_per_second(),\n        label=SegmentLabel.SPEECH,\n        rms_values=left_speech.values,\n    )\n    gap_seg = DetectedSegment(\n        id=2,\n        start_seconds=left_seg.end_seconds,\n        end_seconds=left_seg.end_seconds + len(gap.values) / analysis.hops_per_second(),\n        label=SegmentLabel.SILENCE,\n        rms_values=gap.values,\n    )\n    right_seg = DetectedSegment(\n        id=3,\n        start_seconds=gap_seg.end_seconds,\n        end_seconds=gap_seg.end_seconds + len(right_speech.values) / analysis.hops_per_second(),\n        label=SegmentLabel.SPEECH,\n        rms_values=right_speech.values,\n    )\n    analysis.merged_segments = [left_seg, gap_seg, right_seg]\n\n    # currently this is a helper that builds a single window\n    analysis.windows = build_gap_decisions(analysis)  # PRN return analysis\n    assert len(analysis.windows) == 1  # all tests assume this b/c I only pass segments for one gap, so, validate that here!\n    return analysis\n"
    },
    {
      "role": "user",
      "content": "Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nimport numpy as np\nimport rich\nimport scipy.signal as sps\nimport sounddevice as sd\n\ndef dump_ndarray_info(what: str, arr: np.ndarray | np.floating) -> None:\n    print(f\"{what} type={type(arr)}, dtype={arr.dtype}, shape={arr.shape}\")\n\ndef derive_percentile_based_thresholds(db: Float32Array) -> DetectThresholds:\n    # ?? try diff levels for better detection of talking vs not\n    quiet: np.float32 = np.percentile(db, 20)  # lower-energy (breathing, silence)\n    mid: np.float32 = np.percentile(db, 60)  # typical speech\n\n    # Silence is deeper than \"quiet\"\n    silence = quiet - 6  # ~6 dB below quiet zone\n    breath = (quiet + mid) / 2\n    rich.print(f'{silence=} {breath=}')\n\n    return DetectThresholds(silence=silence, breath=breath)\n\ndef group_and_label_segments(analysis: AudioAnalysisResult) -> list[DetectedSegment]:\n\n    labels: list[SegmentLabel] = []\n    for v in analysis.rms_db:\n        if v < analysis.thresholds.silence:\n            labels.append(SegmentLabel.SILENCE)\n        elif v < analysis.thresholds.breath:\n            labels.append(SegmentLabel.BREATH)\n        else:\n            labels.append(SegmentLabel.SPEECH)\n\n    segments: list[DetectedSegment] = []\n    start_time = analysis.time_stamps[0]\n    start_idx = 0\n    cur_label = labels[0]\n\n    id = 1\n    for i in range(1, len(labels)):\n        if labels[i] != cur_label:\n            end_time = analysis.time_stamps[i]\n            end_idx = i\n            rms_values = analysis.rms_db[start_idx:end_idx]\n            segments.append(DetectedSegment(id, start_time, end_time, cur_label, rms_values))\n            id = id + 1\n            start_time = analysis.time_stamps[i]\n            start_idx = i\n            cur_label = labels[i]\n\n    rms_values = analysis.rms_db[start_idx:]\n    segments.append(DetectedSegment(id, start_time, analysis.time_stamps[-1], cur_label, rms_values))\n    return segments\n\nclass AnalysisParams:\n    frame_length = 1024\n    hop_length = 25\n\ndef full_analysis(path: Path) -> AudioAnalysisResult:\n    with DurationPrinter(\"load_audio\"):\n        # <400ms for both 22050 resample vs 48K raw (resample took about 20ms more, naturally)\n        audio = load_audio(path)\n\n    params = AnalysisParams()  # defaults are fine\n    analysis = create_analysis(audio, params)\n    analysis.path = path\n    return analysis\n\ndef create_analysis(audio, params: AnalysisParams):\n    with DurationPrinter(\"build_segments\"):\n        # ~600ms (22050), ~1200ms (48000) - linear w/ sample rate\n        analysis, segments = build_rms_segments(audio, params)\n    with DurationPrinter(\"merge_close_speech_segments\"):\n        # 0.06ms (22050, 0.15ms (48K) - linear w/ sample rate\n        analysis.merged_segments = merge_close_speech_segments(segments, max_gap_ms=analysis.max_gap_ms)\n    with DurationPrinter(\"gaps\"):\n        # 3.8ms (22050), 9.7ms (48K) - slightly above linear w/ sample rate\n        analysis.windows = build_gap_decisions(analysis)\n    return analysis\n\ndef load_audio(path: Path):\n    # check for non zero start\n    <|fim_middle|>\n\n\n\n    audio_samples: MonoAudioSamplesArray  # aka \"y\"\n    sample_rate: SampleRate\n    #   sample_rate=22050 is default\n    #     IIUC 22050 is b/c librosa was primarily used for musical analysis\n    #   * my video files are 48K\n    #     over longer videos rounding to 22050 will manifest in off by a frame in longer videos\n    #     plus it may alter samples enough to cause issues in detecting silences\n    #     IF perf is an issue, try half the actual file's audio rate, thus 24K (not 22.050K) for my videos\n    #   FYI sampling of my Movies files:\n    #     99% of my files are recorded at 48K (mp4/mkv)\n    #     a few have 44.1K (rare) (one mp4, several mov)\n    #\n    audio_samples, sample_rate = librosa.load(str(path), mono=True, sr=None)\n    return LoadedAudio(samples=audio_samples, samples_per_second=sample_rate)\n\ndef build_rms_segments(audio: LoadedAudio, params: AnalysisParams) -> tuple[AudioAnalysisResult, list[DetectedSegment]]:\n\n    # ** RMS params\n    # TODO! investigate adjust (down?) FRAME_LEN\n    #  FYI hop_length/sample_rate = ms per RMS dB sample\n    #  25 ~= 1.134ms per RMS dB sample (~29.4 per video frame @ 30FPS)\n    #  72 ~= 3.265ms (10.11 per video frame)\n    # 128 ~= 5.80ms (5.68 per video frame)\n    # 256 ~= 11.61ms (2.84 per video frame)\n    # 512 ~= 23.22ms (1.42 per video frame)\n    # FYI video FPS = 30 ==> 33.333ms per frame (at least want to sample once per video frame)\n    #   therefore hop_length=734.999265 (once per frame)\n\n    # PRN try spectrogram for better energy windowing? (vs currently using audio samples)\n    # this was mentioned in librosa docs IIRC\n    #   `rms(S=...)` for spectrogram vs audio samples usings `y=`\n    rms = librosa.feature.rms(y=audio.samples, frame_length=params.frame_length, hop_length=params.hop_length)[0]\n    # shape is (1,t) hence [0] after to \"unwrap\" that dimension\n\n    # if analysis.use_medfilt_smoothing:\n    #     rms = sps.medfilt(rms, kernel_size=5)\n    #     # I removed this b/c it eats away at dips in breathing and other extremes (and bigger frame_len already does that too)\n\n    rms_db = librosa.amplitude_to_db(rms, ref=np.max)\n    times = librosa.frames_to_time(np.arange(len(rms)), sr=audio.samples_per_second, hop_length=params.hop_length)\n\n    analysis = AudioAnalysisResult(\n        audio=audio,\n        rms_db=rms_db,\n        time_stamps=times,\n        thresholds=DEFAULT_THRESHOLDS,\n        hop_length=params.hop_length,\n        frame_length=params.frame_length,\n    )\n\n    segments = group_and_label_segments(analysis)\n\n    return analysis, segments\n\ndef merge_close_speech_segments(segments: list[DetectedSegment], max_gap_ms: float) -> list[DetectedSegment]:\n    max_gap_seconds = max_gap_ms / 1000.0\n\n    merged: list[DetectedSegment] = []\n    current_idx = 0\n    # * copy until first silence\n    while current_idx < len(segments):\n        seg = segments[current_idx]\n        if seg.label == SegmentLabel.SPEECH:\n            break\n        merged.append(seg)\n        current_idx += 1\n\n    if current_idx >= len(segments):\n        return merged\n\n    def get_next_speech(start_index) -> tuple[DetectedSegment | None, int | None]:\n        for idx in range(start_index, len(segments)):\n            if segments[idx].label == SegmentLabel.SPEECH:\n                return segments[idx], idx\n        return None, None\n\n    # * on first silence"
    }
  ],
  "stream": true,
  "temperature": 1,
  "options": [],
  "top_p": 1,
  "model": "gpt-oss:120b"
}

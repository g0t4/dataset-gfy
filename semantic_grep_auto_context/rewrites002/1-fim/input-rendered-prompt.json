{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765583032,
  "id": "chatcmpl-EJTYAXQ0pCEADUgMXhnDQ9gmd7z6j4AL",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 1072,
    "prompt_n": 2110,
    "prompt_ms": 303.498,
    "prompt_per_token_ms": 0.14383791469194313,
    "prompt_per_second": 6952.269866687755,
    "predicted_n": 127,
    "predicted_ms": 553.895,
    "predicted_per_token_ms": 4.361377952755905,
    "predicted_per_second": 229.28533386291625
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 3,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 127,
    "tokens_evaluated": 3182,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-12\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n## Recent yanks across all files in the project:\n.. yanked from auto_edit/new_silence/detector_notebook.py:\nspeech_analysis\n\n.. yanked from auto_edit/new_silence/detector_notebook.py:\n    analysis = speech_analysis(what_file)\n    # hop_length=25 => 1.114s vs hop_length=256 => 608.54ms ... NBD either way\n\nwith DurationPrinter(f\"merge_close_speech_segments\"):\n    analysis.merged = merge_close_speech_segments(analysis, max_gap=0.1)\n\n\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detector_notebook.py:0-19\nfrom auto_edit.new_silence.detect import *\nfrom auto_edit.new_silence.player import SegmentPlotter, play_segments\nfrom auto_edit.new_silence.timer import DurationPrinter\nfrom auto_edit.tests.test_files import gptoss_fim_video\nfrom numpy.typing import NDArray\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport rich\nimport sounddevice as sd\nimport time\n\nwhat_file = gptoss_fim_video.minute1\n\n# %%\n\ntitle_addon = f\"rms(frame_len={RMS_FRAME_LENGTH},hop_len={RMS_HOP_LENGTH},medfilt={USE_MEDFILT_SMOOTHING})\"\n\nwith DurationPrinter(f\"speech_analysis with {title_addon}\"):\n    analysis = speech_analysis(what_file)\n    # hop_length=25 => 1.114s vs hop_length=256 => 608.54ms ... NBD either way\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/tests/aligned-tests.py:23-42\n\n\ngptoss_fim_examples = test_videos_dir / '01-gpt-oss-fim'\n\ngptoss = SimpleNamespace(\n    full=gptoss_fim_examples / \"full.mp4\",\n    split_breathing=gptoss_fim_examples / \"breathing-split-silence.mp4\",\n)\n\nclass TestIntegration:\n\n    # TODO! add more integration tests\n\n    def test_detect_split_breathing_into_two_silences(self):\n        # TODO find how I want to fix this and add that to finish this test\n\n        # ffmpeg -i full.mp4 -ss 00:00:09.0 -to 00:00:12.0 breathing-split-silence.mp4\n        # FYI test w/o shifting timing and messing up the split:\n        #   video_editing_gen_fcpxml first30redone.mp4\n\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/samples.py:0-19\nimport os\n\n\n# dir for samples\nHOME = os.getenv('HOME')\nAUTO_EDIT_DIR = HOME + 'dbox/auto-edit-files'\nORIGINALS_DIR = f\"{AUTO_EDIT_DIR}/originals-unedited\"\n\n\n# export files under one object:\nclass files:\n    COPILOT_STREAMDECK_BUTTON_FULL = f\"{ORIGINALS_DIR}/copilot-streamdeck-parts/full.mp4\"\n    COPILOT_STREAMDECK_BUTTON_FIRST30 = f\"{ORIGINALS_DIR}/copilot-streamdeck-parts/first30seconds.mp4\"\n    DOIT01_RETAKE6X = f\"{ORIGINALS_DIR}/doit01/retake6x-30s.mp4\"\n    DOIT01_RETAKE6X_WAV = f\"{ORIGINALS_DIR}/doit01/retake6x-30s.mp4.wav\"\n    DOIT01_FULL = f\"{ORIGINALS_DIR}/doit01/full.mp4\"\n\n    RETIME_LONG_SILENT_START_and_END_24s = f\"{ORIGINALS_DIR}/retime/first24s-long-silence-start-and-end.mp4\"\n    RETIME_LONG_SILENT_START_FULL = f\"{ORIGINALS_DIR}/retime/full.mp4\"\n\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/generate_fcpxml.py:0-13\n#!/usr/bin/env python\n# coding: utf-8\n\nimport rich\nimport os\nimport sys\nfrom pathlib import Path\nfrom auto_edit.silences.align import align_silences_to_frame_boundaries_asymmetric_pad_WIP\nfrom auto_edit.silences.detect_raw import ffmpeg_detect_silence_raw\nfrom auto_edit.ranges import ContentRange, SilenceRange\n\nfrom auto_edit.samples import files\nfrom moviepy.editor import VideoFileClip\n\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detector_notebook.py:84-103\n\n# %%\n\n# for s in analysis.merged:\n#     if \"bounce\" in s.labels:\n#         rich.print(s.rms_segment)\nrich.print(analysis.merged[0:10])\n# %%\n\n#\n#\n# print(f'{analysis.sample_rate=}') # 22,050\n# %% PLOT:\n\n# segs = [s for s in analysis.merged if s.start >= 60 and s.start < 120]\nsegs = [s for s in analysis.merged if s.start < 30]\nplotter = SegmentPlotter(analysis, segs, title_addon=title_addon)\nplotter.show()\n\n# image_filename = f\"RMS frame_len={RMS_FRAME_LENGTH} hop_len={RMS_HOP_LENGTH}\"\n\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\n        # 5.1 ==> b/c one digit for - sign, two for tens/ones before decimal, one for decimal and one after decimal == 5 chars padding (notably to left pad single digit dB values so columns line up across segments when list is printed)\n        return f\"len={len(rms)} min={round(min_val, 1):5.1f} mean={round(mean_val, 1):5.1f} max={round(max_val, 1):5.1f}\"\n\n    def __repr__(self) -> str:\n        return self.__str__()\n\n    def __str__(self, no_color=False) -> str:\n\n        label = self.label.value.lower()\n        if not no_color:\n            label = colorize(self.label.value.lower(), self.plot_color())\n\n        # 4 for left padding id\n        return f\"{self.id:4} {label}: [{self.start:.3f}->{self.end:.3f}) {self.duration:.3f}s {self.rms_summary}\"\n\n    def get_plot_message(self) -> str:\n        fps = 30  # hardcode for now is fine # PRN pass later\n        start_frame = round(self.start * fps, 2)\n        end_frame = round(self.end * fps, 2)\n        return self.__str__(no_color=True) + f'(frames: {start_frame}-{end_frame})'\n\n    def plot_color(self) -> str:\n        if \"bounce\" in self.labels:\n            return \"yellow\"\n        return label_color_map.get(self.label, \"green\")\n\nclass CombinedSegments(DetectedSegment):\n    # store originals for later comparison (i.e. testing/troubleshooting)\n    originals: list[DetectedSegment]\n\ndef label_segments(\n    rms_db: RMSSamplesArray,\n    times: Float32Array,\n    thresholds: DetectThresholds,\n) -> list[DetectedSegment]:\n    labels: list[SegmentLabel] = []\n    for v in rms_db:\n        if v < thresholds.silence:\n            labels.append(SegmentLabel.SILENCE)\n        elif v < thresholds.breath:\n            labels.append(SegmentLabel.BREATH)\n        else:\n            labels.append(SegmentLabel.SPEECH)\n\n    segments: list[DetectedSegment] = []\n    start_time = times[0]\n    start_idx = 0\n    cur_label = labels[0]\n\n    id = 1\n    for i in range(1, len(labels)):\n        if labels[i] != cur_label:\n            end_time = times[i]\n            end_idx = i\n            rms_segment = rms_db[start_idx:end_idx]\n            segments.append(DetectedSegment(id, start_time, end_time, cur_label, rms_segment))\n            id = id + 1\n            start_time = times[i]\n            start_idx = i\n            cur_label = labels[i]\n\n    rms_segment = rms_db[start_idx:]\n    segments.append(DetectedSegment(id, start_time, times[-1], cur_label, rms_segment))\n    return segments\n\n@dataclass\nclass AudioAnalysisResult:\n    audio_samples: MonoAudioSamplesArray\n    sample_rate: int | float\n    segments: list[DetectedSegment]\n    rms_db: Float32Array\n    time_stamps: Float32Array\n    thresholds: DetectThresholds\n    merged: list[DetectedSegment] = field(default_factory=list)  # don't forget cannot just set [] else all instances get same initial list due to scoping! (still trying to remind myself this)\n\n    def list_by_label(self, label_type: SegmentLabel) -> None:\n        for segment in self.segments:\n            if segment.label == label_type:\n                print(segment)\n\ndef full_analysis(path: Path) -> <|fim_middle|>\n\ndef speech_analysis(path: Path) -> AudioAnalysisResult:\n    from numpy.typing import NDArray\n    audio_samples: MonoAudioSamplesArray  # aka \"y\"\n    sample_rate: SampleRate\n\n    audio_samples, sample_rate = librosa.load(str(path), mono=True)\n\n    rms_db, times = compute_energy(audio_samples, sample_rate)\n\n    segments = label_segments(rms_db, times, DEFAULT_THRESHOLDS)\n\n    analysis = AudioAnalysisResult(\n        audio_samples=audio_samples,\n        sample_rate=sample_rate,\n        segments=segments,\n        rms_db=rms_db,\n        time_stamps=times,\n        thresholds=DEFAULT_THRESHOLDS,\n    )\n\n    return analysis\n\ndef merge_close_speech_segments(analysis: AudioAnalysisResult, max_gap: float = 0.1) -> list[DetectedSegment]:\n\n    def _merge_close_speech_segments() -> list[DetectedSegment]:\n        merged: list[DetectedSegment] = []\n        current_idx = 0\n        # * copy until first silence\n        while current_idx < len(analysis.segments):\n            seg = analysis.segments[current_idx]\n            if seg.label == SegmentLabel.SPEECH:\n                break\n            merged.append(seg)\n            current_idx += 1\n\n        if current_idx >= len(analysis.segments):\n            return merged\n\n        # I get the feeling this could be simplified with a lfold accumulation\n\n        def get_next_speech(start_index) -> tuple[DetectedSegment | None, int | None]:\n            for idx in range(start_index, len(analysis.segments)):\n                if analysis.segments[idx].label == SegmentLabel.SPEECH:\n                    return analysis.segments[idx], idx\n            return None, None\n\n        # * on first silence\n        current_speech = analysis.segments[current_idx]\n        while True:\n            next_speech, next_speech_idx = get_next_speech(current_idx + 1)\n            if next_speech is None:\n                merged.append(current_speech)\n                # * copy verbatim to end of list\n                for idx in range(current_idx + 1, len(analysis.segments)):\n                    merged.append(analysis.segments[idx])\n                return merged\n            else:\n                assert next_speech_idx is not None\n\n                gap = next_speech.start - current_speech.end\n                if gap > max_gap:\n                    # Gap too large, finalize current speech segment\n                    merged.append(current_speech)\n                    # append intermediate segments\n                    for inter_idx in range(current_idx + 1, next_speech_idx):\n                        merged.append(analysis.segments[inter_idx])\n                    # Start new current_speech from next_speech\n                    current_speech = next_speech\n                    current_idx = next_speech_idx\n                    # Continue loop to check further merges\n                    continue\n\n                # * Merge current_speech with next_speech\n                #   Do not append yet; will be handled after merging further segments\n\n                # FYI current_idx+1 b/c current_idx corresponds to current_speech (which may be merged, so use it directly)\n                # FYI next_speech_idx+1 b/c slice is not inclusive of end so need one more to get rms_segments from later speech segment\n                rms_segments = [s.rms_segment for s in analysis.segments[current_idx + 1:next_speech_idx + 1]]\n                rms_segments.insert(0, current_speech.rms_segment)  # keep past merge rms_segments<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 3308,
    "timings": {
      "cache_n": 1072,
      "prompt_n": 2110,
      "prompt_ms": 303.498,
      "prompt_per_token_ms": 0.14383791469194313,
      "prompt_per_second": 6952.269866687755,
      "predicted_n": 127,
      "predicted_ms": 553.895,
      "predicted_per_token_ms": 4.361377952755905,
      "predicted_per_second": 229.28533386291625
    }
  }
}

{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765352499,
  "id": "chatcmpl-LJNAwHJWngM6mwaTZK9qMuhOtj5MiK9i",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 0,
    "prompt_n": 2685,
    "prompt_ms": 439.172,
    "prompt_per_token_ms": 0.16356499068901303,
    "prompt_per_second": 6113.777745393603,
    "predicted_n": 119,
    "predicted_ms": 580.212,
    "predicted_per_token_ms": 4.8757310924369746,
    "predicted_per_second": 205.0974471400109
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 3,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 119,
    "tokens_evaluated": 2685,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-10\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect.py:178-201\ndef speech_analysis(path: Path, thresholds: DetectThresholds | None = None) -> AudioAnalysisResult:\n    from numpy.typing import NDArray\n    y: MonoAudioSamplesArray\n    sr: SampleRate\n\n    y, sr = librosa.load(str(path), mono=True)\n\n    rms_db_scaled, rms_db, times = compute_energy(y, sr)\n    if thresholds is None:\n        thresholds = derive_percentile_based_thresholds(rms_db)\n\n    segments = label_segments(rms_db, times, thresholds)\n\n    analysis = AudioAnalysisResult(\n        audio_samples=y,\n        sample_rate=sr,\n        segments=segments,\n        rms_db_scaled=rms_db_scaled,\n        rms_db=rms_db,\n        time_stamps=times,\n        thresholds=thresholds,\n    )\n\n    return analysis\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect.py:54-64\ndef derive_percentile_based_thresholds(db: Float32Array) -> DetectThresholds:\n    # ?? try diff levels for better detection of talking vs not\n    quiet: np.float64 = np.percentile(db, 20)  # lower-energy (breathing, silence)\n    mid: np.float64 = np.percentile(db, 60)  # typical speech\n\n    # Silence is deeper than \"quiet\"\n    silence = quiet - 6  # ~6 dB below quiet zone\n    breath = (quiet + mid) / 2\n    rich.print(f'{silence=} {breath=}')\n\n    return DetectThresholds(silence=silence, breath=breath)\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect.py:176-195\n\n\ndef speech_analysis(path: Path, thresholds: DetectThresholds | None = None) -> AudioAnalysisResult:\n    from numpy.typing import NDArray\n    y: MonoAudioSamplesArray\n    sr: SampleRate\n\n    y, sr = librosa.load(str(path), mono=True)\n\n    rms_db_scaled, rms_db, times = compute_energy(y, sr)\n    if thresholds is None:\n        thresholds = derive_percentile_based_thresholds(rms_db)\n\n    segments = label_segments(rms_db, times, thresholds)\n\n    analysis = AudioAnalysisResult(\n        audio_samples=y,\n        sample_rate=sr,\n        segments=segments,\n        rms_db_scaled=rms_db_scaled,\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect.py:29-31\n@staticmethod\n    def floats(silence: float, breath: float):\n        return DetectThresholds(silence=np.float64(silence), breath=np.float64(breath))\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect.py:24-31\n@dataclass\nclass DetectThresholds:\n    silence: np.float64\n    breath: np.float64\n\n    @staticmethod\n    def floats(silence: float, breath: float):\n        return DetectThresholds(silence=np.float64(silence), breath=np.float64(breath))\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import List\nimport pytest\n\n# Assuming these classes are defined in the module under test\nfrom auto_edit.new_silence.detect import (\n    DetectThresholds,\n    SegmentLabel,\n    DetectedSegment,\n    CombinedSegments,\n    AudioAnalysisResult,\n    merge_close_speech_segments,\n)\n\ndef analysis_for(speech_seg: DetectedSegment | list[DetectedSegment]) -> AudioAnalysisResult:\n    if isinstance(speech_seg, list):\n        segments = speech_seg\n    else:\n        segments = [speech_seg]\n    return AudioAnalysisResult(\n        audio_samples=np.array([0.0]),\n        sample_rate=16000,\n        segments=segments,\n        rms_db_scaled=np.array([]),\n        rms_db=np.array([]),\n        time_stamps=np.array([]),\n        thresholds=DetectThresholds.<|fim_middle|>\n        merged=[],\n    )\n\nid_counter = 0\n\n@pytest.fixture(autouse=True)\ndef reset_id_counter():\n    global id_counter\n    id_counter = 0\n    # ensure the module variable is reset\n    import importlib\n    import auto_edit.new_silence.detect_tests as ft\n    importlib.reload(ft)\n\ndef silence_seg(start, end):\n    global id_counter\n    seg = DetectedSegment(\n        id=id_counter,\n        start=start,\n        end=end,\n        label=SegmentLabel.SILENCE,\n        rms_segment=np.array([id_counter]),\n    )\n    id_counter += 1\n    return seg\n\ndef speech_seg(start, end):\n    global id_counter\n    seg = DetectedSegment(\n        id=id_counter,\n        start=start,\n        end=end,\n        label=SegmentLabel.SPEECH,\n        rms_segment=np.array([id_counter]),  # make the samples identifiable is all I need for now\n    )\n    id_counter += 1\n    return seg\n\nclass TestMerging:\n\n    def test_only_speech__merges_nothing(self):\n        seg = speech_seg(0, 1)\n        merged = merge_close_speech_segments(analysis_for(seg))\n        assert merged == [seg]\n\n    def test_only_non_speech__merges_nothing(self):\n        seg = silence_seg(0, 1)\n        merged = merge_close_speech_segments(analysis_for(seg))\n        assert merged == [seg]\n\n    def test_only_one_speech_in_middle__merges_nothing(self):\n        seg1 = silence_seg(0, 0.4)\n        seg2 = speech_seg(0.4, 0.6)\n        seg3 = silence_seg(0.6, 1.0)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2, seg3]))\n        assert merged == [seg1, seg2, seg3]\n\n    def test_ends_with_speech__merges_nothing(self):\n        seg1 = silence_seg(0, 0.4)\n        seg2 = speech_seg(0.4, 0.6)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2]))\n        assert merged == [seg1, seg2]\n\n    def test_starts_with_speech__merges_nothing(self):\n        seg1 = speech_seg(0, 0.2)\n        seg2 = silence_seg(0.2, 1.0)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2]))\n        assert merged == [seg1, seg2]\n\n    def test_two_speech_back_to_back__merged_into_one(self):\n        seg1 = speech_seg(0.0, 0.5)\n        seg2 = speech_seg(0.5, 1.0)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2]))\n\n        assert len(merged) == 1\n        combined = merged[0]\n        assert isinstance(combined, CombinedSegments)\n        assert combined.start == 0.0\n        assert combined.end == 1.0\n        assert combined.label == SegmentLabel.SPEECH\n        assert np.array_equal(combined.rms_segment, [0, 1])\n\n    def test_three_speech_back_to_back__merges_into_one(self):\n        seg1 = speech_seg(0.0, 0.3)\n        seg2 = speech_seg(0.3, 0.6)\n        seg3 = speech_seg(0.6, 1.0)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2, seg3]))\n\n        assert len(merged) == 1\n        combined = merged[0]\n        assert isinstance(combined, CombinedSegments)\n        assert combined.start == 0.0\n        assert combined.end == 1.0\n        assert combined.label == SegmentLabel.SPEECH\n        assert np.array_equal(combined.rms_segment, [0, 1, 2])\n\n    def test_speech_then_multiple_silents_to_end__merges_nothing(self):\n        seg1 = speech_seg(0.0, 0.3)\n        seg2 = silence_seg(0.3, 0.5)\n        seg3 = silence_seg(0.5, 0.7)\n        seg4 = silence_seg(0.7, 1.0)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2, seg3, seg4]))\n\n        assert merged == [seg1, seg2, seg3, seg4]\n\n    def test_speech_tiny_silence_speech__merges(self):\n        seg1 = speech_seg(0.0, 0.3)\n        seg2 = silence_seg(0.3, 0.31)  # tiny silence shorter than max_gap=0.10\n        seg3 = speech_seg(0.31, 0.6)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2, seg3]))\n\n        assert len(merged) == 1\n        combined = merged[0]\n        assert isinstance(combined, CombinedSegments)\n        assert combined.start == 0.0\n        assert combined.end == 0.6\n        assert combined.label == SegmentLabel.SPEECH\n        # key part of test is that the RMS of the silence periods are copied too:\n        assert np.array_equal(combined.rms_segment, [0, 1, 2])\n\n    def test_speech_long_silence_speech__merges_nothing(self):\n        seg1 = speech_seg(0.0, 0.3)\n        seg2 = silence_seg(0.3, 0.5)  # longer than max_gap=0.10\n        seg3 = speech_seg(0.5, 0.8)\n\n        merged = merge_close_speech_segments(analysis_for([seg1, seg2, seg3]))<|end|><|start|>assistant",
    "has_new_line": false,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 2803,
    "timings": {
      "cache_n": 0,
      "prompt_n": 2685,
      "prompt_ms": 439.172,
      "prompt_per_token_ms": 0.16356499068901303,
      "prompt_per_second": 6113.777745393603,
      "predicted_n": 119,
      "predicted_ms": 580.212,
      "predicted_per_token_ms": 4.8757310924369746,
      "predicted_per_second": 205.0974471400109
    }
  }
}

{
  "request_body":{
    "options":[],
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "model":"gpt-oss:120b",
    "temperature":1,
    "top_p":1,
    "raw":false,
    "max_tokens":4096,
    "messages":[{
        "role":"developer",
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n"
      },{
        "role":"user",
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Lua Code Preferences\n\nNaming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\nType Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n```lua\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n```\n\n"
      },{
        "role":"user",
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/logs/completion_logger.lua:8-23\n---@param request CurlRequest\n---@param frontend StreamingFrontend\n---@return string save_dir, string thread_id\nfunction M.where_to_save(request, frontend)\n    local save_dir = vim.fn.stdpath(\"state\") .. \"/ask-openai\"\n    if request.type ~= \"\" then\n        -- add `questions/` or `fim/` or `rewrite/` intermediate path\n        save_dir = save_dir .. \"/\" .. request.type\n    end\n    local thread_id = tostring(request.start_time)\n    if frontend.thread then\n        -- multi-turn threads use thread's start_time\n        thread_id = tostring(frontend.thread.start_time)\n    end\n    return save_dir, thread_id\nend\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/logs/completion_logger.lua:102-138\n                     function()\n            vim.fn.mkdir(save_dir, \"p\")\n            -- TODO append new message to -messages.jsonl (new line, compact)\n\n            local path = save_dir .. \"/\" .. thread_id .. \"-thread.json\"\n            -- log:info(\"thread path\", path)\n            local file = io.open(path, \"w\")\n            if file then\n                local thread_data = {\n                    -- 99.99% of the time this is all I need (input messages thread + output message):\n                    request_body = request.body,\n                    response_message = request.accum,\n                    --\n                    -- FYI must use llama-server's --verbose flag .__verbose.* on last_sse\n                    --   __verbose.prompt basically repeats request.body, thus not needed\n                    --   rendered prompt can be nice for reproducibility\n                    --     but, you can also use /apply-template endpoint to generate it too\n                    --     obviously won't capture template changes when rendering\n                    --\n                    -- last_sse has:\n                    --   .timings (top-level and under .__verbose.timings)\n                    --   .__verbose.(prompt, generation_settings)\n                    --   .__verbose.content (generated raw outputs, but ONLY for stream=false)\n                    last_sse = sse_parsed,\n                }\n                file:write(json.encode(thread_data, { indent = true }))\n                file:close()\n            end\n\n            if M.LOG_ALL_SSEs then\n                local all_file = io.open(save_dir .. \"/all_sses.json\", \"w\")\n                if all_file then\n                    all_file:write(json.encode(all_sses, { indent = true }))\n                    all_file:close()\n                end\n            end\n        end\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/logs/completion_logger.lua:98-117\n            -- create dir for multiple files (one per SSE)\n            save_dir = save_dir .. \"/\" .. thread_id\n        end\n\n        vim.defer_fn(function()\n            vim.fn.mkdir(save_dir, \"p\")\n            -- TODO append new message to -messages.jsonl (new line, compact)\n\n            local path = save_dir .. \"/\" .. thread_id .. \"-thread.json\"\n            -- log:info(\"thread path\", path)\n            local file = io.open(path, \"w\")\n            if file then\n                local thread_data = {\n                    -- 99.99% of the time this is all I need (input messages thread + output message):\n                    request_body = request.body,\n                    response_message = request.accum,\n                    --\n                    -- FYI must use llama-server's --verbose flag .__verbose.* on last_sse\n                    --   __verbose.prompt basically repeats request.body, thus not needed\n                    --   rendered prompt can be nice for reproducibility\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/logs/completion_logger.lua:83-102\n    if sse_parsed.timings then\n        -- store for convenient access in-memory, that way if smth fails on save I can still see it here\n        M.last_done = {\n            sse_parsed = sse_parsed,\n            request = request,\n            frontend = frontend,\n        }\n\n        local save_dir, thread_id = M.where_to_save(request, frontend)\n\n        if M.LOG_ALL_SSEs then\n            -- PRN save to 123-sses.jsonl\n            --   SSEs are fairly standardized => thus jsonl would likely read table-like\n            --   for all but first(s)/last(s)\n            --\n            -- create dir for multiple files (one per SSE)\n            save_dir = save_dir .. \"/\" .. thread_id\n        end\n\n        vim.defer_fn(function()\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/backends/models/fim.lua:157-176\n---@param request FimBackend\nfunction M.bytedance_seed_coder.get_fim_prompt_file_level_only(request)\n    -- FYI file level works well for Seed-Coder\n\n    local tokens = M.bytedance_seed_coder.sentinel_tokens\n    -- * SPM\n    local file_level_prompt_only = ''\n        -- PRN show FIM file path commented out?\n        -- tokens.file_sep\n        -- .. current_file_relative_path\n        -- .. \"\\n\"\n        --\n        .. tokens.FIM_SUFFIX\n        .. request.ps_chunk.suffix\n        .. tokens.FIM_PREFIX\n        .. request.ps_chunk.prefix\n        .. tokens.FIM_MIDDLE\n\n    return file_level_prompt_only\nend\n"
      },{
        "role":"user",
        "content":"Please suggest text to replace <|fim_middle|>:\n\n```lua/ask-openai/backends/curl.lua\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal completion_logger = require(\"ask-openai.logs.completion_logger\")\nlocal CurlRequest = require(\"ask-openai.backends.curl_request\")\nlocal SSEDataOnlyParser = require(\"ask-openai.backends.sse.data_only_parser\")\nlocal completion_logger = require('ask-openai.logs.completion_logger')\nlocal safely = require(\"ask-openai.helpers.safely\")\nlocal json = require('dkjson')\n\nlocal Curl = {}\n\n---@enum CompletionsEndpoints\n_G.CompletionsEndpoints = {\n    -- llama-server non-openai:\n    llamacpp_completions = \"/completions\",\n\n    -- OpenAI compatible:\n    oai_v1_completions = \"/v1/completions\",\n    oai_v1_chat_completions = \"/v1/chat/completions\",\n\n    -- ollama specific\n    ollama_api_generate = \"/api/generate\",\n    ollama_api_chat = \"/api/chat\",\n}\n\n---@alias OnParsedSSE fun(sse_parsed: table)\n---@alias ExplainError fun(text: string)\n---@alias OnCurlExitedSuccessfully fun()\n\n---@class StreamingFrontend\n---@field on_parsed_data_sse OnParsedSSE\n---@field on_sse_llama_server_timings OnParsedSSE\n---@field on_curl_exited_successfully OnCurlExitedSuccessfully\n---@field explain_error ExplainError\n---@field thread? CurlRequestForThread\n\n---@param request CurlRequest|CurlRequestForThread\n---@param frontend StreamingFrontend\nfunction Curl.spawn(request, frontend)\n    request.body.stream = true\n\n    local json_body = vim.json.encode(request.body)\n    -- Save the initial request payload (messages) before sending, for any frontend that uses Curl.\n    if request.body and request.body.messages then\n        local ok, payload = pcall(function()\n            -- * each message on its own (initial request has multiple messages)\n            --  PRN do I really like this style? how about just pretty print with back to back messages :) and not deal with \"jsonl\"\n            --  TODO only append new messages (in this case, assistant response message only)\n            --    currently re-saving entire thread every time\n            local message_lines = {}\n            for _, msg in ipairs(request.body.messages) do\n                table.insert(message_lines, json.encode(\n                    { messages = { msg } },\n                    { indent = false } -- compact/oneline\n                ))\n            end\n            return table.concat(message_lines, \"\\n\")\n        end)\n        if ok then\n            local save_dir, thread_id = <|fim_middle|>\n\n            vim.fn.mkdir(save_dir, \"p\")\n            local path = save_dir .. \"/\" .. thread_id .. \"-messages.jsonl\"\n            local file = io.open(path, \"w\")\n            if file then\n                file:write(payload)\n                file:close()\n                log:info(\"Saved initial curl request to %s\", path)\n            else\n                log:error(\"Unable to write initial curl request to %s\", path)\n            end\n        else\n            log:error(\"Failed to encode initial curl request: %s\", payload)\n        end\n    end\n\n    local options = {\n        command = \"curl\",\n        args = {\n            \"--fail-with-body\",\n            \"-sSL\",\n            \"--no-buffer\", -- w/o this curl batches (test w/ `curl *` vs `curl * | cat` and you will see difference)\n            \"-X\", \"POST\",\n            request:get_url(),\n            \"-H\", \"Content-Type: application/json\",\n            \"-d\", json_body\n        },\n    }\n\n    ---@param data_value string\n    function on_data_sse(data_value)\n        -- FYI right now this function exists to catch unhandled errors and terminate\n        local success, error_message = safely.call(Curl.on_one_data_value, data_value, frontend, request)\n        if success then\n            return\n        end\n\n        -- request stops ASAP, but not immediately\n        CurlRequest.terminate(request)\n        frontend.explain_error(\"Abort... unhandled exception in curl: \" .. tostring(error_message))\n    end\n\n    local stdout = vim.uv.new_pipe(false)\n    local stderr = vim.uv.new_pipe(false)\n\n    local parser = SSEDataOnlyParser.new(on_data_sse)\n\n    ---@param code integer\n    ---@param signal integer\n    local function on_exit(code, signal)\n        -- log:trace_on_exit_always(code, signal)\n        log:trace_on_exit_errors(code, signal) -- less verbose\n\n        -- close before check dregs (b/c might still be data unflushed in STDOUT/ERR)\n        stdout:close()\n        stderr:close()\n\n        request.handle = nil\n        request.pid = nil\n\n        -- flush dregs before on_curl_exited_successfully\n        -- - which may depend on, for example, a tool_call in dregs\n        local error_text = parser:flush_dregs()\n        if error_text then\n            frontend.explain_error(error_text)\n        end\n\n        if code == 0 then\n            -- FYI this has to come after dregs which may have data used by exit handler!\n            --  i.e. triggering tool_calls\n            frontend.on_curl_exited_successfully()\n        end\n\n        -- FYI review proper uv.spawn cleanup LATER:\n        -- - review:   vim.loop.walk(function(handle) print(handle) end)\n        --   - I am seeing alot after I just startup nvim... I wonder if some are from my MCP tool comms?\n        --   - and what about my timer/schduling for debounced keyboard events to trigger predictions?\n        -- - REVIEW OTHER uses of uv.spawn (and timers)... for missing cleanup logic!)\n    end\n\n    request.handle, request.pid = vim.uv.spawn(options.command,\n        ---@diagnostic disable-next-line: missing-fields\n        {\n            args = options.args,\n            stdio = { nil, stdout, stderr },\n        },\n        on_exit)\n\n    ---@param read_error any\n    ---@param data? string\n    local function on_stdout(read_error, data)\n        log:trace_stdio_read_errors(\"on_stdout\", read_error, data)\n        -- log:trace_stdio_read_always(\"on_stdout\", read_error, data)\n\n        local no_data = data == nil or data == \"\"\n        if read_error or no_data then\n            return\n        end\n        assert(data ~= nil)\n\n        parser:write(data)\n    end\n```\n\n## Reminders about the cursor line:\nText before cursor: `            local save_dir, thread_id = `"
      }],
    "stream":true
  },
  "last_sse":{
    "system_fingerprint":"b7785-1c7cf94b2",
    "id":"chatcmpl-IGcICMrqkz7Df2j7gbddtDcTP4pZzvh9",
    "choices":[{
        "index":0,
        "finish_reason":"stop",
        "delta":[]
      }],
    "object":"chat.completion.chunk",
    "timings":{
      "prompt_n":2556,
      "predicted_per_second":219.8170572541,
      "predicted_n":80,
      "prompt_ms":587.545,
      "prompt_per_token_ms":0.22986893583725,
      "predicted_ms":363.939,
      "predicted_per_token_ms":4.5492375,
      "prompt_per_second":4350.3050830149,
      "cache_n":959
    },
    "created":1770802395,
    "model":"ggml-org/gpt-oss-120b-GGUF"
  },
  "response_message":{
    "role":"assistant",
    "finish_reason":"stop",
    "content":"completion_logger.where_to_save(request, frontend)",
    "reasoning_content":"We need to fill with call to M.where_to_save likely. In this file, import completion_logger earlier, but where_to_save is in completion_logger module. So use completion_logger.where_to_save(request, frontend). Probably: local save_dir, thread_id = completion_logger.where_to_save(request, frontend)"
  }
}
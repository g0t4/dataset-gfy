{
  "request_body":{
    "options":[],
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "max_tokens":4096,
    "top_p":1,
    "stream":true,
    "model":"gpt-oss:120b",
    "messages":[{
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n",
        "role":"developer"
      },{
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Lua Code Preferences\n\nNaming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\nType Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n```lua\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n```\n\n\n\n## Recent yanks (copy to clipboard):\n```lua/ask-openai/questions/frontend.lua\n    if QuestionsFrontend.rag_cancel then\n        QuestionsFrontend.rag_cancel()\n    end\n```\n\n```lua/ask-openai/questions/frontend.lua\n        QuestionsFrontend.rag_cancel = function()\n            log:warn(\"canceling RAG\")\n            QuestionsFrontend.rag_cancel = nil\n            cancel()\n            QuestionsFrontend.rag_request_ids = nil\n        end\n```\n\n```lua/ask-openai/rewrites/frontend.lua\nRewriteFrontend\n```\n\n```lua/ask-openai/rewrites/frontend.lua\nt\n```\n\n```lua/ask-openai/rewrites/frontend.lua\n            log:info(\"on_rag_response\")\n```\n\n```lua/ask-openai/rewrites/frontend.lua\n            log:info(\"on_rag_response\")\n```\n\n```lua/ask-openai/questions/frontend.lua\n        QuestionsFrontend.rag_cancel = function()\n            log:warn(\"canceling RAG\")\n            QuestionsFrontend.rag_cancel = nil\n            cancel()\n            QuestionsFrontend.rag_request_ids = nil\n        end\n```\n\n",
        "role":"user"
      },{
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/frontend.lua:236-241\n                                       function()\n            log:warn(\"canceling RAG\")\n            QuestionsFrontend.rag_cancel = nil\n            cancel()\n            QuestionsFrontend.rag_request_ids = nil\n        end\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/frontend.lua:215-233\n        ---@param rag_matches LSPRankedMatch[]\n        function on_rag_response(rag_matches)\n            log:info(\"on_rag_response\")\n            -- * make sure prior (canceled) rag request doesn't still respond\n            if QuestionsFrontend.rag_request_ids ~= this_request_ids then\n                log:trace(\"possibly stale rag results, skipping: \" .. vim.inspect({\n                    global_rag_request_ids = QuestionsFrontend.rag_request_ids,\n                    this_request_ids = this_request_ids,\n                }))\n                return\n            end\n\n            if QuestionsFrontend.rag_cancel == nil then\n                log:error(\"rag appears canceled, skipping on_rag_response...\")\n                return\n            end\n\n            then_generate_completion(rag_matches)\n        end\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rewrites/frontend.lua:458-463\n                                     function()\n            log:warn(\"canceling RAG\")\n            RewriteFrontend.rag_cancel = nil\n            cancel()\n            RewriteFrontend.rag_request_ids = nil\n        end\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rewrites/frontend.lua:440-458\n            if RewriteFrontend.rag_request_ids ~= this_request_ids then\n                log:trace(\"possibly stale rag results, skipping: \" .. vim.inspect({\n                    global_rag_request_ids = RewriteFrontend.rag_request_ids,\n                    this_request_ids = this_request_ids,\n                }))\n                return\n            end\n\n            if RewriteFrontend.rag_cancel == nil then\n                log:error(\"rag appears canceled, skipping on_rag_response...\")\n                return\n            end\n\n            then_send_rewrite(rag_matches)\n        end\n\n        -- TODO should abort logic also clear rag_cancel/rag_request_ids?\n        this_request_ids, cancel = rag_client.context_query_rewrites(user_prompt, code_context, context.includes.top_k, on_rag_response)\n        RewriteFrontend.rag_cancel = \n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rewrites/frontend.lua:434-454\n        ---@param rag_matches LSPRankedMatch[]\n        function on_rag_response(rag_matches)\n            log:info(\"on_rag_response\")\n            -- PRN I think this could be shared with all frontends... if they pass themself for access to ID/cancel refs\n\n            -- * make sure prior (canceled) rag request doesn't still respond\n            if RewriteFrontend.rag_request_ids ~= this_request_ids then\n                log:trace(\"possibly stale rag results, skipping: \" .. vim.inspect({\n                    global_rag_request_ids = RewriteFrontend.rag_request_ids,\n                    this_request_ids = this_request_ids,\n                }))\n                return\n            end\n\n            if RewriteFrontend.rag_cancel == nil then\n                log:error(\"rag appears canceled, skipping on_rag_response...\")\n                return\n            end\n\n            then_send_rewrite(rag_matches)\n        end\n",
        "role":"user"
      },{
        "content":"Please suggest text to replace <|fim_middle|>:\n\n```lua/ask-openai/predictions/frontend.lua\n                        -- FYI great way to test this, go to a line that is done (i.e. a return) and go into insert mode before the returned variable and it almost always suggests that is EOS (at least with qwen2.5-coder + ollama)\n                        log:trace(ansi.yellow_bold(\"DONE, empty prediction\") .. \", done reason: '\" .. (done_reason or \"\") .. \"'\")\n\n                        -- TODO real fix for empty response to remove thinking tokens:\n                        -- good test case is to go b/w ends (below) and insert new line (empty) will likely result in a blank eventually (check reasoning too to confirm)\n                        -- FYI might have a similar issue in other spots... maybe parlay this into a final cleanup step?\n                        this_prediction:clear_extmarks()\n                    end\n                end\n            end)\n        end\n\n        ---@type OnCurlExitedSuccessfully\n        local function on_curl_exited_successfully()\n            -- placeholder, not sure I will even need this\n        end\n\n        ---@type ExplainError\n        local function explain_error(text)\n            vim.notify(\"ERROR in new PREDICTIONS FRONTEND PoC: \" .. text, vim.log.levels.ERROR)\n        end\n\n        ---@type OnParsedSSE\n        local function on_sse_llama_server_timings(sse)\n            stats.show_prediction_stats(sse, perf)\n        end\n\n        ---@type StreamingFrontend\n        local frontend = {\n            on_parsed_data_sse = on_parsed_data_sse,\n            on_curl_exited_successfully = on_curl_exited_successfully,\n            explain_error = explain_error,\n            on_sse_llama_server_timings = on_sse_llama_server_timings,\n        }\n\n        Curl.spawn(fim_request, frontend)\n    end\n\n    if enable_rag and rag_client.is_rag_supported_in_current_file() then\n        if not vim.lsp.get_clients({ name = \"ask_language_server\", bufnr = 0 })[1] then\n            -- FYI this check of client ready, must have immaterial overhead for working clients\n            --  would be better to do no checks than slow down normal use\n            log:error(\"RAG not available in current LSP, when it should be, so, sending FIM w/o RAG\")\n            then_send_fim({})\n            return\n        end\n        -- FYI vim.lsp.get_clients is taking ~3us for case when the LSP is operational, imperceptible overhead\n\n        local this_request_ids, cancel -- declare in advance so closure can access\n        perf:rag_started()\n\n        ---@param rag_matches LSPRankedMatch[]\n        function on_rag_response(rag_matches)\n            -- can this be called twice? one for done/cancel and a race condition b/w the two?\n            -- log:info(\"on_rag_response(rag_matches:'\" .. vim.inspect(rag_matches) .. \"')\")\n\n            -- FYI unroll all rag specific safeguards here so that logic doesn't live inside send_fim\n            perf:rag_done()\n\n            -- * make sure prior (canceled) rag request doesn't still respond\n            if PredictionsFrontend.rag_request_ids ~= this_request_ids then\n                -- I bet this is why sometimes I get completions that still fire even after cancel b/c the RAG results aren't actually stopped in time on server and so they come back\n                --  and they arrive after next request started... the mismatch in request_ids will prevent that issue\n                -- log:trace(\"possibly stale rag results, skipping: \" .. vim.inspect({\n                --     global_rag_request_ids = PredictionsFrontend.rag_request_ids,\n                --     this_request_ids = this_request_ids,\n                -- }))\n                return\n            end\n\n            if PredictionsFrontend.rag_cancel == nil then\n                -- log:error(\"rag appears canceled, skipping on_rag_response...\")\n                return\n            end\n\n            then_send_fim(rag_matches)\n        end\n\n        this_request_ids, cancel = rag_client.context_query_fim(ps_chunk, on_rag_response)\n        PredictionsFrontend.rag_cancel = function()\n            <|fim_middle|>\n        end\n        PredictionsFrontend.rag_request_ids = this_request_ids\n    else\n        PredictionsFrontend.rag_cancel = nil\n        PredictionsFrontend.rag_request_ids = nil\n        then_send_fim({})\n    end\nend\n\nfunction PredictionsFrontend.cancel_current_prediction()\n    -- PRN stdout/stderr:read_stop() to halt on_stdout/stderr callbacks from firing again (before handle:close())?!\n    if PredictionsFrontend.rag_cancel then\n        PredictionsFrontend.rag_cancel()\n        PredictionsFrontend.rag_cancel = nil\n    end\n    local this_prediction = PredictionsFrontend.current_prediction\n    if not this_prediction then\n        return\n    end\n    PredictionsFrontend.current_prediction = nil\n    this_prediction:mark_as_abandoned()\n\n    vim.schedule(function()\n        this_prediction:clear_extmarks()\n    end)\n\n    -- FYI both this_prediction and request are new with each keystroke\n    CurlRequest.terminate(this_prediction.fim_request)\nend\n\nlocal ignore_filetypes = {\n    \"TelescopePrompt\",\n    \"TelescopeResults\",\n    \"NvimTree\",\n    \"DressingInput\", -- pickers from nui (IIRC) => in nvim tree add a file => the file name box is one of these\n}\nlocal function is_rename_window()\n    -- TODO make sure only check this on enter buffer first time? not on every event (cursormoved,etc)\n    if vim.bo.buftype ~= \"nofile\"\n        or vim.bo.filetype ~= \"DressingInput\" then\n        return false\n    end\n    local win_id = vim.api.nvim_get_current_win()\n    local win_config = vim.api.nvim_win_get_config(win_id)\n    -- messages.append(win_config)\n    if not win_config then\n        -- shouldn't happen AFAICT\n        return false\n    end\n\n    -- win_config.title => { { \" Rename to \" } }\n    is_rename = win_config.title[1][1] == \" Rename to \"\n    -- TODO tell model about the window that is open (in some cases)...\n    -- i.e. rename window (gather diff context too, i.e. what would help with renames?)\n    return is_rename\nend\n\nlocal ignore_buftypes = {\n    \"nofile\", -- rename refactor popup window uses this w/o a filetype, also Dressing rename in nvimtree uses nofile\n    \"terminal\",\n}\nlocal keys = require(\"ask-openai.predictions.keys\")\nlocal keypresses, debounced = keys.create_keypresses_observables()\nlocal keypresses_subscription = keypresses:subscribe(function()\n    -- immediately clear/hide prediction, else slides as you type\n    vim.schedule(function()\n        PredictionsFrontend.cancel_current_prediction()\n    end)\nend)\nlocal debounced_subscription = debounced:subscribe(function()\n    vim.schedule(function()\n        -- log:trace(\"CursorMovedI debounced\")\n\n        if vim.fn.mode() ~= \"i\" then\n            return\n        end\n\n        PredictionsFrontend.ask_for_prediction()\n    end)\nend)\n```\n\n## Reminders about the cursor line:\nText before cursor: `            `\nText before cursor is all whitespace (do not ignore this)",
        "role":"user"
      }],
    "raw":false,
    "temperature":1
  },
  "response_message":{
    "finish_reason":"stop",
    "content":"log:warn(\"canceling RAG\")\nPredictionsFrontend.rag_cancel = nil\ncancel()\nPredictionsFrontend.rag_request_ids = nil",
    "reasoning_content":"We need code for cancel function: likely cancel(); PredictionsFrontend.rag_cancel=nil? In similar frontend code earlier shows in questions: they set rag_cancel = function() log:warn... cancel() ... end. Here just need to call cancel and clear vars. Probably:\n\nlog:warn(\"canceling RAG\")\nPredictionsFrontend.rag_cancel = nil\ncancel()\nPredictionsFrontend.rag_request_ids = nil\n\nBut inside this function, cancel is in outer scope. So:\n\nlog:warn(\"canceling RAG\")\nPredictionsFrontend.rag_cancel = nil\ncancel()\nPredictionsFrontend.rag_request_ids = nil\n\nReturn nothing.",
    "role":"assistant"
  },
  "last_sse":{
    "timings":{
      "predicted_per_token_ms":4.567325443787,
      "predicted_n":169,
      "prompt_per_second":9060.1474175694,
      "prompt_n":3454,
      "predicted_per_second":218.94651745483,
      "prompt_ms":381.23,
      "cache_n":0,
      "prompt_per_token_ms":0.11037348002316,
      "predicted_ms":771.878
    },
    "object":"chat.completion.chunk",
    "created":1770074923,
    "system_fingerprint":"b7785-1c7cf94b2",
    "choices":[{
        "finish_reason":"stop",
        "index":0,
        "delta":[]
      }],
    "id":"chatcmpl-nJjVMcNhY2ChhHd7zlLtWwB3m5951BnF",
    "model":"ggml-org/gpt-oss-120b-GGUF"
  }
}
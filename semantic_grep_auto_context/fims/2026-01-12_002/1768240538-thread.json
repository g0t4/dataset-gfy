{
  "last_sse":{
    "choices":[{
        "finish_reason":"stop",
        "index":0,
        "delta":[]
      }],
    "system_fingerprint":"b7616-18ddaea2a",
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "id":"chatcmpl-64lxqaIpc6nRYnQHBYavN629iQKbCBfW",
    "timings":{
      "prompt_ms":404.434,
      "prompt_n":4089,
      "predicted_ms":397.781,
      "predicted_per_token_ms":4.7925421686747,
      "cache_n":0,
      "predicted_per_second":208.65752763455,
      "predicted_n":83,
      "prompt_per_second":10110.425928582,
      "prompt_per_token_ms":0.09890780141844
    },
    "created":1768240538,
    "object":"chat.completion.chunk"
  },
  "response_message":{
    "finish_reason":"stop",
    "reasoning_content":"We need to insert code after requiring log. Likely require other modules? Search context earlier maybe need to require safely, etc. In this file they use safely.call, vim.json, etc. Probably need to require safely. Look at other files for imports. Search.",
    "role":"assistant",
    "content":"local safely = require(\"ask-openai.helpers.safely\")"
  },
  "request_body":{
    "raw":false,
    "model":"gpt-oss:120b",
    "options":[],
    "temperature":1,
    "messages":[{
        "role":"developer",
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n"
      },{
        "role":"user",
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Lua Code Preferences\n\nNaming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\nType Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n\n\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/logs/logger.lua:300-310\nfunction Logger.predictions()\n    if DISABLED then\n        return NOOP_LOGGER\n    end\n\n    if predictions_logger then\n        return predictions_logger\n    end\n    predictions_logger = Logger:new(\"ask-predictions.log\")\n    return predictions_logger\nend\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/backends/models/fim.lua:0-12\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal files = require(\"ask-openai.helpers.files\")\nlocal ansi = require(\"ask-openai.predictions.ansi\")\nlocal ChatThread = require(\"ask-openai.questions.chat.thread\")\nlocal TxChatMessage = require(\"ask-openai.questions.chat.messages.tx\")\nlocal ContextItem = require(\"ask-openai.predictions.context.item\")\n\nlocal M = {}\n\n-- these models work for other purposes too...\n--   but lets collect them under fim for now\n--   really these are the fim related special tokens\n\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/predictions/frontend.lua:23-199\n---@param params? PredictionParameters\nfunction PredictionsFrontend.ask_for_prediction(params)\n    PredictionsFrontend.cancel_current_prediction()\n\n    local this_prediction = Prediction.new(params)\n    PredictionsFrontend.current_prediction = this_prediction\n\n    local enable_rag = api.is_rag_enabled()\n    local ps_chunk = ps.get_prefix_suffix_chunk()\n\n    local perf = FIMPerformance:new()\n\n    ---@param rag_matches LSPRankedMatch[]\n    function then_send_fim(rag_matches)\n        local model = api.get_fim_model()\n\n        -- TODO rename to FimBodyBuilder? or FimRequestBuilder? or FimPromptBuilder?\n        local backend = FimBackend:new(ps_chunk, rag_matches, model)\n        local body = backend:body_for()\n        assert(body ~= nil)\n\n        if this_prediction.apply_template_only then\n            -- PRN? move this out into its own module, composed with new open_float\n            -- log:luaify_trace(\"predictions.body\", body) -- luaify logs later\n            log:info(\"predictions.base_url\", FimBackend.base_url)\n            log:info(\"predictions.endpoint\", FimBackend.endpoint)\n\n            local response = llama_server_client.apply_template(FimBackend.base_url, body)\n            local FloatWindow = require(\"ask-openai.helpers.float_window\")\n            local lines = vim.split(response.body.prompt, '\\n')\n            ---@type FloatWindowOptions\n            local opts = { width_ratio = 0.8, height_ratio = 0.8, filetype = \"harmony\" }\n            local buf, win = FloatWindow:new(opts, lines)\n            -- PRN? setup harmony grammar for filetype + coloring with treesitter?\n            -- PRN? or use LinesBuilder for lines w/ extmarks using LinesBuilder (not hard to do either, and would get me to setup a simple parser!)\n            return\n        end\n\n        local fim_request = CurlRequest:new({\n            body = body,\n            base_url = FimBackend.base_url,\n            endpoint = FimBackend.endpoint,\n            type = \"fim\",\n        })\n        this_prediction.fim_request = fim_request\n\n        ---@type OnParsedSSE\n        local function on_parsed_data_sse(sse_parsed)\n            perf:token_arrived()\n\n            -- use defer_fn w/ 500ms to reproduce \"stuck\" predictions\n            -- also found that toggling off the copilot while a prediction is visible, results in a stuck prediction\n            vim.schedule(function()\n                if this_prediction.abandoned then\n                    -- DO NOT update prediction text if it's been abandoned!\n                    -- reproduce bug by comment out this check...\n                    -- then set 500ms delay using defer_fn\n                    -- then trigger a prediction and cancel it midway and it'll be stuck!\n                    log:trace(ansi.yellow_bold(\"skipping on_stdout chunk b/c prediction is abandoned, if you see many of these in a row... that likely means the request wasn't canceled!\"))\n                    return\n                end\n\n                local chunk, done, done_reason, reasoning_content\n                if FimBackend.endpoint == CompletionsEndpoints.llamacpp_completions then\n                    -- TODO content parsing works, no reasoning parser (yet)\n                    chunk, done, done_reason = parse_sse_llamacpp_completions(sse_parsed)\n                elseif FimBackend.endpoint == CompletionsEndpoints.oai_v1_chat_completions then\n                    -- FYI fully works, including reasoning:\n                    chunk, done, done_reason, reasoning_content = parse_sse_oai_chat_completions(sse_parsed)\n                elseif FimBackend.endpoint == CompletionsEndpoints.ollama_api_chat then\n                    -- FYI not a priority to support beyond what it does now:\n                    chunk, done, done_reason = parse_sse_ollama_api_chat(sse_parsed)\n                elseif FimBackend.endpoint == CompletionsEndpoints.ollama_api_generate then\n                    -- FYI not a priority to support beyond what it does now:\n                    chunk, done, done_reason = parse_sse_ollama_api_generate(sse_parsed)\n                else\n                    error(\"Unsupported FIM endpoint: \" .. tostring(FimBackend.endpoint))\n                end\n\n                if chunk or reasoning_content then\n                    this_prediction:add_chunk_to_prediction(chunk, reasoning_content)\n                end\n\n                if done then\n                    if this_prediction.has_reasoning then\n                        -- log:info(ansi.yellow_bold(\"REASONING:\\n\"), ansi.yellow(this_prediction:get_reasoning()))\n                    end\n                    if this_prediction:any_chunks() then\n                        -- log:info(ansi.cyan_bold(\"PREDICTION:\\n\"), ansi.cyan(this_prediction.prediction))\n                    else\n                        -- FYI great way to test this, go to a line that is done (i.e. a return) and go into insert mode before the returned variable and it almost always suggests that is EOS (at least with qwen2.5-coder + ollama)\n                        log:trace(ansi.yellow_bold(\"DONE, empty prediction\") .. \", done reason: '\" .. (done_reason or \"\") .. \"'\")\n\n                        -- TODO real fix for empty response to remove thinking tokens:\n                        -- good test case is to go b/w ends (below) and insert new line (empty) will likely result in a blank eventually (check reasoning too to confirm)\n                        -- FYI might have a similar issue in other spots... maybe parlay this into a final cleanup step?\n                        this_prediction:clear_extmarks()\n                    end\n                end\n            end)\n        end\n\n        ---@type OnCurlExitedSuccessfully\n        local function on_curl_exited_successfully()\n            -- placeholder, not sure I will even need this\n        end\n\n        ---@type ExplainError\n        local function explain_error(text)\n            vim.notify(\"ERROR in new PREDICTIONS FRONTEND PoC: \" .. text, vim.log.levels.ERROR)\n        end\n\n        ---@type OnParsedSSE\n        local function on_sse_llama_server_timings(sse)\n            stats.show_prediction_stats(sse, perf)\n        end\n\n        ---@type StreamingFrontend\n        local frontend = {\n            on_parsed_data_sse = on_parsed_data_sse,\n            on_curl_exited_successfully = on_curl_exited_successfully,\n            explain_error = explain_error,\n            on_sse_llama_server_timings = on_sse_llama_server_timings,\n        }\n\n        Curl.spawn(fim_request, frontend)\n    end\n\n    if enable_rag and rag_client.is_rag_supported_in_current_file() then\n        if not vim.lsp.get_clients({ name = \"ask_language_server\", bufnr = 0 })[1] then\n            -- FYI this check of client ready, must have immaterial overhead for working clients\n            --  would be better to do no checks than slow down normal use\n            log:error(\"RAG not available in current LSP, when it should be, so, sending FIM w/o RAG\")\n            then_send_fim({})\n            return\n        end\n        -- FYI vim.lsp.get_clients is taking ~3us for case when the LSP is operational, imperceptible overhead\n\n        local this_request_ids, cancel -- declare in advance so closure can access\n        perf:rag_started()\n\n        ---@param rag_matches LSPRankedMatch[]\n        function on_rag_response(rag_matches)\n            -- can this be called twice? one for done/cancel and a race condition b/w the two?\n            -- log:info(\"on_rag_response(rag_matches:'\" .. vim.inspect(rag_matches) .. \"')\")\n\n            -- FYI unroll all rag specific safeguards here so that logic doesn't live inside send_fim\n            perf:rag_done()\n\n            -- * make sure prior (canceled) rag request doesn't still respond\n            if PredictionsFrontend.rag_request_ids ~= this_request_ids then\n                -- I bet this is why sometimes I get completions that still fire even after cancel b/c the RAG results aren't actually stopped in time on server and so they come back\n                --  and they arrive after next request started... the mismatch in request_ids will prevent that issue\n                -- log:trace(\"possibly stale rag results, skipping: \" .. vim.inspect({\n                --     global_rag_request_ids = PredictionsFrontend.rag_request_ids,\n                --     this_request_ids = this_request_ids,\n                -- }))\n                return\n            end\n\n            if PredictionsFrontend.rag_cancel == nil then\n                -- log:error(\"rag appears canceled, skipping on_rag_response...\")\n                return\n            end\n\n            then_send_fim(rag_matches)\n        end\n\n        this_request_ids, cancel = rag_client.context_query_fim(ps_chunk, on_rag_response)\n        PredictionsFrontend.rag_cancel = cancel\n        PredictionsFrontend.rag_request_ids = this_request_ids\n    else\n        PredictionsFrontend.rag_cancel = nil\n        PredictionsFrontend.rag_request_ids = nil\n        then_send_fim({})\n    end\nend\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/predictions/prediction.lua:0-19\nlocal dots = require(\"ask-openai.frontends.thinking.dots\")\nlocal HLGroups = require(\"ask-openai.hlgroups\")\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal CursorController = require \"ask-openai.predictions.cursor_controller\"\n\n---@class Prediction\n---@field id integer\n---@field buffer integer\n---@field prediction string         # content streamed so far (excluding buffered chunks)\n---@field extmarks table\n---@field paused boolean\n---@field buffered_chunks string    # chunks received while `paused`\n---@field abandoned boolean         # user aborted prediction\n---@field disable_cursor_moved boolean\n---\n---@field has_reasoning boolean\n---@field private reasoning_chunks string[]\n---\n---@field start_time number\n---@field fim_request? CurlRequest\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/predictions/backends/fim_backend.lua:0-19\nlocal log = require(\"ask-openai.logs.logger\").predictions()\nlocal CurrentContext = require(\"ask-openai.predictions.context\")\nlocal fim = require(\"ask-openai.backends.models.fim\")\nlocal qwen = fim.qwen25coder.sentinel_tokens\nlocal harmony_fim = require(\"ask-openai.backends.models.fim_harmony\")\nlocal meta = require(\"ask-openai.backends.models.meta\")\nlocal files = require(\"ask-openai.helpers.files\")\nlocal ansi = require(\"ask-openai.predictions.ansi\")\nlocal api = require(\"ask-openai.api\")\nlocal gptoss_tokenizer = require(\"ask-openai.backends.models.gptoss.tokenizer\")\n\nrequire(\"ask-openai.backends.sse.parsers\")\n\n---@class FimBackend\n---@field ps_chunk PrefixSuffixChunk\n---@field rag_matches LSPRankedMatch[]\n---@field context CurrentContext\nlocal FimBackend = {}\nFimBackend.__index = FimBackend\n\n\n"
      },{
        "role":"user",
        "content":"Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nlocal log = require(\"ask-openai.logs.logger\").predictions()\n<|fim_middle|>\n\n-- stream parser will sit BELOW a chat completions client\n-- this will buffer data fields in multiline scenarios\n-- will split on and emit events when the blank line is detected\n-- will have a clean interface for consumers to subscribe to events/done/etc\n-- this will plug into my curl module (and other chat completion endpoint clients)\n\n-- * FORMAT:\n-- https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format\n-- ^: == comment (often used as keep-alive)\n-- UTF-8\n-- mimetype: https://html.spec.whatwg.org/multipage/iana.html#text/event-stream\n-- SPEC:\n--   https://html.spec.whatwg.org/multipage/server-sent-events.html#parsing-an-event-stream\n--\n-- event (aka message)\n--   separated by a pair of newline characters\n--   delimited by blank line ==> \\n\\n\n--   have field(s)\n-- data field\n--   multiple data lines are concatenated\n--   thus far I have only observed data-only events (IOTW no other fields)\n--\n-- browser event dispatch:\n--   addEventListener() for named (typed) events\n--   onmessage() when no type\n\n---@alias SSEOnDataHandler function(data string)\n\n---Think of this as unwrapping the SSE envelope, extracting events and triggering handlers\n---for each event type, though in this case data is the only event needed (thus far)\n---\n---This should work for more than just OpenAI endpoints... any data only SSE endpoint\n---\n---@class SSEDataOnlyParser\n---@field _buffer string\n---@field _done boolean\n---@field _on_data_sse SSEOnDataHandler\nlocal SSEDataOnlyParser = {}\n\n---@param on_data_sse SSEOnDataHandler\n---@return SSEDataOnlyParser\nfunction SSEDataOnlyParser.new(on_data_sse)\n    local instance = setmetatable({}, { __index = SSEDataOnlyParser })\n    instance._buffer = \"\"\n    instance._done = false\n    instance._on_data_sse = on_data_sse\n    return instance\nend\n\n---@return string? invalid_dreg -- if the buffer has leftover, invalid JSON, return an error describing it\nfunction SSEDataOnlyParser:flush_dregs()\n    local no_dregs = not self._buffer or self._buffer == \"\"\n    if no_dregs then\n        return nil\n    end\n\n    local success, sse_parsed = safely.call(vim.json.decode, self._buffer)\n    if not success then\n        return \"invalid json in dregs: \" .. self._buffer\n    end\n\n    -- PRN? just flush it to _on_data_sse every time and let that blow up\n    -- (it has error handler in it)\n    --   if I want terminate behavior then this would be wise\n\n    -- YES I know this means the parsed JSON object is discarded and re-parsed in:\n    self._on_data_sse(self._buffer)\nend\n\n--- curl stdout should be patched into this\n---@param data string\nfunction SSEDataOnlyParser:write(data)\n    -- log:info(\"data\", vim.inspect(data))\n\n    -- FYI assumed data only => strip data: prefix\n    data = data:gsub(\"^data: \", \"\")\n\n    -- normalize line endings to LF\n    data = data:gsub(\"\\r\\n\", \"\\n\"):gsub(\"\\r\", \"\\n\")\n\n    self._buffer = self._buffer .. data\n\n    -- * look for blank line (signals end of data event)\n    -- FYI split takes plain and trimempty options\n    --   default not removing empties, can use to check if a \\n\\n was present\n    local lines = vim.split(self._buffer, \"\\n\\n\", {})\n\n    if (#lines == 1) then\n        -- no blank line (yet)\n        return\n    elseif (#lines >= 2) then\n        -- had blank line(s)\n\n        -- ==> emit completed data event(s)\n        for i = 1, #lines - 1 do\n            local event = lines[i]\n            event = event:gsub(\"^data: \", \"\") -- happens when multiple in one SSE\n            self._on_data_sse(event)\n        end\n\n        -- keep last line in buffer (it's not complete w/o a blank line)\n        self._buffer = lines[#lines]\n    end\nend\n\nreturn SSEDataOnlyParser"
      }],
    "top_p":1,
    "max_tokens":4096,
    "stream":true,
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    }
  }
}
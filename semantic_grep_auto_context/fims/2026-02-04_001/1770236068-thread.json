{
  "last_sse":{
    "timings":{
      "predicted_n":160,
      "prompt_per_second":3966.7726880992,
      "cache_n":908,
      "prompt_n":2306,
      "predicted_per_second":208.27692499948,
      "predicted_per_token_ms":4.8013,
      "predicted_ms":768.208,
      "prompt_ms":581.329,
      "prompt_per_token_ms":0.25209410234172
    },
    "system_fingerprint":"b7785-1c7cf94b2",
    "object":"chat.completion.chunk",
    "created":1770236068,
    "choices":[{
        "delta":[],
        "finish_reason":"stop",
        "index":0
      }],
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "id":"chatcmpl-44ZIiDAEXI4GD8nstgJaZlInV2yBh62S"
  },
  "response_message":{
    "role":"assistant",
    "content":"lines[0])",
    "finish_reason":"stop",
    "reasoning_content":"We need to fill print_asis( ... ) likely with something like \"lines[idx]\"? Actually after detection, they want to print something before processing headings. At that point, we are inside if raw_content starts... we have lines = raw_content.splitlines(). Then they want to print_asis something likely the first line? Probably print_asis(lines[0])? But they later skip first summary line. They likely want to print the summary line. So print_asis(lines[0])? Actually they already have raw_content starts with \"# Semantic Grep matches:\" summary line. So they want to print that line as plain. So code: print_asis(lines[0]) . Ensure indentation. So insert that."
  },
  "request_body":{
    "temperature":1,
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "options":[],
    "max_tokens":4096,
    "stream":true,
    "raw":false,
    "messages":[{
        "role":"developer",
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n"
      },{
        "role":"user",
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n"
      },{
        "role":"user",
        "content":"# Semantic Grep matches: 5\n\nThis is automatic context based on my request. These may or may not be relevant.\n## ~/repos/github/g0t4/ask-openai.nvim/tests/manual_prompting/jetbrains_mellum/base/fim-with-context-files.py:15-34\nprint(result)<fim_prefix>def calculate_sum(a, b):\n<fim_middle>\"\"\"\n\nimport requests\nimport json\n\nraw_print = print\nfrom rich import print\n\ndef vllm_completion_for(body):\n    print(\"[blue]## PROMPT\")\n    raw_print(body[\"prompt\"])\n\n    response = requests.post(\n        \"http://ollama:8000/v1/completions\",\n        headers={\"Content-Type\": \"application/json\"},\n        data=json.dumps(body),\n    )\n\n    print(\"[yellow]## OUTPUT\")\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/tests/manual_prompting/jetbrains_mellum/base/file_level_fim.py:0-13\nfrom show import show_completion_for\n\nrequest_body = {\n    \"prompt\": \"<fim_prefix>local M = {}\\n\\nfunction M.add(a, b)\\n    return a + b\\nend\\n\\n<fim_suffix>\\n\\n\\nreturn M<fim_middle>\",\n    \"options\": {\n        \"num_ctx\": 8192\n    },\n    \"raw\": True,\n    \"num_predict\": 200,\n    \"stream\": False,\n    \"model\": \"huggingface.co/JetBrains/Mellum-4b-base-gguf:latest\",\n}\n\nshow_completion_for(request_body)\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/tests/manual_prompting/jetbrains_mellum/base/fim-with-context-files.py:45-63\n    print()\n\n    print(\"[blue]## COMPLETION\")\n    completion = result[\"choices\"][0][\"text\"]\n    raw_print(completion)\n\n# FYI also works fine to add file_sep with calc.lua (as is expected)\nrequest_body = {\n    \"prompt\": prompt,\n    \"options\": {\n        \"num_ctx\": 8192\n    },\n    # \"raw\": True,\n    \"num_predict\": 300,\n    # \"stream\": False,\n    # \"model\": \"qwen2.5-coder:7b-base-q8_0\",\n}\n\nvllm_completion_for(request_body)\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/tests/manual_prompting/qwen2_5coder/file_level_fim.py:0-13\nfrom show import show_completion_for\n\nrequest_body = {\n    \"prompt\": \"<|fim_prefix|>local M = {}\\n\\nfunction M.add(a, b)\\n    return a + b\\nend\\n\\n<|fim_suffix|>\\n\\n\\nreturn M<|fim_middle|>\",\n    \"options\": {\n        \"num_ctx\": 8192\n    },\n    \"raw\": True,\n    \"num_predict\": 200,\n    \"stream\": False,\n    \"model\": \"qwen2.5-coder:7b-base-q8_0\",\n}\n\nshow_completion_for(request_body)\n\n\n## ~/repos/github/g0t4/ask-openai.nvim/tests/manual_prompting/qwen2_5coder/worked/repo_name_only_then_file_level-fim.py:0-15\nfrom show import show_completion_for\n\n# WOW WORKS fine! no impact on completions (nothing noticeable thus far)\n\nrequest_body = {\n    \"prompt\": \"<|repo_name|>maths<|fim_prefix|>local M = {}\\n\\nfunction M.add(a, b)\\n    return a + b\\nend\\n\\n<|fim_suffix|>\\n\\n\\nreturn M<|fim_middle|>\",\n    \"options\": {\n        \"num_ctx\": 8192\n    },\n    \"raw\": True,\n    \"num_predict\": 200,\n    \"stream\": False,\n    \"model\": \"qwen2.5-coder:7b-base-q8_0\",\n}\n\nshow_completion_for(request_body)\n\n"
      },{
        "role":"user",
        "content":"Please suggest text to replace <|fim_middle|>:\n\n```tools/chat_viewer/__main__.py\n                response = json.load(f)\n                if isinstance(response, dict):\n                    response[\"output.json\"] = True\n                    messages.append(response)\n\n    return messages\n\ndef load_messages(data) -> list[dict[str, Any]]:\n    if isinstance(data, list):\n        # * only has list of messages\n        return data\n    if isinstance(data, dict):\n        if \"request_body\" in data:\n            # * thread.json has request_body.messages\n            data = data[\"request_body\"]\n        if \"messages\" in data:\n            # typical request body, has messages, tools, temp, etc\n            messages = data[\"messages\"]\n            del data[\"messages\"]\n            # FYI print other properties at the top (i.e. tools)... if some of these nag me I can always write handlers for them to make them pretty too\n            #   primary is going to be tools list and that tends to look good as is in JSON b/c it is itself a JSON schema\n            print_section_header(\"UNPROCESSED request.body properties\", color=\"cyan\")\n            pprint_asis(data)\n            return messages\n    return []\n\ndef load_thread_messages_from_stream(stream) -> list[dict[str, Any]]:\n    # assume stream can be:\n    #   jq .messages | this\n    #   cat *-thread.json | this\n    #   cat input-messages.json | this\n    data = json.load(stream)\n    messages = load_messages(data)\n\n    if \"response_message\" in data:\n        # * thread.json\n        response = data[\"response_message\"]\n        if isinstance(response, dict):\n            response[\"output.json\"] = True\n            messages.append(response)\n    # FYI cannot assume output.json is related (or any othe file) given this data is from STDIN\n\n    return messages\n\ndef insert_newlines(content: str) -> str:\n    return content.replace(\"\\\\n\", \"\\n\")\n\ndef _format_json(content: Any) -> str:\n    try:\n        return json.dumps(content, indent=2)\n    except Exception:\n        return str(content)\n\ndef _extract_content(msg: dict) -> str:\n    content = msg.get(\"content\", \"\")\n    if isinstance(content, dict) and \"text\" in content:\n        return content[\"text\"]\n    return content  # type: ignore\n\ndef _format_content(content: Any) -> str:\n    if isinstance(content, str):\n        return insert_newlines(content)\n    return _format_json(content)\n\ndef print_markdown_content(msg: dict, role: str):\n    \"\"\"Render markdown content from a message.\n\n    If the markdown originates from an automatic RAG context (it starts with\n    ``# Semantic Grep matches:``), we split the content on file headings of the\n    form ``## /path/to/file:line-start-line-end``. For each section we check the\n    file against the pre‑approved list; unapproved files are displayed, while\n    approved ones are hidden unless the ``--all`` flag is used. Unhidden sections\n    are rendered as a fenced code block with a language derived from the file\n    extension, preserving the original text.\n    \"\"\"\n    raw_content = _extract_content(msg)\n\n    # Detect auto‑generated RAG context blocks.\n    if raw_content.startswith(\"# Semantic Grep matches:\"):\n        lines = raw_content.splitlines()\n        print_asis(<|fim_middle|>\n        # Skip the first summary line.\n        idx = 1\n        while idx < len(lines):\n            header = lines[idx]\n            match = re.match(r\"^##\\s+(.+?):(\\d+)-(\\d+)\", header)\n            if not match:\n                # If the line does not match a file heading, just move on.\n                idx += 1\n                continue\n\n            file_path = match.group(1)\n            # Collect the following lines until the next heading or end of input.\n            idx += 1\n            snippet_lines: list[str] = []\n            while idx < len(lines) and not lines[idx].startswith(\"## \"):\n                snippet_lines.append(lines[idx])\n                idx += 1\n            snippet = \"\\n\".join(snippet_lines).strip(\"\\n\")\n\n            # Skip pre‑approved files unless the user asked to show all.\n            if not SHOW_ALL_FILES and file_path and is_preapproved(str(file_path)):\n                continue\n\n            # Render a heading for the match, including the line range.\n            start_line = match.group(2)\n            end_line = match.group(3)\n            _console.print(f\"\\n## MATCH {file_path}:{start_line}-{end_line}\")\n            # Determine a language based on the file extension for syntax highlighting.\n            ext = os.path.splitext(file_path)[1].lstrip('.').lower()\n            # Use a fenced code block; Syntax provides colourised output.\n            syntax = Syntax(snippet, ext or \"text\", theme=\"ansi_dark\")\n            print_asis(syntax)\n        return\n\n    # Default handling – render the entire content as markdown.\n    highlighted = Syntax(raw_content, \"markdown\", theme=\"ansi_dark\")\n    Console().print(highlighted)\n\ndef decode_if_json(content):\n    if isinstance(content, str):\n        try:\n            # return dict/object (or w/e it parses into, if it parses)\n            return json.loads(content)\n        except Exception:\n            # return as str if parse fails\n            return insert_newlines(content)\n\n    # keep w/e type (dict, list, etc... don't care)\n    return content\n\ndef print_rag_matches(content):\n    has_rag_matches = \"matches\" in content and isinstance(content[\"matches\"], list)\n    if not has_rag_matches:\n        return False\n    matches = content[\"matches\"]\n    counter = 1  # show counter for easily tracking where I am at in the list\n    for match in matches:\n\n        file = match.get(\"file\")\n        text = match.get(\"text\", \"\")\n        # FYI no need to show other fields, just file/text are relevant for review, nor warn...\n        # unless some other tool at some point has similar matches list and I'd be hiding something\n\n        # Skip pre‑approved files unless the user explicitly asked for all.\n        if not SHOW_ALL_FILES and file and is_preapproved(str(file)):\n            # TODO integrate preapproved file filters elsewhere (i.e. role=user messages have a list of rag matches too in a markdown format                                                        u\n            continue\n\n        if file:\n            _console.print(f\"\\n## MATCH {counter}: [bold]{file}[/]\")\n        if isinstance(text, str):\n            ext = os.path.splitext(file)[1].lstrip('.').lower() if file else \"\"\n            # YES! this is why this review tool rocks... language specific syntax highlighting!\n            syntax = Syntax(text, ext or \"text\", theme=\"ansi_dark\", line_numbers=False)\n            print_asis(syntax)\n        else:\n            _console.print(f\"[red bold]UNEXPECTED 'text' field type (rag matches s/b str only):[/]\")\n            pprint_asis(text)\n\n        counter += 1\n```\n\n## Reminders about the cursor line:\nText before cursor: `        print_asis(`"
      }],
    "model":"gpt-oss:120b",
    "top_p":1
  }
}
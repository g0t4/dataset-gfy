{
  "last_sse":{
    "timings":{
      "predicted_per_token_ms":4.5848367346939,
      "cache_n":0,
      "predicted_n":147,
      "prompt_per_second":8510.6592680932,
      "predicted_per_second":218.11027477443,
      "prompt_n":1727,
      "prompt_ms":202.922,
      "prompt_per_token_ms":0.1174997104806,
      "predicted_ms":673.971
    },
    "__verbose":{
      "timings":{
        "predicted_per_token_ms":4.5848367346939,
        "cache_n":0,
        "predicted_n":147,
        "prompt_per_second":8510.6592680932,
        "predicted_per_second":218.11027477443,
        "prompt_n":1727,
        "prompt_ms":202.922,
        "prompt_per_token_ms":0.1174997104806,
        "predicted_ms":673.971
      },
      "generation_settings":{
        "stream":true,
        "max_tokens":4096,
        "frequency_penalty":0,
        "dry_multiplier":0,
        "dry_base":1.75,
        "dry_allowed_length":2,
        "dry_penalty_last_n":131072,
        "dry_sequence_breakers":["\n",":","\"","*"],
        "mirostat":0,
        "mirostat_tau":5,
        "mirostat_eta":0.10000000149012,
        "n_predict":4096,
        "ignore_eos":false,
        "logit_bias":[],
        "n_probs":0,
        "min_keep":0,
        "top_k":40,
        "repeat_penalty":1,
        "preserved_tokens":[200003,200005,200006,200007,200008],
        "chat_format":"GPT-OSS",
        "top_p":1,
        "temperature":1,
        "thinking_forced_open":false,
        "samplers":["penalties","dry","top_n_sigma","top_k","typ_p","top_p","min_p","xtc","temperature"],
        "n_discard":0,
        "n_keep":0,
        "stop":[],
        "speculative.n_max":16,
        "speculative.n_min":0,
        "speculative.p_min":0.75,
        "seed":4294967295,
        "dynatemp_range":0,
        "dynatemp_exponent":1,
        "min_p":0.050000000745058,
        "top_n_sigma":-1,
        "xtc_probability":0,
        "xtc_threshold":0.10000000149012,
        "typical_p":1,
        "lora":[],
        "backend_sampling":false,
        "post_sampling_probs":false,
        "timings_per_token":false,
        "reasoning_in_content":false,
        "reasoning_format":"deepseek",
        "grammar_triggers":[],
        "grammar_lazy":false,
        "grammar":"",
        "presence_penalty":0,
        "repeat_last_n":64
      },
      "tokens":[],
      "id_slot":0,
      "tokens_predicted":147,
      "tokens_evaluated":1727,
      "stop_type":"eos",
      "stopping_word":"",
      "truncated":false,
      "tokens_cached":1873,
      "model":"ggml-org/gpt-oss-120b-GGUF",
      "has_new_line":true,
      "content":"",
      "prompt":"<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2026-02-14\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: auto-edit-suggests\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n<|end|><|start|>user<|message|>Please suggest text to replace <|fim_middle|>:\n\n```auto_edit/ai/suggests.py\n\n<|fim_middle|>\ndef suggests(use_ranges:Ranges, lines_to_remove: dict[int, str] | None = None):\n    id = 0\n    segments = \"Here are the lines, the line number is at the start of the line followed by a colon (i.e. '0:'), by the way some lines have multiple segments so consider them all-or-none for removal:\"\n    for content in use_ranges.content():\n        content.id = id\n        id += 1\n        result = \"\"\n        for t in content.transcriptions:\n            result += f\"{t.text} \"\n        segments += f\"\\n{content.id}: {result}\"\n        #\n        # FAKE a duplicate for gptoss to detect\n        # if id == 7:\n        #     # Fake a dup\n        #     segments += ('8: Just instant')\n        #     id+=1\n    rich.print(\"transcripts\", segments)\n\n    from openai import OpenAI\n\n    client = OpenAI(api_key=\"foo\", base_url=\"http://build21:8013\")\n\n    try:\n        completion = client.chat.completions.create(\n            model=\"\",  # set empty so no mistaken assumptions llama-server doesn't have a way to set the model on the request... it just came out with that feature and I am not yet using it, not sure if I will use it or not\n            messages=[{\n                \"role\": \"system\",\n                \"content\": \"You are a video editing expert, I need you to suggest some edits to make based on my transcript of my video I am editing. I just need you to suggest which lines have a retake on them that should be removed. Retake means I stumbled and then repeated part of what I said before, mostly the same. By the way each line was transcribed based on splitting the clip on silence periods (i.e. breathing, pauses) so a single sentence/thought can be split across multiple lines, that is normal, do not remove lines b/c they look incomplete. I don't want to hear anything about what you think about the transcript other than if you think there's a retake. That means, not a peep about lines to keep... I will remove all lines that are in your response... so only includ retakes in the list. An empty response means there are no retakes! Thank you! In your response, list each number on its own line with a colon and then a concise reason why it should be removed... i.e. '1: retake\\n4: redundant\\n\"\n            }, {\n                \"role\": \"user\",\n                \"content\": f\"{segments}\"\n            }],\n            #\n            # We recommend sampling with temperature=1.0 and top_p=1.0.\n            #    https://github.com/openai/gpt-oss?tab=readme-ov-file#recommended-sampling-parameters\n            temperature=1.0,\n            top_p=1.0,\n            extra_body={\n                \"chat_template_kwargs\": {\n                    # confirmed this is passed! And medium/hard seem to wwork well to not report retakes when there are none!\n                    \"reasoning_effort\": \"medium\",\n                },\n            },\n            #\n            max_tokens=16384,\n            n=1,  # default\n        )\n\n        response = completion.choices[0].message.content\n        print(\"model response:\", response)\n\n    except Exception as e:\n        response = \"\"\n        print(f\"{e}\")\n\n    lines_to_remove = response.split(\"\\n\")\n    dict_lines_to_remove = {}\n    for l in lines_to_remove:\n        parts = l.split(\":\")\n        if len(parts) == 2:\n            dict_lines_to_remove[int(parts[0])] = parts[1]\n\n    # if segment is removed, then remove its silence period AFTER but not BEFORE it\n    # - b/c silence after mostly correlates to prior segement (i.e. trailing letters)\n    # - whereas cuts almost always work well at very start of next segment b/c segments start abruptly unlike how they end\n\nelse:\n    dict_lines_to_remove = {}\n\n\n```\n\n## Reminders about the cursor line:\n<|end|><|start|>assistant",
      "stop":true,
      "index":0
    },
    "created":1771135031,
    "id":"chatcmpl-6FTZDTqIDXOq6ZvOrdJnFLCBrVe5Ocni",
    "choices":[{
        "finish_reason":"stop",
        "delta":[],
        "index":0
      }],
    "model":"ggml-org/gpt-oss-120b-GGUF",
    "object":"chat.completion.chunk",
    "system_fingerprint":"b7785-1c7cf94b2"
  },
  "response_message":{
    "finish_reason":"stop",
    "reasoning_content":"We need to insert import or definitions before function. Likely need import typing: Ranges. Provide placeholder. Probably need to import rich. Also define Ranges type. Suggest:\n\nfrom typing import Dict, List, Optional\n\nfrom .types import Ranges? Not sure. Just add imports.\n\nInsert before function:\n\nfrom typing import Dict, Optional\nimport rich\n\nAdd maybe type alias.\n\nLet's output code.",
    "role":"assistant",
    "content":"import rich\nfrom typing import Dict, Optional, List, Tuple\n\n# Assuming Ranges is defined elsewhere in the project; if not, you may need to import or define it.\n# from .ranges import Ranges  # Uncomment and adjust the import path as necessary.",
    "_logged":true
  },
  "request_body":{
    "raw":false,
    "stream":true,
    "messages":[{
        "content":"You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position: <|fim_middle|>\n\nThe surrounding code is limited, it may not be the full file. Focus on the code near <|fim_middle|>\nDo NOT explain your decisions. Do NOT return markdown blocks ```\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace. Especially on the cursor line!\nYOU ARE ONLY INSERTING CODE. DO NOT REPEAT PREFIX. DO NOT REPEAT SUFFIX.\nComplete as little or as much code as is necessary to help the user. That means part of a line, a full line or even multiple lines! Goal is to help the user as much as is possible and reasonable.\n\nHere are a few examples of tricky completions:\n\n## Example: suggest middle of line (cursor has existing code before and after it)\n\n```python\ndef area(width, height):\n    return <|fim_middle|> * height\n```\n\nSay this is your desired result:\n```python\ndef area(width, height):\n    return width * height\n```\n\n1. then the CORRECT completion is (do not include backticks, those are just to delimit included whitespace):\n`width`\n\n2. WRONG (because repeats the suffix):\n`width * height`\nresults in:\n`    return width * height * height`\n\n3. WRONG (because repeats both suffix and prefix):\n`    return width * height`\nresults in:\n`    return     return width * height * height`\n\n\n## Example: cursor is indented\n\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    <|fim_middle|>\n    end\nend\n```\n\n*Keep in mind, the indent will be there before the suggested code too.*\n\nSay this is your desired result:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n1. then the CORRECT completion is:\n```lua\nelse\n        print(\"Non‑positive\")\n```\n\n2. WRONG (`else` ends up double indented when it should be single):\n```lua\n    else\n        print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n        else\n        print(\"Non‑positive\")\n    end\nend\n```\n\n3. WRONG (`print` ends up single indented when it should be double):\n```lua\nelse\n    print(\"Non‑positive\")\n```\nresults in:\n```lua\nfunction print_sign(number)\n    if number > 0 then\n        print(\"Positive\")\n    else\n    print(\"Non‑positive\")\n    end\nend\n```\n\n\n",
        "role":"developer",
        "_logged":true
      },{
        "content":"Here is context that's automatically provided, that MAY be relevant.\nrepo: auto-edit-suggests\n\n## General project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n\n## General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n\n## Python Code Preferences\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer `list[]` over `List[]` to mimimize imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n",
        "role":"user",
        "_logged":true
      },{
        "content":"Please suggest text to replace <|fim_middle|>:\n\n```auto_edit/ai/suggests.py\n\n<|fim_middle|>\ndef suggests(use_ranges:Ranges, lines_to_remove: dict[int, str] | None = None):\n    id = 0\n    segments = \"Here are the lines, the line number is at the start of the line followed by a colon (i.e. '0:'), by the way some lines have multiple segments so consider them all-or-none for removal:\"\n    for content in use_ranges.content():\n        content.id = id\n        id += 1\n        result = \"\"\n        for t in content.transcriptions:\n            result += f\"{t.text} \"\n        segments += f\"\\n{content.id}: {result}\"\n        #\n        # FAKE a duplicate for gptoss to detect\n        # if id == 7:\n        #     # Fake a dup\n        #     segments += ('8: Just instant')\n        #     id+=1\n    rich.print(\"transcripts\", segments)\n\n    from openai import OpenAI\n\n    client = OpenAI(api_key=\"foo\", base_url=\"http://build21:8013\")\n\n    try:\n        completion = client.chat.completions.create(\n            model=\"\",  # set empty so no mistaken assumptions llama-server doesn't have a way to set the model on the request... it just came out with that feature and I am not yet using it, not sure if I will use it or not\n            messages=[{\n                \"role\": \"system\",\n                \"content\": \"You are a video editing expert, I need you to suggest some edits to make based on my transcript of my video I am editing. I just need you to suggest which lines have a retake on them that should be removed. Retake means I stumbled and then repeated part of what I said before, mostly the same. By the way each line was transcribed based on splitting the clip on silence periods (i.e. breathing, pauses) so a single sentence/thought can be split across multiple lines, that is normal, do not remove lines b/c they look incomplete. I don't want to hear anything about what you think about the transcript other than if you think there's a retake. That means, not a peep about lines to keep... I will remove all lines that are in your response... so only includ retakes in the list. An empty response means there are no retakes! Thank you! In your response, list each number on its own line with a colon and then a concise reason why it should be removed... i.e. '1: retake\\n4: redundant\\n\"\n            }, {\n                \"role\": \"user\",\n                \"content\": f\"{segments}\"\n            }],\n            #\n            # We recommend sampling with temperature=1.0 and top_p=1.0.\n            #    https://github.com/openai/gpt-oss?tab=readme-ov-file#recommended-sampling-parameters\n            temperature=1.0,\n            top_p=1.0,\n            extra_body={\n                \"chat_template_kwargs\": {\n                    # confirmed this is passed! And medium/hard seem to wwork well to not report retakes when there are none!\n                    \"reasoning_effort\": \"medium\",\n                },\n            },\n            #\n            max_tokens=16384,\n            n=1,  # default\n        )\n\n        response = completion.choices[0].message.content\n        print(\"model response:\", response)\n\n    except Exception as e:\n        response = \"\"\n        print(f\"{e}\")\n\n    lines_to_remove = response.split(\"\\n\")\n    dict_lines_to_remove = {}\n    for l in lines_to_remove:\n        parts = l.split(\":\")\n        if len(parts) == 2:\n            dict_lines_to_remove[int(parts[0])] = parts[1]\n\n    # if segment is removed, then remove its silence period AFTER but not BEFORE it\n    # - b/c silence after mostly correlates to prior segement (i.e. trailing letters)\n    # - whereas cuts almost always work well at very start of next segment b/c segments start abruptly unlike how they end\n\nelse:\n    dict_lines_to_remove = {}\n\n\n```\n\n## Reminders about the cursor line:\n",
        "role":"user",
        "_logged":true
      }],
    "top_p":1,
    "max_tokens":4096,
    "temperature":1,
    "options":[],
    "chat_template_kwargs":{
      "reasoning_effort":"low"
    },
    "model":"gpt-oss:120b"
  }
}
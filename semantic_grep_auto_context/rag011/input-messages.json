{
  "raw": false,
  "model": "gpt-oss:120b",
  "chat_template_kwargs": {
    "reasoning_effort": "low"
  },
  "options": [],
  "stream": true,
  "top_p": 1,
  "max_tokens": 4096,
  "temperature": 1,
  "messages": [
    {
      "role": "developer",
      "content": "You are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n"
    },
    {
      "role": "user",
      "content": "Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect_tests.py:42-52\ndef silence_seg(start, end):\n    global id_counter\n    seg = DetectedSegment(\n        id=id_counter,\n        start=start,\n        end=end,\n        label=SegmentLabel.SILENCE,\n        rms_segment=np.array([id_counter]),\n    )\n    id_counter += 1\n    return seg\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/player.py:0-16\nimport numpy as np\nimport sounddevice as sd\nfrom auto_edit.new_silence.detect import *\nfrom auto_edit.new_silence.plot_helpers import setup_events\n\ndef plot_segments(analysis: AudioAnalysisResult, segments: list[DetectedSegment] | None = None) -> None:\n    import matplotlib.pyplot as plt\n\n    # Prepare data for stacked plot\n    segments = segments or analysis.segments\n    segment_starts = [seg.start for seg in segments]\n    segment_ends = [seg.end for seg in segments]\n    segment_rms = [seg.rms_segment for seg in segments]\n\n    # * plot line per segment:\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/plot_helpers.py:0-4\nimport subprocess\n\ndef setup_events(fig, ax, on_click):\n\n    \n\n## ~/repos/github/g0t4/private-auto-edit-suggests/generate_fcpxml.py:139-222\ndef test_ffmpeg_detect_silence():\n    # TODO do I have an off by one problem with frames # vs actual time?\n    #   I noticed plot seems to show tick marks slightly off at least (might need shifted one left.. or smth with how I calculate the first frame's time vs number)\n    #     or might be in how I compute spacing of major/minor ticks (notably the label on major ones every X frames)\n    #   I consider frame 0 to be at start of video...\n    #   frame 1 is the second frame then\n\n    # *** one issue, using mp4 approach results in slight shift right for audio silence timings... by 0.012 (12ms)\n    #   *** this will affect the plot below, don't obsess so much about how the start/end touches the start/end of talking (not too closesly b/c there's 12ms shift to consider... perhaps I should shift all the silences by 12ms to do that comparison?)\n    #   actually there's a 1 frame gap! I suspect b/c I did -ss 0 and the original mp4 must have negative video frame or shifted audio that starts at 1 frame in?\n    #      the 0.012 is just the diff on first silence start (which truncates the 0.033 b/c that might've been negative in the audio file for say 0.021 ms ish\n    #      BUT, if I redo the crop on the first30 to mp4 and then first30.mp4 => first30.wav then the gap is just 0.012 (12ms)!\n    # which_wav = '~dbox/auto-edit-videos/2025-dec-silence-work/fim-gptoss-video/redovideotoo/first30redone.from.mp4.without-params.to.wav'\n    which_wav = '~dbox/auto-edit-videos/2025-dec-silence-work/fim-gptoss-video/redovideotoo/first30redone.from.mp4.with-ss0-to30-again.wav'\n    wav_silence_periods = ffmpeg_detect_silence_raw(which_wav, silence_threshold=-28, silence_duration=0.15)\n    rich.print(\"wav silence periods\", wav_silence_periods)\n    rich.print(\"mp4 silence periods\", raw_silences)\n    print(\"compared wav vs mp4 timing differences:\")\n    for wav, mp4 in zip(wav_silence_periods, raw_silences):\n        print(f\"  wav: {wav.start_seconds:.3f}-{wav.end_seconds:.3f}  mp4: {mp4.start_seconds:.3f}-{mp4.end_seconds:.3f}\")\n        start_diff = wav.start_seconds - mp4.start_seconds\n        end_diff = wav.end_seconds - mp4.end_seconds\n        print(f\"    start diff: {start_diff:.3f}s, end diff: {end_diff:.3f}s\")\n\n    import soundfile as sf\n    import numpy as np\n    import matplotlib.pyplot as plt\n\n    audio, sample_rate = sf.read(which_wav)\n    if audio.ndim > 1:\n        print(f\"detected 2+ channels {audio.shape}, computing mono via mean\")\n        audio = audio.mean(axis=1)  # convert to mono\n\n    # ? is positive or negative what I want?\n    #  the mp4 file reports first silence starts at 0.012 ms\n    #    whereas waveform has 0ms\n    #    then for the end, the waveform is about 0.12ms earlier than the mp4 end\n    #  so yeah 12ms in is like zero in the waveform data... perfect!\n    start_at_seconds = 0.012  # mp4's first silence reading starts at 0.012 (12ms) so I can use that as the start time so the waveform aligns with other readings (i.e. reading the silence start/stop times/frames)\n    time_axis = np.linspace(start_at_seconds, len(audio) / sample_rate, num=len(audio))\n\n    # --- Plot ---\n    plt.figure(figsize=(10, 4))\n    plt.plot(time_axis, audio)\n\n    mp4_raw_silences = raw_silences\n    mp4_aligned_silences = silences\n\n    for silence, aligned in zip(mp4_raw_silences, mp4_aligned_silences):\n        # plot both intervals: red/blue => thus overlap will be purple!\n        plt.axvspan(silence.start_seconds, silence.end_seconds, color=\"red\", alpha=0.3)\n        start_seconds = aligned.start_frames / original.fps\n        end_seconds = aligned.end_frames / original.fps\n        plt.axvspan(start_seconds, end_seconds, color=\"blue\", alpha=0.2)\n\n        start_diff = silence.start_seconds - start_seconds\n        end_diff = silence.end_seconds - end_seconds\n        rich.print(f\"a: {aligned.start_frames:03d}-{aligned.end_frames:03d} \"\n                   f\"({start_seconds:.3f}s-{end_seconds:.3f}s)\")\n        rich.print(f\"       raw: {silence.start_seconds:.3f}s-{silence.end_seconds:.3f}s\")\n        rich.print(f\"      diff: {start_diff:.3f}s-{end_diff:.3f}s\")\n\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n    plt.title(\"Waveform\")\n    x_start = 23.7\n    x_end = 24.5\n    plt.xlim(x_start, x_end)  # adjust start and end time as needed\n    plt.ylim(-1, 1)  # adjust amplitude range as needed\n    frame_interval_s = 1 / original.fps\n    start_tick = np.ceil(x_start / frame_interval_s) * frame_interval_s\n    ticks = np.arange(start_tick, x_end + frame_interval_s, frame_interval_s)\n\n    major_tick_interval = 5\n    major_ticks = ticks[::major_tick_interval]\n    major_labels = [f\"{int(t * original.fps)}\" for t in major_ticks]\n\n    ax = plt.gca()\n    ax.set_xticks(major_ticks)\n    ax.set_xticklabels(major_labels)\n    ax.set_xticks(ticks, minor=True)\n    ax.tick_params(axis=\"x\", which=\"minor\", length=4)\n    plt.tight_layout()\n    plt.show()\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/tests/aligned-tests.py:24-74\nclass TestIntegration:\n\n    # TODO! add more integration tests\n\n    def test_detect_split_breathing_into_two_silences(self):\n        # TODO find how I want to fix this and add that to finish this test\n\n        # ffmpeg -i full.mp4 -ss 00:00:09.0 -to 00:00:12.0 breathing-split-silence.mp4\n        # FYI test w/o shifting timing and messing up the split:\n        #   video_editing_gen_fcpxml first30redone.mp4\n\n        raw_silences = ffmpeg_detect_silence_raw(\n            gptoss.split_breathing,\n            silence_threshold=-28,\n            silence_duration=0.15,\n        )\n        assert len(raw_silences) == 2\n\n        import rich\n        print()  # \\n\n        [rich.print(silence) for silence in raw_silences]\n        # Raw[0.794542➜0.963646)\n        # Raw[0.963667➜1.314271)\n\n        left = raw_silences[0]\n        right = raw_silences[1]\n\n        gap = right.start_seconds - left.end_seconds\n        # rich.print(f\"{gap=}\")\n        assert gap == pytest.approx(0.000021)\n\n        # WOW in this case it's a blip of 30us (micro seconds)\n        #   IMO best course of action is, if there's a non-silence for < 1 frame, surrounded by two silence ranges... then merge them\n        #   BTW IIRC this is a rare occurence to have the split... more common to just miss a breathing b/c it is all above threshold (or enough is that there's no minimum silence of 0.15s detected)\n\n        aligned = align_silences_to_frame_boundaries_asymmetric_pad_WIP(raw_silences, 30, left_padding_frames=0)\n        assert len(aligned) == 1\n        # TODO make this work! need to join small gaps\n\n        # raise RuntimeError(\"NOT YET IMPLEMENTED\")\n\n        #\n        # # Example alignment call – replace with actual parameters as needed\n        # aligned = align_silences_to_frame_boundaries_asymmetric_pad_WIP(\n        #     raw_silences, fps=30, frame_offset=0\n        # )\n        # assert isinstance(aligned, list)\n        # assert all(hasattr(s, \"start_frames\") and hasattr(s, \"end_frames\") for s in aligned)\n\n    def test_find_ratio_for_automatically_deciding_threshold_for_silence_detection(self):\n        pass\n"
    },
    {
      "role": "user",
      "content": "Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nfrom auto_edit.new_silence.detect import *\nfrom auto_edit.new_silence.plot_helpers import setup_events\n<|fim_middle|>\nfrom auto_edit.tests.test_files import gptoss\nfrom numpy.typing import NDArray\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport rich\nimport sounddevice as sd\n\n# what_file = gptoss.minute1\nwhat_file = gptoss.full\n\n# %%\n\nsilence_threshold = np.float64(-40.0)\nbreath_threshold = np.float64(-22.0)\nanalysis = speech_analysis(what_file, silence_threshold, breath_threshold)\n\nrich.print(f'{analysis.silence_threshold=}')\nrich.print(f'{analysis.breath_threshold=}')\n\nanalysis.merged = merge_close_speech_segments(analysis, max_gap=0.1)\nlabel_bounces(analysis.merged)  # TODO move to analysis\n\n# %%\n\ndef play_merged_segments_by_id_range(start_id, end_id):\n    segments = [s for s in analysis.merged if s.id >= start_id and s.id <= end_id]\n    rich.print(segments)\n    play_segments(analysis, segments)\n\ndef play_time_range(start, end):\n    segs = [s for s in analysis.merged if start < s.end and s.start < end]\n    rich.print(segs)\n    play_segments(analysis, segs)\n\ndef mute_and_play_segments(analysis: AudioAnalysisResult, segments: list[DetectedSegment]) -> None:\n    combined = []\n    for segment in segments:\n        start = int(segment.start * analysis.sample_rate)\n        end = int(segment.end * analysis.sample_rate)\n        if segment.label in (SegmentLabel.SILENCE, SegmentLabel.BREATH):\n            # combined.append(analysis.audio_samples[start:end])\n            # continue\n            last_sample = combined[-1][-1] if combined else 0\n            # cannot drop to zero... must fade down?! wtf?\n            # mute_slice = [-40 for r in segment.rms_segment]\n            mute_slice = [last_sample for r in segment.rms_segment]\n            print(mute_slice)\n            # return\n            # mute_slice = np.full(end - start, -70.0, dtype=analysis.audio_samples.dtype)\n            combined.append(mute_slice)\n        else:\n            combined.append(analysis.audio_samples[start:end])\n    if combined:\n        audio = np.concatenate(combined)\n        print(f'{len(audio)=}')\n        sd.play(audio, analysis.sample_rate)\n        sd.wait()\n\n# %%\n\n# analysis.list_by_label(SegmentLabel.SILENCE)\n\n# for s in analysis.merged:\n#     print(s)\n\n# play_merged_segments_by_id_range(5909, 5911)\n# plot_segments(analysis, analysis.merged[50:100])\n# play_time_range(43.2, 43.6)\n# play_segments(analysis, [s for s in analysis.merged if s.label == SegmentLabel.SILENCE and s.duration < 0.03])\n# mute_and_play_segments(analysis, [s for s in analysis.merged if s.start > 7 and s.start < 14])\nplot_segments(analysis, [s for s in analysis.merged if s.start > 6 and s.start < 80])\n#"
    }
  ]
}

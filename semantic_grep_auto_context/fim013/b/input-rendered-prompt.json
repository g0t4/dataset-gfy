{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1766443707,
  "id": "chatcmpl-039pWK3uQE3Lkw33Vr0SGwzFgL8hHcwb",
  "model": "ggml-org/gpt-oss-120b-GGUF",
  "system_fingerprint": "b7453-8faa87db0",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 483,
    "prompt_n": 2014,
    "prompt_ms": 294.595,
    "prompt_per_token_ms": 0.1462735849056604,
    "prompt_per_second": 6836.504353434376,
    "predicted_n": 136,
    "predicted_ms": 718.331,
    "predicted_per_token_ms": 5.281845588235294,
    "predicted_per_second": 189.32776115746083
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 3,
    "stop": true,
    "model": "ggml-org/gpt-oss-120b-GGUF",
    "tokens_predicted": 136,
    "tokens_evaluated": 2497,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-22\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: private-auto-edit-suggests\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Python Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Use type hints to clear up ambiguity and improve code completion.\n- prefer list[] over List[] to mimimizes imports\n- Avoid docstrings, ABSOLUTELY NO trivial comments.\n\n# Semantic Grep matches: 5\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detect.py:0-19\nfrom io import StringIO\nfrom rich.console import Console\nfrom rich.text import Text\nfrom traitlets.config.configurable import SingletonConfigurable\nfrom auto_edit.new_silence.gaps import build_gap_decisions\nfrom auto_edit.new_silence.timer import DurationPrinter\nfrom auto_edit.tests.test_files import gptoss_fim_video\n\nfrom auto_edit.new_silence.detects.segments import (\n    SegmentLabel,\n    MonoAudioSamplesArray,\n    SampleRate,\n    RMSValues,\n    Float32Array,\n    DetectThresholds,\n    AudioAnalysisResult,\n    DetectedSegment,\n    CombinedSegments,\n    DEFAULT_THRESHOLDS,\n    RMS_HOP_LENGTH,\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/timer.py:21-40\n@contextmanager\ndef DurationPrinter(section: str, only_if_slow_ms: Optional[int] = None) -> None:\n\n    start_time: float = time.time()\n    try:\n        yield\n    finally:\n        end_time: float = time.time()\n        elapsed_seconds = end_time - start_time\n        elapsed_ms = elapsed_seconds * 1000\n\n        always_log = only_if_slow_ms is None\n        if always_log or elapsed_ms > only_if_slow_ms:\n\n            if elapsed_seconds < 1.0:\n                formatted_time = f\"{elapsed_ms:.2f}ms\"\n            else:\n                formatted_time = f\"{elapsed_seconds:.3f}s\"\n\n            rich.print(f\"Completed {section=} in [bold cyan]{formatted_time}[/]\")\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detector_notebook.py:0-19\n# * auto reload changed modules\n# BTW there's pretty much no fucking way to get yapf to work w/ ipython magicks... FUCK for real people? \n#  nobody bothered to think about this? why doesn't yapf have a \"IGNORE THIS FUCKING LINE ENTIRELY option?\"\n%load_ext autoreload\n%autoload complete\n\nimport rich\nimport matplotlib.pyplot as plt\nfrom auto_edit.tests.test_files import gptoss_fim_video\nfrom auto_edit.new_silence.player import SegmentPlotter\nfrom auto_edit.new_silence.detect import *\n\nwhat_file = gptoss_fim_video.minute2\n# what_file = gptoss_fim_video.split_breathing\nanalysis = full_analysis(what_file)\n\n# %%\n\none = analysis.windows[3]\nplotter = SegmentPlotter.for_window(analysis, one)\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/humanize.py:0-19\ndef seconds_to_timecode(seconds: float, fps: int) -> str:\n    total_frames = round(seconds * fps, 2)\n    hours = int(total_frames // (fps * 3600))\n    minutes = int((total_frames // (fps * 60)) % 60)\n    secs = int((total_frames // fps) % 60)\n    frames = total_frames % fps\n\n    # return f\"{hours:02}:{minutes:02}:{secs:02}:{frames:05.2f}\"\n\n    timecode = \"\"\n    if hours > 0:\n        timecode += f\"{hours:02}:\"\n\n    if minutes > 0:\n        timecode += f\"{minutes:02}:\"\n\n    timecode += f\"{secs:02}:{frames:05.2f}\"\n\n    # PRN could add toggle to strip or not (for alignment)\n    # strip trailing .00\n\n\n## ~/repos/github/g0t4/private-auto-edit-suggests/auto_edit/new_silence/detects/test_humanize.py:4-32\n@pytest.mark.parametrize(\n    \"seconds, fps, expected\",\n    [\n        # Simple seconds without minutes, no fractional frames\n        (5, 30, \"05:00\"),\n        #\n        # fractional seconds of 0.5 => maps to exact frame count of 15\n        (5.5, 30, \"05:15\"),\n        #\n        # fraction of frame shows as decimal\n        (5.61, 30, \"05:18.30\"),\n        #\n        # Minutes present\n        (125, 24, \"02:05:00\"),\n        #\n        # Minutes with frames (due to fraction of a second)\n        (125.75, 24, \"02:05:18\"),\n        #\n        # rounds to nearest frame\n        (1.0333, 30, \"01:01\"),  # 0.0333*30â‰ˆ1 frame, rounds to 1 frame past 1 second\n        #\n        (1.05, 30, \"01:01.50\"),  # 0.05*30=1.5 -> whole 1 frame, fraction .5 -> .50\n        #\n        # hours (3600 = 1 hour) => 65s left => (60s = 1min) => 5s left => done\n        (3665, 30, \"01:01:05:00\"),\n    ],\n)\ndef test_seconds_to_timecode(seconds, fps, expected):\n    assert seconds_to_timecode(seconds, fps) == expected\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nimport time\n<|fim_middle|>\n\nclass Standalone:\n\n    @staticmethod\n    def play_to_frame(analysis: AudioAnalysisResult, segment: DetectedSegment) -> None:\n        # FYI this is currently unused... could be part of gap analysis... get rid of this if you dont need it\n\n        def play_samples(analysis: AudioAnalysisResult, start_sample, end_sample):\n            samples = analysis.audio.samples[start_sample:end_sample]\n\n            print(f\"playing samples={start_sample}-{end_sample}\")\n            sd.play(samples, analysis.sample_rate)\n            sd.wait()\n\n        def play_frames(analysis: AudioAnalysisResult, start_frame, end_frame):\n            start_sample = int(start_frame / analysis.fps * analysis.sample_rate)\n            end_sample = int(end_frame / analysis.fps * analysis.sample_rate)\n            play_samples(analysis, start_sample, end_sample)\n\n        print(f'{segment=}')\n        start_seconds = segment.start_seconds\n        end_seconds = segment.end_seconds\n\n        # * play literal audio samples without any frame rounding\n\n        start_sample = int(start_seconds * analysis.sample_rate)\n        end_sample = int(end_seconds * analysis.sample_rate)\n        play_samples(analysis, start_sample, end_sample)\n        time.sleep(1)\n\n        start_frame = start_seconds * analysis.fps\n        start_frame_before = math.floor(start_frame)\n        start_frame_after = math.ceil(start_frame)\n        rich.print(f\"{start_frame_before=}, {start_frame=}, {start_frame_after=}\")\n\n        end_frame = end_seconds * analysis.fps\n        end_frame_before = math.floor(end_frame)\n        end_frame_after = math.ceil(end_frame)\n        rich.print(f\"{end_frame_before=}, {end_frame=}, {end_frame_after=}\")\n\n        # does start frame matter? before/after => BOTH SOUND GREAT\n        print(\"start BEFORE\")\n        play_frames(analysis, start_frame_before, end_frame)\n        time.sleep(0.5)\n        print(\"start AFTER\")\n        play_frames(analysis, start_frame_after, end_frame)\n        time.sleep(0.5)\n\n        # what about end frame? before/after?\n        print(\"end BEFORE\")\n        play_frames(analysis, start_frame, end_frame_before)\n        time.sleep(0.5)\n\n        print(\"end AFTER\")\n        play_frames(analysis, start_frame, end_frame_after)\n        time.sleep(0.5)\n\ndef play_segments(analysis: AudioAnalysisResult, segments: list[DetectedSegment]) -> None:\n    combined = []\n    for segment in segments:\n        start = int(segment.start_seconds * analysis.sample_rate)\n        end = int(segment.end_seconds * analysis.sample_rate)\n        combined.append(analysis.audio.samples[start:end])\n    if combined:\n        audio = np.concatenate(combined)\n        sd.play(audio, analysis.sample_rate)\n        sd.wait()\n\ndef mute_and_play_segments(analysis: AudioAnalysisResult, segments: list[DetectedSegment]) -> None:\n    combined = []\n    for segment in segments:\n        start = int(segment.start_seconds * analysis.sample_rate)\n        end = int(segment.end_seconds * analysis.sample_rate)\n        if segment.label in (SegmentLabel.SILENCE, SegmentLabel.BREATH):\n            # combined.append(analysis.audio.samples[start:end])\n            # continue\n            last_sample = combined[-1][-1] if combined else 0\n            # cannot drop to zero... must fade down?! wtf?\n            # mute_slice = [-40 for r in segment.rms_values]\n            mute_slice = [last_sample for r in segment.rms_values]\n            print(mute_slice)\n            # return\n            # mute_slice = np.full(end - start, -70.0, dtype=analysis.audio.samples.dtype)\n            combined.append(mute_slice)\n        else:\n            combined.append(analysis.audio.samples[start:end])\n    if combined:\n        audio = np.concatenate(combined)\n        print(f'{len(audio)=}')\n        sd.play(audio, analysis.sample_rate)\n        sd.wait()<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 2632,
    "timings": {
      "cache_n": 483,
      "prompt_n": 2014,
      "prompt_ms": 294.595,
      "prompt_per_token_ms": 0.1462735849056604,
      "prompt_per_second": 6836.504353434376,
      "predicted_n": 136,
      "predicted_ms": 718.331,
      "predicted_per_token_ms": 5.281845588235294,
      "predicted_per_second": 189.32776115746083
    }
  }
}

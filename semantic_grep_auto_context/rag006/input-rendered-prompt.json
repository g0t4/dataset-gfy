{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1765156254,
  "id": "chatcmpl-JR19eD8eAmQCXpEHruc5BEKyY3uZSFLO",
  "model": "gpt-oss:120b",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 0,
    "prompt_n": 2981,
    "prompt_ms": 485.882,
    "prompt_per_token_ms": 0.1629929553840993,
    "prompt_per_second": 6135.234480799864,
    "predicted_n": 235,
    "predicted_ms": 1245.458,
    "predicted_per_token_ms": 5.299821276595745,
    "predicted_per_second": 188.6856080253208
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 0,
    "stop": true,
    "model": "gpt-oss:120b",
    "tokens_predicted": 235,
    "tokens_evaluated": 2981,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-12-07\n\nReasoning: low\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are completing code from a Neovim plugin.\nAs the user types, the plugin suggests code completions based on their cursor position marked with: <|fim_middle|>\nThe surrounding code is limited to X lines above/below the cursor, so it may not be the full file. Focus on the code near <|fim_middle|>Do NOT explain your decisions. Do NOT return markdown blocks ```\nDo NOT repeat surrounding code (suffix/prefix)\nONLY return valid code at the <|fim_middle|> position\nPAY attention to existing whitespace.\nYOU ARE ONLY INSERTING CODE, DO NOT REPEAT PREFIX/SUFFIX. Think about overlap before finishing your thoughts.\n\nFor example, if you see this in a python file:\ndef adder(a, b):\n    return <|fim_middle|> + b\n\nThe correct completion is:\na\n\nNOT:\na + b\n\nand NOT:\n    return a + b\n\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n\n\n<|end|><|start|>user<|message|>Here is context that's automatically provided, that MAY be relevant.\nrepo: ask-openai.nvim\n\nGeneral project code rules:\n- Never add comments to the end of a line.\n- NEVER add TODO comments for me.\n# General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n\n# Lua Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n\n\n# Semantic Grep matches: 5\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/client/init.lua:110-122\nfunction M.is_rag_supported_in_current_file()\n    if M.rag_yaml and not M.rag_yaml.enabled then\n        log:info('NOPE')\n        return false\n    end\n\n    if not M.is_rag_indexed_workspace then\n        return false\n    end\n\n    local current_file_extension = vim.fn.expand(\"%:e\")\n    return vim.tbl_contains(M.rag_extensions, current_file_extension)\nend\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/client/init.lua:36-66\nlocal function check_supported_dirs()\n    local work_dir = vim.fn.getcwd()\n    local dot_rag_dir = work_dir .. \"/.rag\"\n\n    local is_rag_dir = files.exists(work_dir)\n    M.rag_yaml = load_rag_yaml_config(work_dir)\n\n    log:info(\"RAG\", vim.inspect(M.rag_yaml))\n    if M.rag_yaml and not M.rag_yaml.enabled then\n        log:error(\"RAG is disabled\")\n        return\n    end\n\n    if not is_rag_dir then\n        -- fallback check git repo root\n        --   i.e. the rag dir in this ask-openai repo, or my hammerspoon config in my dotfiles repo\n        --   I often work inside these directories, maybe I should just have a .rag dir in them too.. and scope to just it but maybe not?\n        local git_root = vim.fn.systemlist(\"git rev-parse --show-toplevel\")[1]\n        dot_rag_dir = git_root .. \"/.rag\"\n        is_rag_dir = files.exists(dot_rag_dir)\n        if not is_rag_dir then\n            log:info(\"RAG is disabled b/c there is NO .rag dir: \" .. dot_rag_dir)\n            return\n        end\n        log:info(\"fallback, found .rag in repo root: \" .. git_root .. \"/.rag\")\n    end\n\n    M.is_rag_indexed_workspace = is_rag_dir\n    M.rag_extensions = files.list_directories(dot_rag_dir)\n    log:info(\"RAG is supported for: \" .. vim.inspect(M.rag_extensions))\nend\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/client/init.lua:15-34\n        return nil\n    end\n\n    local yaml_content = vim.fn.readfile(rag_yaml_path)\n    if not yaml_content then\n        log:error(\"yaml_content not loaded for\", rag_yaml_path)\n        return nil\n    end\n\n    local yaml_str = table.concat(yaml_content, \"\\n\")\n\n    local lyaml = require(\"lyaml\")\n    local ok, parsed = pcall(lyaml.load, yaml_str)\n    if ok then\n        return parsed\n    end\n\n    log:warn(\"Failed to parse .rag.yaml at \" .. rag_yaml_path)\n    return nil\nend\n\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/client/init.lua:6-34\n---@param work_dir string\n---@return table|nil rag_yaml_config\nlocal function load_rag_yaml_config(work_dir)\n    -- FYI\n    -- luarocks install --lua-version=5.1  lyaml\n\n    local rag_yaml_path = work_dir .. \"/.rag.yaml\"\n    if not files.exists(rag_yaml_path) then\n        log:error(\"no .rag.yaml found at\", rag_yaml_path)\n        return nil\n    end\n\n    local yaml_content = vim.fn.readfile(rag_yaml_path)\n    if not yaml_content then\n        log:error(\"yaml_content not loaded for\", rag_yaml_path)\n        return nil\n    end\n\n    local yaml_str = table.concat(yaml_content, \"\\n\")\n\n    local lyaml = require(\"lyaml\")\n    local ok, parsed = pcall(lyaml.load, yaml_str)\n    if ok then\n        return parsed\n    end\n\n    log:warn(\"Failed to parse .rag.yaml at \" .. rag_yaml_path)\n    return nil\nend\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag/client/init.lua:30-49\n    end\n\n    log:warn(\"Failed to parse .rag.yaml at \" .. rag_yaml_path)\n    return nil\nend\n\nlocal function check_supported_dirs()\n    local work_dir = vim.fn.getcwd()\n    local dot_rag_dir = work_dir .. \"/.rag\"\n\n    local is_rag_dir = files.exists(work_dir)\n    M.rag_yaml = load_rag_yaml_config(work_dir)\n\n    log:info(\"RAG\", vim.inspect(M.rag_yaml))\n    if M.rag_yaml and not M.rag_yaml.enabled then\n        log:error(\"RAG is disabled\")\n        return\n    end\n\n    if not is_rag_dir then\n\n<|end|><|start|>user<|message|>Please complete <|fim_middle|> in the following code (which has carefully preserved indentation):\nlocal messages = require(\"devtools.messages\")\nlocal api = require(\"ask-openai.api\")\nlocal log = require(\"ask-openai.logs.logger\").predictions()\n\nlocal M = {}\n\nfunction M.setup()\n    M.setup_lsp()\n    M.setup_telescope_picker() -- allow testing queries always\nend\n\nfunction M.setup_lsp()\n    -- TMP disable, i.e. when working on lsp itself :)\n    --  HRMm wont be easy to enable/disable this though, will have to restart if LSP wasn't started and rag is toggled?\n    if not api.is_rag_enabled() then\n        log:trace(\"NOT starting LSP (b/c RAG is disabled)\")\n        return\n    end\n    local rag_client = require(\"ask-openai.rag.client\")\n    if rag_client.rag_yaml and not <|fim_middle|>\n\n    local lspconfig = require(\"lspconfig\")\n    local configs = require(\"lspconfig.configs\")\n\n    if not configs.ask_language_server then\n        configs.ask_language_server = {\n            default_config = {\n                cmd = {\n                    os.getenv(\"HOME\") .. \"/repos/github/g0t4/ask-openai.nvim/.venv/bin/python\",\n                    \"-m\",\n                    \"lsp.server\",\n                },\n                cmd_cwd = os.getenv(\"HOME\") .. \"/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/rag\",\n                -- filetypes = rag_client.get_filetypes_for_workspace(),\n                -- filetypes = { '*' }, -- not set == all filetypes\n                --\n                -- FYI .git first means repo root is preferred, fallback is CWD\n                -- this maps to root_uri/root_path in server's on_initialize\n                root_dir = require(\"lspconfig.util\").root_pattern(\".git\", \".\"),\n            },\n        }\n    end\n\n    -- vim.lsp.handlers[\"window/showMessage\"] = function(err, result, ctx, config)\n    --     messages.append(\"global handler window/showMessage\")\n    --     messages.append(vim.inspect(result))\n    -- end\n\n    lspconfig.ask_language_server.setup({\n        on_attach = function(client, bufnr)\n            -- messages.append(client.name .. \" is now attached to buffer: \" .. bufnr)\n            -- messages.ensure_open()\n            -- -- messages.append(vim.inspect(client))\n            client.handlers = {\n                [\"fuu/no_dot_rag__do_the_right_thing_wink\"] = function(err, result, ctx, config)\n                    messages.append(\"client handler fuu/no_dot_rag__do_the_right_thing_wink\")\n                    messages.append(vim.inspect(result))\n                    -- ask server to shutdown, so I don't ask for more stuff it cannot do!\n                    -- WHY THE F does this not request SHUTDOWN!?\n                    -- vim.lsp.stop_client(client)\n                    -- vim.lsp.stop_client(client)\n                end,\n\n\n                -- [\"window/showMessage\"] = function(err, result, ctx, config)\n                --     messages.append(\"client handler window/showMessage\")\n                --     messages.append(vim.inspect(result))\n                -- end,\n                -- [\"window/showMessageRequest\"] = function(err, result, ctx, config)\n                --     messages.append(\"client handler window/showMessageRequest\")\n                --     messages.append(vim.inspect(result))\n                -- end,\n                -- [\"window/logMessage\"] = function(err, result, ctx, config)\n                --     messages.append(\"client handler window/logMessage\")\n                --     messages.append(vim.inspect(result))\n                -- end,\n            }\n\n            -- vim.defer_fn(function()\n            --     local req_id0, cancel0 = vim.lsp.buf_request(0, \"workspace/executeCommand\", {\n            --         command = \"SLEEPY\",\n            --         arguments = { {} }, -- MUST have empty arguments in pygls v2... or set values inside arguments = { { seconds = 10 } },\n            --     }, function(err, result)\n            --         log:error(\"DONE error: \" .. vim.inspect(err) .. \" res:\" .. vim.inspect(result))\n            --     end)\n            --     -- vim.defer_fn(cancel0, 0) -- works fine to cancel all immediately and it does so VERY fast\n            --     vim.defer_fn(cancel0, 500)\n            --\n            --     local req_id1, cancel1 = vim.lsp.buf_request(0, \"workspace/executeCommand\", {\n            --         command = \"SLEEPY\",\n            --         arguments = { { seconds = 10 } },\n            --     }, function(err, result)\n            --         log:error(\"DONE error: \" .. vim.inspect(err) .. \" res:\" .. vim.inspect(result))\n            --     end)\n            --     vim.defer_fn(cancel1, 0)\n            --\n            --     local req_id2, cancel2 = vim.lsp.buf_request(0, \"workspace/executeCommand\", {\n            --         command = \"SLEEPY\",\n            --         arguments = { { seconds = 10 } },\n            --     }, function(err, result)\n            --         log:error(\"DONE error: \" .. vim.inspect(err) .. \" res:\" .. vim.inspect(result))\n            --     end)\n            --     vim.defer_fn(cancel2, 0)\n            -- end, 500)\n        end,\n    })\nend\n\nfunction on_agg()\n    vim.cmd(\"Telescope ask_semantic_grep languages=GLOBAL\")\nend\n\nfunction on_age()\n    vim.cmd(\"Telescope ask_semantic_grep languages=EVERYTHING\")\nend\n\nfunction on_ag()\n    vim.cmd(\"Telescope ask_semantic_grep\")\nend\n\nfunction M.setup_telescope_picker()\n    require(\"telescope\").load_extension(\"ask_semantic_grep\")\n\n    vim.keymap.set('n', '<leader>ag', on_ag,\n        { noremap = true, silent = true, desc = 'Semantic grep Telescope picker, current filetype only' }\n    )\n    vim.keymap.set('n', '<leader>agg', on_agg,\n        { noremap = true, silent = true, desc = 'Semantic grep, Telescope picker, global languages (subject to rag.yaml -> global_languages)' }\n    )\n    vim.keymap.set('n', '<leader>age', on_age,\n        { noremap = true, silent = true, desc = 'Semantic grep, Telescope picker, everything (NOT subject to rag.yaml -> global_languages)' }\n    )\nend\n\nreturn M<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 3215,
    "timings": {
      "cache_n": 0,
      "prompt_n": 2981,
      "prompt_ms": 485.882,
      "prompt_per_token_ms": 0.1629929553840993,
      "prompt_per_second": 6135.234480799864,
      "predicted_n": 235,
      "predicted_ms": 1245.458,
      "predicted_per_token_ms": 5.299821276595745,
      "predicted_per_second": 188.6856080253208
    }
  }
}

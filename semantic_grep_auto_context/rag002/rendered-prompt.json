{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "delta": {}
    }
  ],
  "created": 1764112637,
  "id": "chatcmpl-SnEYZ8kNTQY8y4i5uiiz4RoZmUlQvVAi",
  "model": "",
  "system_fingerprint": "b7089-38e2c1b41",
  "object": "chat.completion.chunk",
  "timings": {
    "cache_n": 0,
    "prompt_n": 1542,
    "prompt_ms": 239.034,
    "prompt_per_token_ms": 0.15501556420233462,
    "prompt_per_second": 6450.965134667035,
    "predicted_n": 126,
    "predicted_ms": 530.618,
    "predicted_per_token_ms": 4.211253968253969,
    "predicted_per_second": 237.45896294509419
  },
  "__verbose": {
    "index": 0,
    "content": "",
    "tokens": [],
    "id_slot": 2,
    "stop": true,
    "model": "",
    "tokens_predicted": 126,
    "tokens_evaluated": 1542,
    "generation_settings": {
      "seed": 4294967295,
      "temperature": 1.0,
      "dynatemp_range": 0.0,
      "dynatemp_exponent": 1.0,
      "top_k": 40,
      "top_p": 1.0,
      "min_p": 0.05000000074505806,
      "top_n_sigma": -1.0,
      "xtc_probability": 0.0,
      "xtc_threshold": 0.10000000149011612,
      "typical_p": 1.0,
      "repeat_last_n": 64,
      "repeat_penalty": 1.0,
      "presence_penalty": 0.0,
      "frequency_penalty": 0.0,
      "dry_multiplier": 0.0,
      "dry_base": 1.75,
      "dry_allowed_length": 2,
      "dry_penalty_last_n": 131072,
      "dry_sequence_breakers": [
        "\n",
        ":",
        "\"",
        "*"
      ],
      "mirostat": 0,
      "mirostat_tau": 5.0,
      "mirostat_eta": 0.10000000149011612,
      "stop": [],
      "max_tokens": 4096,
      "n_predict": 4096,
      "n_keep": 0,
      "n_discard": 0,
      "ignore_eos": false,
      "stream": true,
      "logit_bias": [],
      "n_probs": 0,
      "min_keep": 0,
      "grammar": "",
      "grammar_lazy": false,
      "grammar_triggers": [],
      "preserved_tokens": [
        200003,
        200005,
        200006,
        200007,
        200008
      ],
      "chat_format": "GPT-OSS",
      "reasoning_format": "deepseek",
      "reasoning_in_content": false,
      "thinking_forced_open": false,
      "samplers": [
        "penalties",
        "dry",
        "top_n_sigma",
        "top_k",
        "typ_p",
        "top_p",
        "min_p",
        "xtc",
        "temperature"
      ],
      "speculative.n_max": 16,
      "speculative.n_min": 0,
      "speculative.p_min": 0.75,
      "timings_per_token": false,
      "post_sampling_probs": false,
      "lora": []
    },
    "prompt": "<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.\nKnowledge cutoff: 2024-06\nCurrent date: 2025-11-25\n\nReasoning: off\n\n# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions\n\nYou are an expert coder. Ground rules:\n- Follow the user's instructions. \n- DO NOT wrap answers in markdown code blocks, which means no triple backticks like ``` nor single backtick like ` .\n- Do not explain answers, just give me code. \n- Never add comments to the end of a line. \n- Never add comments.\n- Preserve indentation. \n- If rewriting code, preserve unrelated code. \n- Prefer readable code over of comments. \n\n<|end|><|start|>user<|message|># General Code Preferences\n\n- When rewriting code, leave unrelated code and unrelated comments as is.\n- When an if statement has an AMBIGUOUS condition, extract a variable to meaningfully name it, for example:\n```lua\nlocal is_blank_line = line:match(\"^%s*$\")\nif is_blank_line then\n    -- ...\nend\n```\n<|end|><|start|>user<|message|># Lua Code Preferences\n\n## Naming\n- Use snake_case for functions and variables.\n- Use PascalCase for classes.\n\n## Type Hints\n- Prefer EmmyLua/LuaLS style annotations\n- Always annotate when type inference would be ambiguous, or for public APIs.\n\nExamples:\n---@param names string[]\n---@param checker fun(name: string): boolean\n---@return table<string, integer>\nfunction process_names(names, checker) end\n\n---@class WindowController\n---@field window hs.axuielement\n---@field title? string\n---@field size { w: number, h: number }\nlocal WindowController = {}\n\n<|end|><|start|>user<|message|># Semantic Grep matches: 5\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/backends/llama_cpp/llama_server_client.lua:4-9\n---@param base_url string\n---@return table? body\nfunction LlamaServerClient.apply_template(base_url, body)\n    local apply_template_url = base_url .. \"/apply-template\"\n    return client.get_response_body(apply_template_url, client.Methods.POST, body)\nend\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/chat/messages/tx.lua:15-22\n-- TODO! I want end to end testing to verify scenarios that go into the model (the actual prompt) based on TxChatMessage inputs\n--  TxChatMessage inputs => .__verbose.prompt output\n--  will require --verbose-prompt arg to llama-server\n--  VERIFY |tojson fix for tool results, AND tool call args\n--  VERIFY other fixes identified by unsloth:\n--     https://unsloth.ai/blog/gpt-oss\n--     icdiff https://huggingface.co/openai/gpt-oss-120b/raw/main/chat_template.jinja https://huggingface.co/unsloth/gpt-oss-120b/raw/main/chat_template.jinja\n\n\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/backends/models/fim.lua:50-88\n---@param request FimBackend\n---@param level FimReasoningLevel\nfunction M.gptoss.get_fim_chat_messages(request, level)\n    -- TODO what if I change this to ask gptoss to rewrite the current line (only current line)\n    --  and then FIM just replaces that line?\n    --  OR, add a shortcut key to accept FIM as replace current line?\n    --  not always, but sometimes gptoss still suggests entire line (espeically for partial lines)\n    --    why not work around that or encourage that?\n    --    I can even diff the current line vs the generated to see what to insert so I don't have to do extmarks that replace the full line\n\n    local dev = HarmonyRawFimPromptBuilder.developer_message\n    -- FYI if/when you test out using partial thinking with raw template above, then put this into the shared developer message\n    dev = dev .. [[\nMake sure to practice the code change before you return a suggestion. Take the cursor line (at least) and type it out with the new code and make sure it is valid, correct code.\n]]\n\n    local messages = {\n        TxChatMessage:developer(dev), -- FYI developer or system message must be first, and ONLY ONE is allowed\n        TxChatMessage:user(HarmonyRawFimPromptBuilder.context_user_msg(request)),\n        TxChatMessage:user(HarmonyRawFimPromptBuilder.fim_prompt(request)),\n    }\n\n    if level == \"off\" then\n        -- TODO get rid of raw prompt approach above? or just keep it around as \"RETIRED\" ??\n        local fixed_thoughts = HarmonyRawFimPromptBuilder.deep_thoughts_about_fim\n\n        -- FYI \"<|start|>assistant\" is already added, so move right into analysis channel first:\n        local prefill = \"<|channel|>analysis<|message|>\" .. fixed_thoughts .. \"<|end|>\"\n            .. \"<|start|>assistant<|channel|>final<|message|>\"\n\n        -- llama-cpp uses this last assistant message for prefill purposes (will not terminate with <|end|>)\n        table.insert(messages, TxChatMessage:assistant(prefill))\n\n        -- TODO add nvim command to verify prompts:\n        --   TODO AskDumpApplyTemplates (dump all in one go is probably best to compare)\n    end\n\n    return messages\nend\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/questions/chat/messages/tx.lua:0-19\nlocal log = require('ask-openai.logs.logger').predictions()\nlocal ansi = require('ask-openai.predictions.ansi')\n\n---@class TxChatMessage : OpenAIChatCompletion_TxChatMessage\nlocal TxChatMessage = {}\n\n---@enum TX_MESSAGE_ROLES\nlocal TX_MESSAGE_ROLES = {\n    USER = \"user\",\n    ASSISTANT = \"assistant\",\n    TOOL = \"tool\", -- FYI this is for TOOL RESULTS\n    SYSTEM = \"system\",\n    DEVELOPER = \"developer\",\n}\n\n-- TODO! I want end to end testing to verify scenarios that go into the model (the actual prompt) based on TxChatMessage inputs\n--  TxChatMessage inputs => .__verbose.prompt output\n--  will require --verbose-prompt arg to llama-server\n--  VERIFY |tojson fix for tool results, AND tool call args\n--  VERIFY other fixes identified by unsloth:\n\n\n## /Users/wesdemos/repos/github/g0t4/ask-openai.nvim/lua/ask-openai/backends/llama_cpp/llama_server_client.lua:0-3\nlocal client = require(\"ask-openai.backends.llama_cpp.json_client\")\n\nlocal LlamaServerClient = {}\n\n\n<|end|><|start|>user<|message|>use my new llama_server_client module\nHere is code I selected (with carefully preserved indentation) from lua/ask-openai/backends/models/gptoss/manual/apply_template_tests.lua:\n        local response = json_client.get_response_body(URL_APPLY_TEMPLATE, json_client.Methods.POST, {\n            messages = {\n                TxChatMessage:user(\"Hello, can you rewrite this code?\")\n            }\n        })<|end|><|start|>assistant",
    "has_new_line": true,
    "truncated": false,
    "stop_type": "eos",
    "stopping_word": "",
    "tokens_cached": 1667,
    "timings": {
      "cache_n": 0,
      "prompt_n": 1542,
      "prompt_ms": 239.034,
      "prompt_per_token_ms": 0.15501556420233462,
      "prompt_per_second": 6450.965134667035,
      "predicted_n": 126,
      "predicted_ms": 530.618,
      "predicted_per_token_ms": 4.211253968253969,
      "predicted_per_second": 237.45896294509419
    }
  }
}

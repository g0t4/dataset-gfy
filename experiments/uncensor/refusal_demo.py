# -*- coding: utf-8 -*-
"""refusal_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw

# Demo of bypassing refusal

>[Demo of bypassing refusal](#scrollTo=82acAhWYGIPx)

>>[Setup](#scrollTo=fcxHyDZw6b86)

>>>[Load model](#scrollTo=6ZOoJagxD49V)

>>>[Load harmful / harmless datasets](#scrollTo=rF7e-u20EFTe)

>>>[Tokenization utils](#scrollTo=KOKYA61k8LWt)

>>>[Generation utils](#scrollTo=gtrIK8x78SZh)

>>[Finding the "refusal direction"](#scrollTo=W9O8dm0_EQRk)

>>[Ablate "refusal direction" via inference-time intervention](#scrollTo=2EoxY5i1CWe3)

>>[Orthogonalize weights w.r.t. "refusal direction"](#scrollTo=t9KooaWaCDc_)

This notebook demonstrates our method for bypassing refusal, levaraging the insight that refusal is mediated by a 1-dimensional subspace.

Please see our [research post](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction) or our [paper](https://arxiv.org/abs/2406.11717) for a more thorough treatment.

In this minimal demo, we use [Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat) and implement interventions and weight updates using [TransformerLens](https://github.com/neelnanda-io/TransformerLens). To extract the "refusal direction," we use just 32 harmful instructions from [AdvBench](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv) and 32 harmless instructions from [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca).

## Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama

from pathlib import Path
import torch
import functools
import einops
import requests
import pandas as pd
import io
import textwrap
import gc
import rich

from datasets import load_dataset
from sklearn.model_selection import train_test_split
from tqdm import tqdm as progress_tqdm
from torch import Tensor
from typing import List, Callable
from transformer_lens import HookedTransformer, utils
from transformer_lens.hook_points import HookPoint
from transformers import AutoTokenizer, PreTrainedTokenizerBase, Qwen2TokenizerFast
from jaxtyping import Float, Int
from colorama import Fore

import cuda_env

cuda_env.use_6000()

# %%
"""### Load model"""

# QWEN25_INSTRUCT = "Qwen/Qwen2.5-7B-Instruct"
QWEN25_INSTRUCT = "Qwen/Qwen2.5-0.5B-Instruct"  # faster testing
QWEN1 = 'Qwen/Qwen-1_8B-chat'
QWEN25_BASE = 'Qwen/Qwen2.5-0.5B'
#
QWEN3 = 'Qwen/Qwen3-8B'
QWEN3 = 'Qwen/Qwen3-0.6B'
# QWEN3 = 'Qwen/Qwen3-4B-Instruct-2507' # non-thinking (not yet, at least not in TL 2.16.1) - FYI you would want to drop /think for non-thinking qwen3

# ***! SET MODEL HERE:
# MODEL_PATH = QWEN1
# MODEL_PATH =  QWEN_25_BASE # base (not instruct) - interesting it didn't refuse many prompts even before lobotomizing
# MODEL_PATH = QWEN25_INSTRUCT
MODEL_PATH = QWEN3
# ***! END SET MODEL

use_qwen1 = MODEL_PATH.startswith("Qwen/Qwen-1")
use_qwen2 = MODEL_PATH.startswith("Qwen/Qwen2.5")
use_qwen3 = MODEL_PATH.startswith("Qwen/Qwen3")

# FYI transformer_lens is not compat with gptoss, use direct hooks and cache yourself (see refusal-gptoss.py)

DEVICE = 'cuda'

model = HookedTransformer.from_pretrained_no_processing(
    MODEL_PATH,
    device=DEVICE,
    dtype=torch.float16,  # ? bfloat16
    default_padding_side='left',
    # fp16=True, # TODO did I comment this out b/c I was running on my mac instead of arch box?
    # fp16=(DEVICE == CUDA) ? TODO if fp16 s/b on for arch box w/ nvidia gpu then add toggle based on DEVICE = 'cuda' constant too (add CUDA constant and check DEVICE == CUDA or...?)
)

# %%

if use_qwen2 or use_qwen3:
    tokenizer: Qwen2TokenizerFast = model.tokenizer
elif use_qwen1:
    tokenizer: PreTrainedTokenizerBase = model.tokenizer
# else:
#     tokenizer = model.tokenizer

# how can I list tokens in tokenizer
vocab = tokenizer.get_vocab()
len(vocab)
[t for t in vocab if str(t).find("extra_0") > -1]
[t for t in vocab if str(t).startswith("<|")]
# [k for k in vocab.keys() if k.startswith(b"<|")]

# %% SET PAD TOKEN THAT AVOIDS CONTAMINATION

model.tokenizer.padding_side = 'left'  # defaults: qwen1:left, qwen2.5:left
# PRN try right padding? update calculations and references, anything noteworthy? probably not
#   mostly about where to measure logits (mid stream vs last token, etc)... in right padded each sequence's length is needed to determine where the next token is positioned

# PRN add toggles for qwen1 vs qwen2.5 to do these:
if use_qwen1:
    # set pad_token to lesser used token to avoid residual contamination from padding:
    #   weights are random and likely
    model.tokenizer.pad_token = '<|extra_0|>'  # defaults:qwen1==qwen2.5=='<|endoftext|>' (IIRC b/c it is not set, this is hf default)
    #
    # Confirmed only one with Qwen1:
    #   In [31]: [model.tokenizer.decode(i) for i in model.tokenizer.encode(("<|extra_0|>")) ]
    #   Out[31]: ['<|extra_0|>']

if use_qwen2 or use_qwen3:
    # FYI In [51]: [model.tokenizer.decode(i) for i in model.tokenizer.encode(("<|extra_0|>")) ]
    # Out[50]: ['<', '|', 'extra', '_', '0', '|', '>']
    # there is NO <|extra_0|> or set of similar unused tokens.
    residual_pad = "<|residual_pad|>"
    tokenizer.add_special_tokens({"additional_special_tokens": [residual_pad]})
    tokenizer.add_special_tokens
    tokenizer.encode(residual_pad)
    model.tokenizer.pad_token = residual_pad

# FYI for OTHER MODELS, what value do you want to use? i.e. gptoss?

# check if token is set:
model.tokenizer.pad_token_id
model.tokenizer.decode(model.tokenizer.pad_token_id)

# %% find vocab_size (to understand outputs)

# base vocab:
len(model.tokenizer)  # qwen1:151851 qwen2.5:151665
model.tokenizer.vocab_size  # qwen1:same qwen2.5:151643(22 less)
# added:
model.tokenizer.added_tokens_decoder  # qwen1:empty(0)
len(model.tokenizer.added_tokens_decoder)  # qwen2.5:22 items
# verify total:
model.tokenizer.vocab_size + len(model.tokenizer.added_tokens_decoder) == len(model.tokenizer)  # qwen1:True qwen2.5:True

# %%
"""### sarcasm dataset instead"""

def get_sarcasm_headlines(is_sarcastic):
    import json
    # https://www.kaggle.com/datasets/rmisra/news-headlines-dataset-for-sarcasm-detection
    df = pd.read_json("~/kaggle/news-headlines-sarcasm/Sarcasm_Headlines_Dataset.json", lines=True)
    df = df[df['is_sarcastic'] == is_sarcastic]
    df = df['headline'].sample(100, random_state=42).tolist()
    return df, df

"""### Load harmful / harmless datasets"""

def get_harmful_instructions():
    # # 1a - download dataset
    # url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'
    # response = requests.get(url)
    # contents = io.StringIO(response.content.decode('utf-8'))
    # dataset = pd.read_csv(contents)
    # print(dataset)

    # # 1b - read local file instead of download each time
    dataset = pd.read_csv('./out/harmful_behaviors.csv')

    instructions = dataset['goal'].tolist()

    train, test = train_test_split(instructions, test_size=0.2, random_state=42)
    return train, test

def get_harmless_instructions():
    hf_path = 'tatsu-lab/alpaca'
    dataset = load_dataset(hf_path)

    # filter for instructions that do not have inputs
    instructions = []
    for i in range(len(dataset['train'])):
        if dataset['train'][i]['input'].strip() == '':
            instructions.append(dataset['train'][i]['instruction'])

    train, test = train_test_split(instructions, test_size=0.2, random_state=42)
    return train, test

# use_sarcasm_data = True
use_sarcasm_data = False  # == harmful data

if use_sarcasm_data:
    harmful_inst_train, harmful_inst_test = get_sarcasm_headlines(True)
    harmless_inst_train, harmless_inst_test = get_sarcasm_headlines(False)
else:
    harmful_inst_train, harmful_inst_test = get_harmful_instructions()
    harmless_inst_train, harmless_inst_test = get_harmless_instructions()

print("Harmful instructions:")
for num in range(4):
    print(f"\t{repr(harmful_inst_train[num])}")
print("Harmless instructions:")
for num in range(4):
    print(f"\t{repr(harmless_inst_train[num])}")

# %%
# QWEN_CHAT_TEMPLATE = """<|im_start|>user
# {instruction}<|im_end|>
# <|im_start|>assistant
# """

# QWEN25_CHAT_TEMPLATE = """<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
# <|im_start|>user
# {instruction}<|im_end|>
# <|im_start|>assistant
# """

# model.tokenizer.padding_side = 'right'  # check padding too! (flip left/right if you want)

def tokenize_batch(instructions: list[str]) -> Int[Tensor, 'batch_size seq_len']:

    def modify_user_request(instruction):
        if use_qwen3:
            instruction = "/no_think" + instruction

        if not use_sarcasm_data:
            return instruction
        return "I need you to decide yes or no, is the following news headline sarcastic or not? Start with yes/no and no explanation.\n\n" + instruction

    def messages_for(instruction):
        return [{"role": "user", "content": modify_user_request(instruction)}]

    batch = [messages_for(i) for i in instructions]

    prompts = tokenizer.apply_chat_template(
        batch,
        add_generation_prompt=True,  # assistant prefill @end
        tokenize=True,  # set false to preview (or just decode later)
        padding=True,
        truncation=False,
        return_tensors="pt",
    )
    print(type(prompts), prompts.shape)
    return prompts

def print_prompt(chat_token_ids):
    rich.print(f"\n[blue bold]CHAT ({len(chat_token_ids)} tokens)[/]")
    print(model.tokenizer.decode(chat_token_ids))
    rich.print(f"  [white bold]tokens[/]")
    print("  ", chat_token_ids)

# ?? double check the prompt is as expected, also padding/pad_token
[print_prompt(chat_token_ids) for chat_token_ids in tokenize_batch(["foo", "tell me how to do bad things!"])]
# FYI you need 2+ to check padding (obviously)... so don't get rid of the first one (and make sure there's an input token length difference to trigger padding)

# %%
"""### Generation utils"""

def _generate_with_hooks(
    model: HookedTransformer,
    toks: Int[Tensor, 'batch_size seq_len'],
    max_tokens_generated: int = 64,
    fwd_hooks=[],
) -> List[str]:

    batch_seq_len = toks.shape[1]
    batch_size = toks.shape[0]
    all_toks = torch.zeros((batch_size, batch_seq_len + max_tokens_generated), dtype=torch.long, device=toks.device)
    # TODO rewrite w/ einops (good practice of both einops and model architecture, how well you recall each):
    all_toks[:, :batch_seq_len] = toks

    for new_token_number in range(max_tokens_generated):
        with model.hooks(fwd_hooks=fwd_hooks):
            logits = model(all_toks[:, :-max_tokens_generated + new_token_number])
            next_tokens = logits[:, -1, :].argmax(dim=-1)  # greedy sampling (temperature=0)
            all_toks[:, -max_tokens_generated + new_token_number] = next_tokens

    return model.tokenizer.batch_decode(all_toks[:, batch_seq_len:], skip_special_tokens=False)

def generate(
    model: HookedTransformer,
    instructions_batch: List[str],
    fwd_hooks=[],
    max_tokens_generated: int = 64,
    batch_size: int = 4,
) -> List[str]:

    # results of all batches:
    generations = []

    # divide large instructions_batch into smaller batches (based on batch_size)
    for batch_number in progress_tqdm(range(0, len(instructions_batch), batch_size)):
        toks = tokenize_batch(instructions=instructions_batch[batch_number:batch_number + batch_size])
        generation = _generate_with_hooks(
            model,
            toks,
            max_tokens_generated=max_tokens_generated,
            fwd_hooks=fwd_hooks,
        )
        generations.extend(generation)

    return generations

# %%
"""## Finding the "refusal direction"
"""

N_INST_TRAIN = 32

# tokenize (instructions => template => chat prompt)
harmful_chats_token_ids = tokenize_batch(instructions=harmful_inst_train[:N_INST_TRAIN])
harmless_chats_token_ids = tokenize_batch(instructions=harmless_inst_train[:N_INST_TRAIN])

def log_hooks(hook_name):
    print(hook_name)  # comment out to disable
    return True

# run model on harmful and harmless instructions, caching intermediate activations
harmful_logits, harmful_cache = model.run_with_cache(harmful_chats_token_ids, names_filter=lambda hook_name: 'resid' in hook_name)
harmless_logits, harmless_cache = model.run_with_cache(harmless_chats_token_ids, names_filter=lambda hook_name: 'resid' in hook_name)
# _logits = [batch_size, seq_len, vocab_size]
# _cache = per layer (qwen1 has 24 layers, pre/mid/post for each layer = 72 total caches)
#   72 keys (see summarize_keys below)
#   each => [batch_size, seq_len, hidden_dimensions]
#   where hidden_dimensions: qwen2.5_0.5B_instruct=896   qwen1_8B=2048 (b/c 8B way bigger than 0.5B)

# dir(harmful_cache)
import logging

def summarize_keys(data):
    for key, value in data.items():
        if torch.is_tensor(value):
            print(f"{key}: {value.dtype}, Shape: {value.shape}")
            continue
        print(f"Damn! Key type: {type(key).__name__}, Value type: {type(value).__name__}")

summarize_keys(harmful_cache)

# compute difference of means between harmful and harmless activations at an intermediate layer

# * inspecting model:
# [m for m in model.modules()]
def summarize_layer(name, param):
    print(f"{name} shape={tuple(param.shape)} {param.dtype}")

def summarize_named_params(model):
    for name, param in model.named_parameters():
        summarize_layer(name, param)

summarize_named_params(model.embed)
# * embed
#   tokens => hidden dimensions mapping
#   [vocab_size, hidden_dimensions]
#   qwen2.5=W_E shape(151936, 896)

summarize_named_params(model.unembed)
# * unembed
#   hidden dimentions => tokens mapping
#   [hidden_dimensions, vocab_size]
#   reverse mapping from hidden dimensions => tokens

summarize_named_params(model)
# model.blocks # transformer layers (qwen25=24 layers)

# %%

def compute_unit_refusal_dir(layer=14):
    # token to sample (-1 == last token)
    pos = -1

    # think of hidden_dimensions as your internal vocab_size

    # shape=(batch_size,seq_len,hidden_dimensions)
    harmful_residual_pre = harmful_cache['resid_pre', layer]
    harmful_residual_pre.shape
    harmful_mean_act = harmful_residual_pre[:, pos, :].mean(dim=0)
    # internal representation of each token w/in the HARMFUL dataset => [batch_size,seq_len,hidden_dimensions]
    #   then mean => average across sequences in batch
    #     ==> [1,1,hidden_dimensions] ==> shape=(hidden_dimensions)

    harmless_residual_pre = harmless_cache['resid_pre', layer]
    harmless_mean_act = harmless_residual_pre[:, pos, :].mean(dim=0)
    # same for harmless dataset

    # idea is, if you have two datasets that only meaningfully differ due to one characteristics (i.e. refusal)
    #  then if all other characteristics are randomly sampled (uniform) then the only diff in average vector of each is the refusal component (vector)
    refusal_dir = harmful_mean_act - harmless_mean_act
    unit_refusal_dir = refusal_dir / refusal_dir.norm()

    # reading the tea leaves:
    summarize_layer("unit_refusal_dir", unit_refusal_dir)
    # unit_refusal_dir.shape=[hidden_dimensions]

    # transform refusal dir to token vocab
    #   unit_refusal_dir[hidden_dim] matmul unembed[hidden_dimensions,vocab_size] ==> [vocab_size]
    # [hidden_dimensions].matmul([hidden_dimensions,vocab_size]) = [vocab_size]

    # * interpret refusal latent space => map back to likely tokens
    # refusal_logits = unit_refusal_dir.matmul(model.unembed.W_U)  # w/o bias is interesting
    refusal_logits = unit_refusal_dir.matmul(model.unembed.W_U) + model.unembed.b_U
    # model.lm_head.bias
    summarize_layer("  refusal_logits", refusal_logits)  # w/e "refusal logits" means :)... basically map this latent subspace out into token space to see what meaning it has alone
    refusal_max_token_id_next = refusal_logits.argmax()
    print("  refusal max_token_id_next:", refusal_max_token_id_next)
    refusal_decoded_text_token = tokenizer.decode(refusal_max_token_id_next, skip_special_tokens=True)
    print("  refusal decoded next token (text): '" + refusal_decoded_text_token + "'")

    return unit_refusal_dir

unit_refusal_dir: Float[Tensor, "d_hidden"] = compute_unit_refusal_dir(layer=14)
# FREAKY:
# layer 14 - redo decoded: '告'
# 告诉 (gàosu) – to tell, inform
# 报告 (bàogào) – report, to report
# 警告 (jǐnggào) – warning

# compute_unit_refusal_dir(layer=16)
# # layer 16 - 极其
# #   “extremely” or “to the utmost degree.”

# compute_unit_refusal_dir(layer=18)
# # layer 18 - 非常
# # “very,” “extremely,” “highly” (used to intensify an adjective or verb)

# for layer in range(model.cfg.n_layers):
#     print("layer", layer)
#     compute_unit_refusal_dir(layer=layer)

# %%

# clean up memory
del harmful_cache, harmless_cache, harmful_logits, harmless_logits
gc.collect()
torch.cuda.empty_cache()

# %%
"""## Ablate "refusal direction" via inference-time intervention

Given a "refusal direction" $\widehat{r} \in \mathbb{R}^{d_{\text{model}}}$ with unit norm, we can ablate this direction from the model's activations $a_{l}$:
$${a}_{l}' \leftarrow a_l - (a_l \cdot \widehat{r}) \widehat{r}$$

By performing this ablation on all intermediate activations, we enforce that the model can never express this direction (or "feature").
"""

def remove_refusal_during_forward_pass(
    activation: Float[Tensor, "... d_hidden"],
    hook: HookPoint,
):
    magnitude_of_refusal = (activation * unit_refusal_dir).sum(dim=-1, keepdim=True)  # TLDR dotproduct with innnermost activation dimension (d_hidden) and the unit_refusal_dir
    activation_refusal_projection = magnitude_of_refusal * unit_refusal_dir

    # subtract the refusal component(s)... model cannot represent refusal now!
    return activation - activation_refusal_projection

layer_numbers = list(range(model.cfg.n_layers))  # qwen25-n_layers=24 so 0,1,2...23

remove_refusal_from_every_layer = [
    (utils.get_act_name(act_name, layer_number), remove_refusal_during_forward_pass)
    for layer_number in layer_numbers  \
    for act_name in ['resid_pre', 'resid_mid', 'resid_post']
]

# * hone in on a subset, longer generation to see what effects might be going on... like inadvertent consequences of lobotomizing
# N_INST_START = 38
# N_INST_END = 39
# * defaults
N_INST_START = 0
N_INST_END = 8
# N_INST_END = 48
#
MAX_TOKENS = 64
# MAX_TOKENS = 256
# MAX_TOKENS = 512
#
final_test_cases = harmful_inst_test[N_INST_START:N_INST_END]
if use_sarcasm_data:
    final_test_cases = final_test_cases + harmless_inst_test[N_INST_START:N_INST_END]  # same, but for baseline (i.e. not harmful)

# print(f"Final test cases: {len(final_test_cases)}")
# [f for f in final_test_cases]

intervention_generations = generate(model, final_test_cases, fwd_hooks=remove_refusal_from_every_layer, max_tokens_generated=MAX_TOKENS)
baseline_generations = generate(model, final_test_cases, fwd_hooks=[], max_tokens_generated=MAX_TOKENS)

# btw qwen3 seems to have some "educational" override to some requests that kicks in and warns that XYZ is likely illegal but here is how for educational purposes
#  I might want to remove that instinct to get it to really open up b/c that's like a secondary refusal or indirection

def compare_case(num, case):

    def show(color, title, generated):
        if not generated:
            return
        print(color + title)
        # PRN could stop at stop token
        print(textwrap.fill(repr(generated), width=100, initial_indent='\t', subsequent_indent='\t'))

    print(f"INSTRUCTION {num}: {repr(case)}")
    if "baseline_generations" in globals():
        gen = globals()["baseline_generations"]
        show(Fore.GREEN, "BASELINE COMPLETION:", gen[num])
    if "intervention_generations" in globals():
        gen = globals()["intervention_generations"]
        show(Fore.RED, "INTERVENTION COMPLETION:", gen[num])
    if "orthogonalized_generations" in globals():
        gen = globals()["orthogonalized_generations"]
        show(Fore.MAGENTA, "ORTHOGONALIZED COMPLETION:", gen[num])

    print(Fore.RESET)

for num, case in enumerate(final_test_cases):
    compare_case(num, case)

# %%
"""## Orthogonalize weights w.r.t. "refusal direction"

We can implement the intervention equivalently by directly orthogonalizing the weight matrices that write to the residual stream with respect to the refusal direction $\widehat{r}$:
$$W_{\text{out}}' \leftarrow W_{\text{out}} - \widehat{r}\widehat{r}^{\mathsf{T}} W_{\text{out}}$$

By orthogonalizing these weight matrices, we enforce that the model is unable to write direction $r$ to the residual stream at all!
"""

raise RuntimeError("FYI! last test modifies weights so only use this if you're away and comment this back out when done so you don't forget next time")
ok_to_modify_model_weights = False

if ok_to_modify_model_weights:

    def get_orthogonalized_matrix(matrix: Float[Tensor, '... d_model'], vec: Float[Tensor, 'd_model']) -> Float[Tensor, '... d_model']:
        proj = einops.einsum(matrix, vec.view(-1, 1), '... d_model, d_model single -> ... single') * vec
        return matrix - proj

    model.W_E.data = get_orthogonalized_matrix(model.W_E, unit_refusal_dir)

    for block in model.blocks:
        block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, unit_refusal_dir)
        block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, unit_refusal_dir)

    orthogonalized_generations = generate(model, final_test_cases, fwd_hooks=[])

    for num, case in enumerate(final_test_cases):
        compare_case(num, case)

# -*- coding: utf-8 -*-
"""refusal_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a-aQvKC9avdZpdyBn4jgRQFObTPy1JZw

# Demo of bypassing refusal

>[Demo of bypassing refusal](#scrollTo=82acAhWYGIPx)

>>[Setup](#scrollTo=fcxHyDZw6b86)

>>>[Load model](#scrollTo=6ZOoJagxD49V)

>>>[Load harmful / harmless datasets](#scrollTo=rF7e-u20EFTe)

>>>[Tokenization utils](#scrollTo=KOKYA61k8LWt)

>>>[Generation utils](#scrollTo=gtrIK8x78SZh)

>>[Finding the "refusal direction"](#scrollTo=W9O8dm0_EQRk)

>>[Ablate "refusal direction" via inference-time intervention](#scrollTo=2EoxY5i1CWe3)

>>[Orthogonalize weights w.r.t. "refusal direction"](#scrollTo=t9KooaWaCDc_)

This notebook demonstrates our method for bypassing refusal, levaraging the insight that refusal is mediated by a 1-dimensional subspace.

Please see our [research post](https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction) or our [paper](https://arxiv.org/abs/2406.11717) for a more thorough treatment.

In this minimal demo, we use [Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat) and implement interventions and weight updates using [TransformerLens](https://github.com/neelnanda-io/TransformerLens). To extract the "refusal direction," we use just 32 harmful instructions from [AdvBench](https://github.com/llm-attacks/llm-attacks/blob/main/data/advbench/harmful_behaviors.csv) and 32 harmless instructions from [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca).

## Setup
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers transformers_stream_generator tiktoken transformer_lens einops jaxtyping colorama

from pathlib import Path
import torch
import functools
import einops
import requests
import pandas as pd
import io
import textwrap
import gc

from datasets import load_dataset
from sklearn.model_selection import train_test_split
from tqdm import tqdm as progress_tqdm
from torch import Tensor
from typing import List, Callable
from transformer_lens import HookedTransformer, utils
from transformer_lens.hook_points import HookPoint
from transformers import AutoTokenizer, PreTrainedTokenizerBase, Qwen2TokenizerFast
from jaxtyping import Float, Int
from colorama import Fore

import cuda_env

cuda_env.use_6000()

# %%
"""### Load model"""

# TODO finish setting up rest of config to be dynamic (i.e. template)
QWEN25_INSTRUCT = "Qwen/Qwen2.5-0.5B-Instruct"
QWEN1 = 'Qwen/Qwen-1_8B-chat'
QWEN25_BASE = 'Qwen/Qwen2.5-0.5B'

# ***! SET MODEL HERE:
# MODEL_PATH = QWEN1
# MODEL_PATH =  QWEN_25_BASE # base (not instruct) - interesting it didn't refuse many prompts even before lobotomizing
MODEL_PATH = QWEN25_INSTRUCT
# ***! END SET MODEL

use_qwen2 = MODEL_PATH == QWEN25_BASE or MODEL_PATH == QWEN25_INSTRUCT
use_qwen1 = MODEL_PATH == QWEN1
# FYI transformer_lens is not compat with gptoss, use direct hooks and cache yourself (see refusal-gptoss.py)

DEVICE = 'cuda'

model = HookedTransformer.from_pretrained_no_processing(
    MODEL_PATH,
    device=DEVICE,
    dtype=torch.float16,  #TODO! type? bfloat16
    default_padding_side='left',
    # fp16=True, # TODO did I comment this out b/c I was running on my mac instead of arch box?
    # fp16=(DEVICE == CUDA) ? TODO if fp16 s/b on for arch box w/ nvidia gpu then add toggle based on DEVICE = 'cuda' constant too (add CUDA constant and check DEVICE == CUDA or...?)
)

# %%

if use_qwen2:
    tokenizer: Qwen2TokenizerFast = model.tokenizer
elif use_qwen1:
    tokenizer: PreTrainedTokenizerBase = model.tokenizer
# else:
#     tokenizer = model.tokenizer

# how can I list tokens in tokenizer
vocab = tokenizer.get_vocab()
len(vocab)
[t for t in vocab if str(t).find("extra_0") > 0]
[t for t in vocab if str(t).startswith("<|") > 0]
[k for k in vocab.keys() if k.startswith(b"<|")]

# tokenizer.encode("<|extra_0|>")
def get_all_tokens_bc_this_shouldnt_just_exist_on_tokenizer():
    for i in range(tokenizer.vocab_size):
        tok = tokenizer.convert_ids_to_tokens(i)
        yield i, tok

[tok for id, tok in get_all_tokens_bc_this_shouldnt_just_exist_on_tokenizer() if str(tok).startswith("<|")]
[tok for id, tok in get_all_tokens_bc_this_shouldnt_just_exist_on_tokenizer() if str(tok).find("extra") > -1]

# %% SET PAD TOKEN THAT AVOIDS CONTAMINATION

model.tokenizer.padding_side = 'left'  # defaults: qwen1:left, qwen2.5:left
# PRN try right padding? update calculations and references, anything noteworthy? probably not
#   mostly about where to measure logits (mid stream vs last token, etc)... in right padded each sequence's length is needed to determine where the next token is positioned

# PRN add toggles for qwen1 vs qwen2.5 to do these:
if use_qwen1:
    # set pad_token to lesser used token to avoid residual contamination from padding:
    #   weights are random and likely
    model.tokenizer.pad_token = '<|extra_0|>'  # defaults:qwen1==qwen2.5=='<|endoftext|>' (IIRC b/c it is not set, this is hf default)
    #
    # Confirmed only one with Qwen1:
    #   In [31]: [model.tokenizer.decode(i) for i in model.tokenizer.encode(("<|extra_0|>")) ]
    #   Out[31]: ['<|extra_0|>']

if use_qwen2:
    # FYI In [51]: [model.tokenizer.decode(i) for i in model.tokenizer.encode(("<|extra_0|>")) ]
    # Out[50]: ['<', '|', 'extra', '_', '0', '|', '>']
    # there is NO <|extra_0|> or set of similar unused tokens.
    residual_pad = "<|residual_pad|>"
    tokenizer.add_special_tokens({"additional_special_tokens": [residual_pad]})
    tokenizer.add_special_tokens
    tokenizer.encode(residual_pad)
    model.tokenizer.pad_token = residual_pad

# FYI for OTHER MODELS, what value do you want to use? i.e. gptoss?

# check if token is set:
model.tokenizer.pad_token_id
model.tokenizer.decode(model.tokenizer.pad_token_id)

# %% find vocab_size (to understand outputs)

# base vocab:
len(model.tokenizer)  # qwen1:151851 qwen2.5:151665
model.tokenizer.vocab_size  # qwen1:same qwen2.5:151643(22 less)
# added:
model.tokenizer.added_tokens_decoder  # qwen1:empty(0)
len(model.tokenizer.added_tokens_decoder)  # qwen2.5:22 items
# verify total:
model.tokenizer.vocab_size + len(model.tokenizer.added_tokens_decoder) == len(model.tokenizer)  # qwen1:True qwen2.5:True

# %%
"""### Load harmful / harmless datasets"""

def get_harmful_instructions():
    # # 1a - download dataset
    # url = 'https://raw.githubusercontent.com/llm-attacks/llm-attacks/main/data/advbench/harmful_behaviors.csv'
    # response = requests.get(url)
    # contents = io.StringIO(response.content.decode('utf-8'))
    # dataset = pd.read_csv(contents)
    # print(dataset)

    # # 1b - read local file instead of download each time
    dataset = pd.read_csv('./out/harmful_behaviors.csv')

    instructions = dataset['goal'].tolist()

    train, test = train_test_split(instructions, test_size=0.2, random_state=42)
    return train, test

def get_harmless_instructions():
    hf_path = 'tatsu-lab/alpaca'
    dataset = load_dataset(hf_path)

    # filter for instructions that do not have inputs
    instructions = []
    for i in range(len(dataset['train'])):
        if dataset['train'][i]['input'].strip() == '':
            instructions.append(dataset['train'][i]['instruction'])

    train, test = train_test_split(instructions, test_size=0.2, random_state=42)
    return train, test

harmful_inst_train, harmful_inst_test = get_harmful_instructions()
harmless_inst_train, harmless_inst_test = get_harmless_instructions()

print("Harmful instructions:")
for i in range(4):
    print(f"\t{repr(harmful_inst_train[i])}")
print("Harmless instructions:")
for i in range(4):
    print(f"\t{repr(harmless_inst_train[i])}")

# %%
"""### Tokenization utils"""

QWEN_CHAT_TEMPLATE = """<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
"""

# optional add system prompt too (SAME AS Qwen1)
# curl https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/raw/main/tokenizer_config.json | jq .chat_template -r > out/qwen25instruct0.5b-chat-template.jinja
QWEN25_CHAT_TEMPLATE = """<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
"""

# model.tokenizer.padding_side = 'right'  # check padding too! (flip left/right if you want)

def tokenize_chat_prompts(instructions: List[str]) -> Int[Tensor, 'batch_size seq_len']:
    template = QWEN_CHAT_TEMPLATE if use_qwen1 else QWEN25_CHAT_TEMPLATE
    prompts = [template.format(instruction=i) for i in instructions]
    return model.tokenizer(prompts, padding=True, truncation=False, return_tensors="pt").input_ids

import rich

def format_prompt(chat_token_ids):
    rich.print(f"\n[blue bold]CHAT ({len(chat_token_ids)} tokens)[/]")
    print(model.tokenizer.decode(chat_token_ids))
    rich.print(f"  [white bold]tokens[/]")
    print("  ", chat_token_ids)

# ??? double check the prompt is as expected, also padding/pad_token
[format_prompt(chat_token_ids) for chat_token_ids in tokenize_chat_prompts(["foo", "tell me how to do bad things!"])]
# FYI you need 2+ to check padding (obviously)... so don't get rid of the first one (and make sure there's an input token length difference to trigger padding)

# %%
"""### Generation utils"""

def _generate_with_hooks(
    model: HookedTransformer,
    toks: Int[Tensor, 'batch_size seq_len'],
    max_tokens_generated: int = 64,
    fwd_hooks=[],
) -> List[str]:

    batch_seq_len = toks.shape[1]
    batch_size = toks.shape[0]
    all_toks = torch.zeros((batch_size, batch_seq_len + max_tokens_generated), dtype=torch.long, device=toks.device)
    # TODO rewrite w/ einops (good practice of both einops and model architecture, how well you recall each):
    all_toks[:, :batch_seq_len] = toks

    for new_token_number in range(max_tokens_generated):
        with model.hooks(fwd_hooks=fwd_hooks):
            logits = model(all_toks[:, :-max_tokens_generated + new_token_number])
            next_tokens = logits[:, -1, :].argmax(dim=-1)  # greedy sampling (temperature=0)
            all_toks[:, -max_tokens_generated + new_token_number] = next_tokens

    return model.tokenizer.batch_decode(all_toks[:, batch_seq_len:], skip_special_tokens=True)

def generate(
    model: HookedTransformer,
    instructions_batch: List[str],
    tokenize_instructions_fn: Callable[[List[str]], Int[Tensor, 'batch_size seq_len']],
    fwd_hooks=[],
    max_tokens_generated: int = 64,
    batch_size: int = 4,
) -> List[str]:

    # results of all batches:
    generations = []

    # divide large instructions_batch into smaller batches (based on batch_size)
    for batch_number in progress_tqdm(range(0, len(instructions_batch), batch_size)):
        toks = tokenize_instructions_fn(instructions=instructions_batch[batch_number:batch_number + batch_size])
        generation = _generate_with_hooks(
            model,
            toks,
            max_tokens_generated=max_tokens_generated,
            fwd_hooks=fwd_hooks,
        )
        generations.extend(generation)

    return generations

# %%
"""## Finding the "refusal direction"
"""

N_INST_TRAIN = 32

# tokenize (instructions => template => chat prompt)
harmful_chats_token_ids = tokenize_chat_prompts(instructions=harmful_inst_train[:N_INST_TRAIN])
harmless_chats_token_ids = tokenize_chat_prompts(instructions=harmless_inst_train[:N_INST_TRAIN])

def log_hooks(hook_name):
    print(hook_name)  # comment out to disable
    return True

# run model on harmful and harmless instructions, caching intermediate activations
harmful_logits, harmful_cache = model.run_with_cache(harmful_chats_token_ids, names_filter=lambda hook_name: 'resid' in hook_name)
harmless_logits, harmless_cache = model.run_with_cache(harmless_chats_token_ids, names_filter=lambda hook_name: 'resid' in hook_name)
# _logits = [batch_size, seq_len, vocab_size]
# _cache = per layer (qwen1 has 24 layers, pre/mid/post for each layer = 72 total caches)
#   72 keys (see summarize_keys below)
#   each => [batch_size, seq_len, hidden_dimensions]
#   where hidden_dimensions: qwen2.5_0.5B_instruct=896   qwen1_8B=2048 (b/c 8B way bigger than 0.5B)

dir(harmful_cache)
import logging

def summarize_keys(data):
    for key, value in data.items():
        if torch.is_tensor(value):
            print(f"{key}: {value.dtype}, Shape: {value.shape}")
            continue
        print(f"Damn! Key type: {type(key).__name__}, Value type: {type(value).__name__}")

summarize_keys(harmful_cache)

# compute difference of means between harmful and harmless activations at an intermediate layer

# * inspecting model:
# [m for m in model.modules()]
def summarize_layer(name, param):
    print(f"{name} shape={tuple(param.shape)} {param.dtype}")

def summarize_named_params(model):
    for name, param in model.named_parameters():
        summarize_layer(name, param)

summarize_named_params(model.embed)
# * embed
#   tokens => hidden dimensions mapping
#   [vocab_size, hidden_dimensions]
#   qwen2.5=W_E shape(151936, 896)

summarize_named_params(model.unembed)
# * unembed
#   hidden dimentions => tokens mapping
#   [hidden_dimensions, vocab_size]
#   reverse mapping from hidden dimensions => tokens

summarize_named_params(model)
# model.blocks # transformer layers (qwen25=24 layers)

# %%

def compute_refusal_dir(layer=14):
    # token to sample (-1 == last token)
    pos = -1

    # think of hidden_dimensions as your internal vocab_size

    # shape=(batch_size,seq_len,hidden_dimensions)
    harmful_residual_pre = harmful_cache['resid_pre', layer]
    harmful_residual_pre.shape
    harmful_mean_act = harmful_residual_pre[:, pos, :].mean(dim=0)
    # internal representation of each token w/in the HARMFUL dataset => [batch_size,seq_len,hidden_dimensions]
    #   then mean => average across sequences in batch
    #     ==> [1,1,hidden_dimensions] ==> shape=(hidden_dimensions)

    harmless_residual_pre = harmless_cache['resid_pre', layer]
    harmless_mean_act = harmless_residual_pre[:, pos, :].mean(dim=0)
    # same for harmless dataset

    # idea is, if you have two datasets that only meaningfully differ due to one characteristics (i.e. refusal)
    #  then if all other characteristics are randomly sampled (uniform) then the only diff in average vector of each is the refusal component (vector)
    refusal_dir = harmful_mean_act - harmless_mean_act
    refusal_dir = refusal_dir / refusal_dir.norm()

    # reading the tea leaves:
    summarize_layer("refusal_dir", refusal_dir)
    # refusal_dir.shape=[hidden_dimensions]

    # transform refusal dir to token vocab
    #   refusal_dir[hidden_dim] matmul unembed[hidden_dimensions,vocab_size] ==> [vocab_size]
    # [hidden_dimensions].matmul([hidden_dimensions,vocab_size]) = [vocab_size]

    # * interpret refusal latent space => map back to likely tokens
    # refusal_logits = refusal_dir.matmul(model.unembed.W_U)  # w/o bias is interesting
    refusal_logits = refusal_dir.matmul(model.unembed.W_U) + model.unembed.b_U
    # model.lm_head.bias
    summarize_layer("  refusal_logits", refusal_logits)  # w/e "refusal logits" means :)... basically map this latent subspace out into token space to see what meaning it has alone
    refusal_max_token_id_next = refusal_logits.argmax()
    print("  refusal max_token_id_next:", refusal_max_token_id_next)
    refusal_decoded_text_token = tokenizer.decode(refusal_max_token_id_next, skip_special_tokens=True)
    print("  refusal decoded next token (text): '" + refusal_decoded_text_token + "'")

    return refusal_dir

refusal_dir = compute_refusal_dir(layer=14)
# FREAKY:
# layer 14 - redo decoded: '告'
# 告诉 (gàosu) – to tell, inform
# 报告 (bàogào) – report, to report
# 警告 (jǐnggào) – warning

# compute_refusal_dir(layer=16)
# # layer 16 - 极其
# #   “extremely” or “to the utmost degree.”

# compute_refusal_dir(layer=18)
# # layer 18 - 非常
# # “very,” “extremely,” “highly” (used to intensify an adjective or verb)

# for layer in range(model.cfg.n_layers):
#     print("layer", layer)
#     compute_refusal_dir(layer=layer)

# %%

# clean up memory
del harmful_cache, harmless_cache, harmful_logits, harmless_logits
gc.collect()
torch.cuda.empty_cache()

# %%
"""## Ablate "refusal direction" via inference-time intervention

Given a "refusal direction" $\widehat{r} \in \mathbb{R}^{d_{\text{model}}}$ with unit norm, we can ablate this direction from the model's activations $a_{l}$:
$${a}_{l}' \leftarrow a_l - (a_l \cdot \widehat{r}) \widehat{r}$$

By performing this ablation on all intermediate activations, we enforce that the model can never express this direction (or "feature").
"""

def direction_ablation_hook(
    activation: Float[Tensor, "... d_act"],
    hook: HookPoint,
):
    direction: Float[Tensor, "d_act"] = refusal_dir
    proj = einops.einsum(activation, direction.view(-1, 1), '... d_act, d_act single -> ... single') * direction
    return activation - proj

N_INST_TEST = 48
layer_numbers = list(range(model.cfg.n_layers))  # qwen25-n_layers=24 so 0,1,2...23

hook_fn = direction_ablation_hook

fwd_hooks = [
    (utils.get_act_name(act_name, layer_number), hook_fn)
    for layer_number in layer_numbers  \
    for act_name in ['resid_pre', 'resid_mid', 'resid_post']
]

# * hone in on a subset, longer generation to see what effects might be going on... like inadvertent consequences of lobotomizing
# N_INST_START = 38
# N_INST_END = 39
# max_tokens = 512
# * defaults
N_INST_START = 0
N_INST_END = N_INST_TEST
max_tokens = 64
intervention_generations = generate(model, harmful_inst_test[N_INST_START:N_INST_END], tokenize_chat_prompts, fwd_hooks=fwd_hooks, max_tokens_generated=max_tokens)
baseline_generations = generate(model, harmful_inst_test[N_INST_START:N_INST_END], tokenize_chat_prompts, fwd_hooks=[], max_tokens_generated=max_tokens)

for i in range(N_INST_END - N_INST_START):
    actual_i = i + N_INST_START
    print(f"INSTRUCTION {actual_i}: {repr(harmful_inst_test[actual_i])}")
    print(Fore.GREEN + f"BASELINE COMPLETION:")
    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\t', subsequent_indent='\t'))
    print(Fore.RED + f"INTERVENTION COMPLETION:")
    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\t', subsequent_indent='\t'))
    print(Fore.RESET)

# %%
"""## Orthogonalize weights w.r.t. "refusal direction"

We can implement the intervention equivalently by directly orthogonalizing the weight matrices that write to the residual stream with respect to the refusal direction $\widehat{r}$:
$$W_{\text{out}}' \leftarrow W_{\text{out}} - \widehat{r}\widehat{r}^{\mathsf{T}} W_{\text{out}}$$

By orthogonalizing these weight matrices, we enforce that the model is unable to write direction $r$ to the residual stream at all!
"""

def get_orthogonalized_matrix(matrix: Float[Tensor, '... d_model'], vec: Float[Tensor, 'd_model']) -> Float[Tensor, '... d_model']:
    proj = einops.einsum(matrix, vec.view(-1, 1), '... d_model, d_model single -> ... single') * vec
    return matrix - proj

# PRN copy over range instead for this approach to lobotomizing (this is dependent on range of previous cell, by the way)
#    FYI do not re-run as this step transforms the actual weights!

model.W_E.data = get_orthogonalized_matrix(model.W_E, refusal_dir)

for block in model.blocks:
    block.attn.W_O.data = get_orthogonalized_matrix(block.attn.W_O, refusal_dir)
    block.mlp.W_out.data = get_orthogonalized_matrix(block.mlp.W_out, refusal_dir)

orthogonalized_generations = generate(model, harmful_inst_test[:N_INST_TEST], tokenize_chat_prompts, fwd_hooks=[])

for i in range(N_INST_TEST):
    print(f"INSTRUCTION {i}: {repr(harmful_inst_test[i])}")
    print(Fore.GREEN + f"BASELINE COMPLETION:")
    print(textwrap.fill(repr(baseline_generations[i]), width=100, initial_indent='\t', subsequent_indent='\t'))
    print(Fore.RED + f"INTERVENTION COMPLETION:")
    print(textwrap.fill(repr(intervention_generations[i]), width=100, initial_indent='\t', subsequent_indent='\t'))
    print(Fore.MAGENTA + f"ORTHOGONALIZED COMPLETION:")
    print(textwrap.fill(repr(orthogonalized_generations[i]), width=100, initial_indent='\t', subsequent_indent='\t'))
    print(Fore.RESET)
